<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh"><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="zh" /><updated>2020-07-16T15:03:34+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">旭的小窝</title><subtitle>这里讲我的故事
</subtitle><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><entry xml:lang="zh"><title type="html">每日一笔</title><link href="http://localhost:4000/2020/07/13/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94.html" rel="alternate" type="text/html" title="每日一笔" /><published>2020-07-13T00:00:00+09:00</published><updated>2020-07-13T00:00:00+09:00</updated><id>http://localhost:4000/2020/07/13/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94</id><content type="html" xml:base="http://localhost:4000/2020/07/13/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94.html">&lt;h3 id=&quot;2020年07月03日-161231&quot;&gt;2020年07月03日 16:12:31&lt;/h3&gt;
&lt;p&gt;许就没有来研究室了。
今天来的人还算很多，没有见到教授。&lt;/p&gt;

&lt;p&gt;免费的空调太舒服了啊，哈哈哈。&lt;/p&gt;

&lt;h3 id=&quot;2020年07月07日-163111&quot;&gt;2020年07月07日 16:31:11&lt;/h3&gt;
&lt;p&gt;今天上了三个小时的英语课，听着日本人说着日式英语觉得还挺好玩的这节课。&lt;br /&gt;
虽然上课的时间有点长就是了。&lt;/p&gt;

&lt;p&gt;然后接着来研究室蹭空调吧，不过大家都没来貌似。&lt;br /&gt;
一个人坐在这里也挺安心的。&lt;/p&gt;

&lt;h3 id=&quot;2020年07月09日-181418&quot;&gt;2020年07月09日 18:14:18&lt;/h3&gt;
&lt;p&gt;要学的东西太多了，感觉压力很大。&lt;br /&gt;
这学期的报告也太多了，有些都是没什么意义的报告，但为了学分也要写。&lt;/p&gt;

&lt;h3 id=&quot;2020年07月10日-154612&quot;&gt;2020年07月10日 15:46:12&lt;/h3&gt;
&lt;p&gt;今天去把驾照翻译了，马上要开始换驾照啦。&lt;br /&gt;
还久违的去逛了香林坊，真的疫情开始之后就再也没有去过了啊。&lt;/p&gt;

&lt;p&gt;下了一天的下雨，真的是梅雨季节啊。&lt;/p&gt;

&lt;h3 id=&quot;2020年07月13日-180734&quot;&gt;2020年07月13日 18:07:34&lt;/h3&gt;
&lt;p&gt;金泽的小雨还是一直下，梅雨天气一直持续着。&lt;/p&gt;

&lt;p&gt;最近看电视剧太多了，有点太疯狂了。&lt;br /&gt;
现在人总是不能接受延迟满足，总是寻求一时的刺激获得满足。&lt;br /&gt;
真是变得越来越没有耐心了。&lt;/p&gt;

&lt;h3 id=&quot;2020年07月16日-144817&quot;&gt;2020年07月16日 14:48:17&lt;/h3&gt;
&lt;p&gt;终于不下雨了，不过今天也实在是太热了吧！&lt;/p&gt;

&lt;p&gt;昨天买到了半价羊肉，还买了半价牛肉，真是太难得了啊。&lt;br /&gt;
不过今天的牛肉套料放多了，现在嘴里还有一股调料味。🤦‍&lt;/p&gt;

&lt;p&gt;不下雨太好了，今天终于能跑步了。&lt;br /&gt;
加油看论文吧。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="每日一笔" /><summary type="html">2020年07月03日 16:12:31 许就没有来研究室了。 今天来的人还算很多，没有见到教授。 免费的空调太舒服了啊，哈哈哈。 2020年07月07日 16:31:11 今天上了三个小时的英语课，听着日本人说着日式英语觉得还挺好玩的这节课。 虽然上课的时间有点长就是了。 然后接着来研究室蹭空调吧，不过大家都没来貌似。 一个人坐在这里也挺安心的。 2020年07月09日 18:14:18 要学的东西太多了，感觉压力很大。 这学期的报告也太多了，有些都是没什么意义的报告，但为了学分也要写。 2020年07月10日 15:46:12 今天去把驾照翻译了，马上要开始换驾照啦。 还久违的去逛了香林坊，真的疫情开始之后就再也没有去过了啊。 下了一天的下雨，真的是梅雨季节啊。 2020年07月13日 18:07:34 金泽的小雨还是一直下，梅雨天气一直持续着。 最近看电视剧太多了，有点太疯狂了。 现在人总是不能接受延迟满足，总是寻求一时的刺激获得满足。 真是变得越来越没有耐心了。 2020年07月16日 14:48:17 终于不下雨了，不过今天也实在是太热了吧！ 昨天买到了半价羊肉，还买了半价牛肉，真是太难得了啊。 不过今天的牛肉套料放多了，现在嘴里还有一股调料味。🤦‍ 不下雨太好了，今天终于能跑步了。 加油看论文吧。</summary></entry><entry><title type="html">晃晃悠悠</title><link href="http://localhost:4000/2020/07/06/%E6%99%83%E6%99%83%E6%82%A0%E6%82%A0.html" rel="alternate" type="text/html" title="晃晃悠悠" /><published>2020-07-06T00:00:00+09:00</published><updated>2020-07-06T00:00:00+09:00</updated><id>http://localhost:4000/2020/07/06/%E6%99%83%E6%99%83%E6%82%A0%E6%82%A0</id><content type="html" xml:base="http://localhost:4000/2020/07/06/%E6%99%83%E6%99%83%E6%82%A0%E6%82%A0.html">&lt;h2 id=&quot;2020年07月06日-164840&quot;&gt;2020年07月06日 16:48:40&lt;/h2&gt;
&lt;p&gt;三天以后，考试开始了，那段日子怎么过的，想想手心就出汗，有趣的是华杨在考第一门普通物理前做了一个怪梦特有意思，他梦见他站在考场外面看着同学们一个个进去，心情非常不好，于是蹲下拉了一泡屎，监考老师催他进考场，他不去，蹲在那儿玩屎，老师说，进去呀，他说，等会儿，让我再玩会儿。&lt;/p&gt;

&lt;p&gt;11月中旬，我忽然开始疯狂地复习功课，因为快考试了，我如果还想把大学混下去的话，每门功课就得考到75分以上。叫我奇怪的是，我做到了，当然，除了对儿虾的那门，考完试后，我们班有两个同学被开除了。我听说了他们的名字，可不记得他们的模样了，这是两个不声不响的同学，听说学的很用功，他们默默地考进来，又默默地被开除出去，真惨。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="读后感" /><summary type="html">2020年07月06日 16:48:40 三天以后，考试开始了，那段日子怎么过的，想想手心就出汗，有趣的是华杨在考第一门普通物理前做了一个怪梦特有意思，他梦见他站在考场外面看着同学们一个个进去，心情非常不好，于是蹲下拉了一泡屎，监考老师催他进考场，他不去，蹲在那儿玩屎，老师说，进去呀，他说，等会儿，让我再玩会儿。 11月中旬，我忽然开始疯狂地复习功课，因为快考试了，我如果还想把大学混下去的话，每门功课就得考到75分以上。叫我奇怪的是，我做到了，当然，除了对儿虾的那门，考完试后，我们班有两个同学被开除了。我听说了他们的名字，可不记得他们的模样了，这是两个不声不响的同学，听说学的很用功，他们默默地考进来，又默默地被开除出去，真惨。</summary></entry><entry xml:lang="zh"><title type="html">每日一笔</title><link href="http://localhost:4000/2020/06/20/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94.html" rel="alternate" type="text/html" title="每日一笔" /><published>2020-06-20T00:00:00+09:00</published><updated>2020-06-20T00:00:00+09:00</updated><id>http://localhost:4000/2020/06/20/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94</id><content type="html" xml:base="http://localhost:4000/2020/06/20/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94.html">&lt;h2 id=&quot;每日一笔记录生活&quot;&gt;每日一笔，记录生活&lt;/h2&gt;
&lt;h3 id=&quot;2020年06月20日-114155&quot;&gt;2020年06月20日 11:41:55&lt;/h3&gt;
&lt;p&gt;跳过5月的时间，生活直接从6月开始。&lt;/p&gt;

&lt;p&gt;这几天过的和做梦一样，短短的时间之内连续两天彻夜未眠。&lt;br /&gt;
仿佛对我来说，有这样的时候就说明有什么事情发生了。&lt;/p&gt;

&lt;p&gt;这次我确实有所成长，但还是不够成熟。&lt;br /&gt;
但至少要好很多，因为我的内心变得足够强大了。&lt;br /&gt;
也许是直接变得没心没肺了吧。&lt;/p&gt;

&lt;p&gt;不过我能感受到我的内心变得足够强大了，而且也能感受到现在的顾虑比以前多了很多。&lt;br /&gt;
这样究竟算是好的改变，还是坏的转变呢？&lt;/p&gt;

&lt;p&gt;不知道从什么时候开始变得婆婆妈妈的了，再也没有从前的潇洒从容自信。&lt;br /&gt;
真的变了很多呀，你。&lt;/p&gt;

&lt;p&gt;毕竟每次受伤的总是我自己，吃亏的也是我。&lt;/p&gt;

&lt;p&gt;害，好好学习吧，每天都想这些有的没的。&lt;/p&gt;

&lt;h3 id=&quot;2020年06月23日-172332&quot;&gt;2020年06月23日 17:23:32&lt;/h3&gt;
&lt;p&gt;最近开始在研究《红楼梦》。&lt;/p&gt;

&lt;p&gt;我这大概是疯了吧，也许是闲的吧。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="每日一笔" /><summary type="html">每日一笔，记录生活 2020年06月20日 11:41:55 跳过5月的时间，生活直接从6月开始。 这几天过的和做梦一样，短短的时间之内连续两天彻夜未眠。 仿佛对我来说，有这样的时候就说明有什么事情发生了。 这次我确实有所成长，但还是不够成熟。 但至少要好很多，因为我的内心变得足够强大了。 也许是直接变得没心没肺了吧。 不过我能感受到我的内心变得足够强大了，而且也能感受到现在的顾虑比以前多了很多。 这样究竟算是好的改变，还是坏的转变呢？ 不知道从什么时候开始变得婆婆妈妈的了，再也没有从前的潇洒从容自信。 真的变了很多呀，你。 毕竟每次受伤的总是我自己，吃亏的也是我。 害，好好学习吧，每天都想这些有的没的。 2020年06月23日 17:23:32 最近开始在研究《红楼梦》。 我这大概是疯了吧，也许是闲的吧。</summary></entry><entry><title type="html">周志华《机器学习》</title><link href="http://localhost:4000/2020/05/21/%E7%BB%AA%E8%AE%BA.html" rel="alternate" type="text/html" title="周志华《机器学习》" /><published>2020-05-21T00:00:00+09:00</published><updated>2020-05-21T00:00:00+09:00</updated><id>http://localhost:4000/2020/05/21/%E7%BB%AA%E8%AE%BA</id><content type="html" xml:base="http://localhost:4000/2020/05/21/%E7%BB%AA%E8%AE%BA.html">&lt;h1 id=&quot;绪论&quot;&gt;绪论&lt;/h1&gt;
&lt;h2 id=&quot;基本术语&quot;&gt;基本术语&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;数据集&lt;/li&gt;
  &lt;li&gt;示例&lt;/li&gt;
  &lt;li&gt;属性/特征&lt;/li&gt;
  &lt;li&gt;属性值&lt;/li&gt;
  &lt;li&gt;属性空间/样本空间/输入空间&lt;/li&gt;
  &lt;li&gt;特征向量&lt;/li&gt;
  &lt;li&gt;维数：有几个属性就是几维&lt;/li&gt;
  &lt;li&gt;学习/训练：从数据中学的模型的过程，通过执行某个算法完成。&lt;/li&gt;
  &lt;li&gt;训练数据
    &lt;ul&gt;
      &lt;li&gt;训练样本&lt;/li&gt;
      &lt;li&gt;训练样本&lt;/li&gt;
      &lt;li&gt;训练集&lt;/li&gt;
      &lt;li&gt;假设：学得模型对应了关于数据的某些前在的规律&lt;/li&gt;
      &lt;li&gt;真相/真实：潜在规律自身&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;预测&lt;/li&gt;
  &lt;li&gt;标记&lt;/li&gt;
  &lt;li&gt;样例：有标记信息的是示例&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;标记空间/输出空间&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;分类
    &lt;ul&gt;
      &lt;li&gt;二分类任务
        &lt;ul&gt;
          &lt;li&gt;正类&lt;/li&gt;
          &lt;li&gt;反类&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;多分类任务&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;回归&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;测试&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;测试样本&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;聚类&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;簇&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;监督学习：分类，回归&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;无监督学习：聚类&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;泛化&lt;/li&gt;
  &lt;li&gt;分布
    &lt;ul&gt;
      &lt;li&gt;独立分布&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;假设空间&quot;&gt;假设空间&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;归纳&lt;/li&gt;
  &lt;li&gt;演绎&lt;/li&gt;
  &lt;li&gt;概念&lt;/li&gt;
  &lt;li&gt;版本空间&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;归纳偏好&quot;&gt;归纳偏好&lt;/h2&gt;
&lt;p&gt;机器学习算法在学习过程中对某种类型假设的偏好&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;奥卡姆剃刀：若有多个假设与观察一致，则选择最简单的那个&lt;/li&gt;
  &lt;li&gt;没有免费午餐定理：没有一种机器学习算法是适用于所有情况的&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;发展历程&quot;&gt;发展历程&lt;/h2&gt;
&lt;h2 id=&quot;应用现状&quot;&gt;应用现状&lt;/h2&gt;
&lt;h2 id=&quot;阅读材料&quot;&gt;阅读材料&lt;/h2&gt;

&lt;h1 id=&quot;模型评估与选择&quot;&gt;模型评估与选择&lt;/h1&gt;
&lt;h2 id=&quot;经验误差与过拟合&quot;&gt;经验误差与过拟合&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;错误率&lt;/li&gt;
  &lt;li&gt;精度&lt;/li&gt;
  &lt;li&gt;误差
    &lt;ul&gt;
      &lt;li&gt;训练误差/经验误差&lt;/li&gt;
      &lt;li&gt;泛化误差&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;过拟合&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;欠拟合&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;模型选择问题&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;评估方法&quot;&gt;评估方法&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;测试集&lt;/li&gt;
  &lt;li&gt;测试误差&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;留出法&quot;&gt;留出法&lt;/h3&gt;
&lt;p&gt;分层采样&lt;/p&gt;

&lt;h3 id=&quot;交叉验证法k折交叉验证&quot;&gt;交叉验证法(k折交叉验证)&lt;/h3&gt;
&lt;p&gt;留一法&lt;/p&gt;

&lt;h3 id=&quot;自助法&quot;&gt;自助法&lt;/h3&gt;
&lt;p&gt;包外估计&lt;/p&gt;

&lt;h3 id=&quot;调参与最终模型&quot;&gt;调参与最终模型&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;调参&lt;/li&gt;
  &lt;li&gt;验证集&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;性能度量&quot;&gt;性能度量&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;均方误差&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;错误率与精度&quot;&gt;错误率与精度&lt;/h3&gt;
&lt;h3 id=&quot;查准率查全率与f1&quot;&gt;查准率，查全率与F1&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;查准率&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;查全率&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;真正例&lt;/li&gt;
  &lt;li&gt;真反例&lt;/li&gt;
  &lt;li&gt;假正例&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;假反例&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;混淆矩阵&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;比较学习器性能&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;平衡点（BEP）&lt;/li&gt;
  &lt;li&gt;F1度量&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;有多个二分类混淆矩阵&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;宏查准率&lt;/li&gt;
  &lt;li&gt;宏查全率&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;宏F1&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;微查准率&lt;/li&gt;
  &lt;li&gt;微查全率&lt;/li&gt;
  &lt;li&gt;微F1&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;roc与auc&quot;&gt;ROC与AUC&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;分类阈值&lt;/li&gt;
  &lt;li&gt;ROC：受试者工作特征曲线
    &lt;ul&gt;
      &lt;li&gt;真正例率&lt;/li&gt;
      &lt;li&gt;假正例率&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;AUC：ROC曲线下的面积，比较不同学习器ROC性能&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;代价敏感错误率与代价曲线&quot;&gt;代价敏感错误率与代价曲线&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;非均等代价：衡量不同类型错误所造成的不同损失&lt;/li&gt;
  &lt;li&gt;代价矩阵&lt;/li&gt;
  &lt;li&gt;代价敏感错误率&lt;/li&gt;
  &lt;li&gt;代价曲线&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;比较校验&quot;&gt;比较校验&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;统计假设检验&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;假设检验&quot;&gt;假设检验&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;二项检验
    &lt;ul&gt;
      &lt;li&gt;置信度&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;t检验&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;交叉验证t检验&quot;&gt;交叉验证t检验&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;成对t检验&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;mcnemar检验&quot;&gt;McNemar检验&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;列联表&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;friedman验证与nemenyi后续检验&quot;&gt;Friedman验证与Nemenyi后续检验&lt;/h3&gt;

&lt;h2 id=&quot;偏差与方差&quot;&gt;偏差与方差&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;偏差-方差分解&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;偏差：度量了学习算法的期望预测与真实结果的偏离程度，既刻画了学习算法本身的拟合程度&lt;/li&gt;
  &lt;li&gt;方差：度量了同样大小的训练集的变动所导致的学习性能的变化，既刻画了数据扰动所造成的影响&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;噪声：表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，既刻画了学习问题本身的难度&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;偏差-方差窘境&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;线性模型&quot;&gt;线性模型&lt;/h1&gt;
&lt;h2 id=&quot;基本形式&quot;&gt;基本形式&lt;/h2&gt;

&lt;h2 id=&quot;线性回归&quot;&gt;线性回归&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;欧氏距离&lt;/li&gt;
  &lt;li&gt;最小二乘法
    &lt;ul&gt;
      &lt;li&gt;参数估计&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;正则化项&lt;/li&gt;
  &lt;li&gt;对数线性回归&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;对数几率回归&quot;&gt;对数几率回归&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;替代函数
    &lt;ul&gt;
      &lt;li&gt;对数几率函数&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;几率&lt;/li&gt;
  &lt;li&gt;对数几率&lt;/li&gt;
  &lt;li&gt;极大似然法&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;线性判别分析&quot;&gt;线性判别分析&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;类内散度矩阵&lt;/li&gt;
  &lt;li&gt;类间散度矩阵&lt;/li&gt;
  &lt;li&gt;广义瑞利商&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;多分类学习&quot;&gt;多分类学习&lt;/h2&gt;
&lt;p&gt;拆分策略：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;OvO&lt;/li&gt;
  &lt;li&gt;OvR&lt;/li&gt;
  &lt;li&gt;MvM
    &lt;ul&gt;
      &lt;li&gt;纠错输出编码&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;类别不平衡问题&quot;&gt;类别不平衡问题&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;再缩放
    &lt;ul&gt;
      &lt;li&gt;欠采样&lt;/li&gt;
      &lt;li&gt;过采样&lt;/li&gt;
      &lt;li&gt;阈值移动&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;阅读材料-1&quot;&gt;阅读材料&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;稀疏表示&lt;/li&gt;
  &lt;li&gt;多标记学习&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;决策树&quot;&gt;决策树&lt;/h1&gt;
&lt;h2 id=&quot;基本流程&quot;&gt;基本流程&lt;/h2&gt;

&lt;h2 id=&quot;划分选择&quot;&gt;划分选择&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;纯度&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;信息增商&quot;&gt;信息增商&lt;/h3&gt;

&lt;h3 id=&quot;增益率&quot;&gt;增益率&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;固有值&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;基尼指数&quot;&gt;基尼指数&lt;/h3&gt;

&lt;h2 id=&quot;剪枝处理&quot;&gt;剪枝处理&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;预剪枝&lt;/li&gt;
  &lt;li&gt;后剪枝&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;预剪枝&quot;&gt;预剪枝&lt;/h3&gt;

&lt;h3 id=&quot;后剪枝&quot;&gt;后剪枝&lt;/h3&gt;

&lt;h2 id=&quot;连续与缺失值&quot;&gt;连续与缺失值&lt;/h2&gt;
&lt;h3 id=&quot;连续值处理&quot;&gt;连续值处理&lt;/h3&gt;

&lt;h3 id=&quot;缺失值处理&quot;&gt;缺失值处理&lt;/h3&gt;

&lt;h2 id=&quot;多变量决策树&quot;&gt;多变量决策树&lt;/h2&gt;

&lt;h1 id=&quot;神经网络&quot;&gt;神经网络&lt;/h1&gt;
&lt;h2 id=&quot;神经元模型&quot;&gt;神经元模型&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;神经元&lt;/li&gt;
  &lt;li&gt;激活函数
    &lt;ul&gt;
      &lt;li&gt;sigmoid&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;感知机与多层网络&quot;&gt;感知机与多层网络&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;哑节点&lt;/li&gt;
  &lt;li&gt;学习率&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;隐含层&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;多层前馈神经网络&lt;/li&gt;
  &lt;li&gt;双隐层前馈网络&lt;/li&gt;
  &lt;li&gt;连接权&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;误差逆传播算法&quot;&gt;误差逆传播算法&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;试错法
    &lt;ul&gt;
      &lt;li&gt;早停&lt;/li&gt;
      &lt;li&gt;正则化&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;全局最小与局部极小&quot;&gt;全局最小与局部极小&lt;/h2&gt;

&lt;h2 id=&quot;其他常见神经网络&quot;&gt;其他常见神经网络&lt;/h2&gt;
&lt;h3 id=&quot;rbf网络&quot;&gt;RBF网络&lt;/h3&gt;
&lt;h3 id=&quot;art网络&quot;&gt;ART网络&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;竞争型学习&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;som网络&quot;&gt;SOM网络&lt;/h3&gt;
&lt;h3 id=&quot;级联相关网络&quot;&gt;级联相关网络&lt;/h3&gt;
&lt;h3 id=&quot;elman网络&quot;&gt;Elman网络&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;递归神经网络&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;boltzmann机&quot;&gt;Boltzmann机&lt;/h3&gt;

&lt;h2 id=&quot;深度学习&quot;&gt;深度学习&lt;/h2&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><category term="机器学习" /><summary type="html">绪论 基本术语 数据集 示例 属性/特征 属性值 属性空间/样本空间/输入空间 特征向量 维数：有几个属性就是几维 学习/训练：从数据中学的模型的过程，通过执行某个算法完成。 训练数据 训练样本 训练样本 训练集 假设：学得模型对应了关于数据的某些前在的规律 真相/真实：潜在规律自身 预测 标记 样例：有标记信息的是示例 标记空间/输出空间 分类 二分类任务 正类 反类 多分类任务 回归 测试 测试样本 聚类 簇 监督学习：分类，回归 无监督学习：聚类 泛化 分布 独立分布 假设空间 归纳 演绎 概念 版本空间 归纳偏好 机器学习算法在学习过程中对某种类型假设的偏好 奥卡姆剃刀：若有多个假设与观察一致，则选择最简单的那个 没有免费午餐定理：没有一种机器学习算法是适用于所有情况的 发展历程 应用现状 阅读材料 模型评估与选择 经验误差与过拟合 错误率 精度 误差 训练误差/经验误差 泛化误差 过拟合 欠拟合 模型选择问题 评估方法 测试集 测试误差 留出法 分层采样 交叉验证法(k折交叉验证) 留一法 自助法 包外估计 调参与最终模型 调参 验证集 性能度量 均方误差 错误率与精度 查准率，查全率与F1 查准率 查全率 真正例 真反例 假正例 假反例 混淆矩阵 比较学习器性能 平衡点（BEP） F1度量 有多个二分类混淆矩阵 宏查准率 宏查全率 宏F1 微查准率 微查全率 微F1 ROC与AUC 分类阈值 ROC：受试者工作特征曲线 真正例率 假正例率 AUC：ROC曲线下的面积，比较不同学习器ROC性能 代价敏感错误率与代价曲线 非均等代价：衡量不同类型错误所造成的不同损失 代价矩阵 代价敏感错误率 代价曲线 比较校验 统计假设检验 假设检验 二项检验 置信度 t检验 交叉验证t检验 成对t检验 McNemar检验 列联表 Friedman验证与Nemenyi后续检验 偏差与方差 偏差-方差分解 偏差：度量了学习算法的期望预测与真实结果的偏离程度，既刻画了学习算法本身的拟合程度 方差：度量了同样大小的训练集的变动所导致的学习性能的变化，既刻画了数据扰动所造成的影响 噪声：表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，既刻画了学习问题本身的难度 偏差-方差窘境 线性模型 基本形式 线性回归 欧氏距离 最小二乘法 参数估计 正则化项 对数线性回归 对数几率回归 替代函数 对数几率函数 几率 对数几率 极大似然法 线性判别分析 类内散度矩阵 类间散度矩阵 广义瑞利商 多分类学习 拆分策略： OvO OvR MvM 纠错输出编码 类别不平衡问题 再缩放 欠采样 过采样 阈值移动 阅读材料 稀疏表示 多标记学习 决策树 基本流程 划分选择 纯度 信息增商 增益率 固有值 基尼指数 剪枝处理 预剪枝 后剪枝 预剪枝 后剪枝 连续与缺失值 连续值处理 缺失值处理 多变量决策树 神经网络 神经元模型 神经元 激活函数 sigmoid 感知机与多层网络 哑节点 学习率 隐含层 多层前馈神经网络 双隐层前馈网络 连接权 误差逆传播算法 试错法 早停 正则化 全局最小与局部极小 其他常见神经网络 RBF网络 ART网络 竞争型学习 SOM网络 级联相关网络 Elman网络 递归神经网络 Boltzmann机 深度学习</summary></entry><entry><title type="html">卷积神经网络笔记</title><link href="http://localhost:4000/2020/05/20/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B0.html" rel="alternate" type="text/html" title="卷积神经网络笔记" /><published>2020-05-20T00:00:00+09:00</published><updated>2020-05-20T00:00:00+09:00</updated><id>http://localhost:4000/2020/05/20/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B0</id><content type="html" xml:base="http://localhost:4000/2020/05/20/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B0.html">&lt;h1 id=&quot;卷积层&quot;&gt;卷积层&lt;/h1&gt;
&lt;h2 id=&quot;概述和直观介绍&quot;&gt;概述和直观介绍&lt;/h2&gt;
&lt;p&gt;卷积层的参数是有一些可学习的滤波器集合构成的。每个滤波器在空间上（宽度和高度）都比较小，但是深度和输入数据一致。举例来说，卷积神经网络第一层的一个典型的滤波器的尺寸可以是5x5x3（宽高都是5像素，深度是3是因为图像应为颜色通道，所以有3的深度）。在前向传播的时候，让每个滤波器都在输入数据的宽度和高度上滑动（更精确地说是卷积），然后计算整个滤波器和输入数据任一处的内积。当滤波器沿着输入数据的宽度和高度滑过后，会生成一个2维的激活图（activation map），激活图给出了在每个空间位置处滤波器的反应。&lt;/p&gt;

&lt;h2 id=&quot;局部连接&quot;&gt;局部连接&lt;/h2&gt;
&lt;p&gt;在处理图像这样的高维度输入时，让每个神经元都与前一层中的所有神经元进行全连接是不现实的。相反，我们让每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的感受野（receptive field），它的尺寸是一个超参数（其实就是滤波器的空间尺寸）。在深度方向上，这个连接的大小总是和输入量的深度相等。需要再次强调的是，我们对待空间维度（宽和高）与深度维度是不同的：连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;例1：假设输入数据体尺寸为[32x32x3]（比如CIFAR-10的RGB图像），如果感受野（或滤波器尺寸）是5x5，那么卷积层中的每个神经元会有输入数据体中[5x5x3]区域的权重，共5x5x3=75个权重（还要加一个偏差参数）。注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;空间排列&quot;&gt;空间排列&lt;/h2&gt;
&lt;p&gt;上文讲解了卷积层中每个神经元与输入数据体之间的连接方式，但是尚未讨论输出数据体中神经元的数量，以及它们的排列方式。3个超参数控制着输出数据体的尺寸：深度（depth），步长（stride）和零填充（zero-padding）。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;输出数据体的深度是一个超参数：它和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西。举例来说，如果第一个卷积层的输入是原始图像，那么在深度维度上的不同神经元将可能被不同方向的边界，或者是颜色斑点激活。我们将这些沿着深度方向排列、感受野相同的神经元集合称为深度列（depth column），也有人使用纤维（fibre）来称呼它们。&lt;/li&gt;
  &lt;li&gt;在滑动滤波器的时候，必须指定步长。当步长为1，滤波器每次移动1个像素。当步长为2（或者不常用的3，或者更多，这些在实际中很少使用），滤波器滑动时每次移动2个像素。这个操作会让输出数据体在空间上变小。&lt;/li&gt;
  &lt;li&gt;这个零填充（zero-padding）的尺寸是一个超参数。零填充有一个良好性质，即可以控制输出数据体的空间尺寸（最常用的是用来保持输入数据体在空间上的尺寸，这样输入和输出的宽高都相等）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;参数共享&quot;&gt;参数共享&lt;/h2&gt;
&lt;p&gt;在卷积层中使用参数共享是用来控制参数的数量。&lt;/p&gt;

&lt;h2 id=&quot;小结&quot;&gt;小结&lt;/h2&gt;
&lt;p&gt;我们总结一下卷积层的性质：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;输入数据体的尺寸为 W1xH1xD1&lt;/li&gt;
  &lt;li&gt;4个超参数：
    &lt;ul&gt;
      &lt;li&gt;滤波器的数量K&lt;/li&gt;
      &lt;li&gt;滤波器的空间尺寸F&lt;/li&gt;
      &lt;li&gt;步长S&lt;/li&gt;
      &lt;li&gt;零填充数量P&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;输出数据体的尺寸为 W2xH2xD2&lt;/li&gt;
  &lt;li&gt;在输出数据体中，第d个深度切片，用第d个滤波器和输入数据进行有效卷积运算的结果（使用步长s），最后在加上第d个偏差。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;用矩阵乘法实现&quot;&gt;用矩阵乘法实现&lt;/h2&gt;
&lt;p&gt;卷积运算本质上就是在滤波器和输入数据的局部区域间做点积。卷积层的常用实现方式就是利用这一点，将卷积层的前向传播变成一个巨大的矩阵乘法。&lt;/p&gt;

&lt;h1 id=&quot;汇聚层&quot;&gt;汇聚层&lt;/h1&gt;
&lt;p&gt;通常，在连续的卷积层之间会周期性地插入一个汇聚层。它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。汇聚层使用MAX操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。&lt;/p&gt;

&lt;h2 id=&quot;不使用汇聚层&quot;&gt;不使用汇聚层&lt;/h2&gt;
&lt;p&gt;很多人不喜欢汇聚操作，认为可以不使用它。比如在Striving for Simplicity: The All Convolutional Net一文中，提出使用一种只有重复的卷积层组成的结构，抛弃汇聚层。通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为，在训练一个良好的生成模型时，弃用汇聚层也是很重要的。比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）。现在看起来，未来的卷积网络结构中，可能会很少使用甚至不使用汇聚层。&lt;/p&gt;

&lt;h1 id=&quot;全连接层&quot;&gt;全连接层&lt;/h1&gt;
&lt;p&gt;在全连接层中，神经元对于前一层中的所有激活数据是全部连接的，这个常规神经网络中一样。它们的激活可以先用矩阵乘法，再加上偏差。更多细节请查看神经网络章节。&lt;/p&gt;

&lt;h1 id=&quot;把全连接层转化成卷积层&quot;&gt;把全连接层转化成卷积层&lt;/h1&gt;
&lt;p&gt;全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在两类层中，神经元都是计算点积，所以它们的函数形式是一样的。因此，将此两者相互转化是可能的：&lt;/p&gt;

&lt;h2 id=&quot;全连接层转化为卷积层&quot;&gt;全连接层转化为卷积层&lt;/h2&gt;
&lt;p&gt;在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是224x224x3的图像，一系列的卷积层和汇聚层将图像数据变为尺寸为7x7x512的激活数据体（在AlexNet中就是这样，通过使用5个汇聚层来对输入数据进行空间上的降采样，每次尺寸下降一半，所以最终空间尺寸为224/2/2/2/2/2=7）。从这里可以看到，AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。&lt;/p&gt;

&lt;h1 id=&quot;卷积神经网络的结构&quot;&gt;卷积神经网络的结构&lt;/h1&gt;
&lt;p&gt;卷积神经网络通常是由三种层构成：卷积层，汇聚层（除非特别说明，一般就是最大值汇聚）和全连接层（简称FC）。ReLU激活函数也应该算是是一层，它逐元素地进行激活函数操作。在本节中将讨论在卷积神经网络中这些层通常是如何组合在一起的。&lt;/p&gt;

&lt;h2 id=&quot;层的排列规律&quot;&gt;层的排列规律&lt;/h2&gt;
&lt;p&gt;卷积神经网络最常见的形式就是将一些卷积层和ReLU层放在一起，其后紧跟汇聚层，然后重复如此直到图像在空间上被缩小到一个足够小的尺寸，在某个地方过渡成成全连接层也较为常见。最后的全连接层得到输出，比如分类评分等。换句话说，最常见的卷积神经网络结构如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;INPUT -&amp;gt; [[CONV -&amp;gt; RELU]*N -&amp;gt; POOL?]*M -&amp;gt; [FC -&amp;gt; RELU]*K -&amp;gt; FC
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;其中*指的是重复次数，POOL?指的是一个可选的汇聚层。其中N &amp;gt;=0,通常N&amp;lt;=3,M&amp;gt;=0,K&amp;gt;=0,通常K&amp;lt;3。&lt;/p&gt;

&lt;h2 id=&quot;层的尺寸设置规律&quot;&gt;层的尺寸设置规律&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;输入层（包含图像的）应该能被2整除很多次。常用数字包括32（比如CIFAR-10），64，96（比如STL-10）或224（比如ImageNet卷积神经网络），384和512。&lt;/li&gt;
  &lt;li&gt;卷积层应该使用小尺寸滤波器（比如3x3或最多5x5），使用步长1。还有一点非常重要，就是对输入数据进行零填充，这样卷积层就不会改变输入数据在空间维度上的尺寸。&lt;/li&gt;
  &lt;li&gt;汇聚层负责对输入数据的空间维度进行降采样。最常用的设置是用用2x2感受野的最大值汇聚，步长为2。注意这一操作将会把输入数据中75%的激活数据丢弃（因为对宽度和高度都进行了2的降采样）。另一个不那么常用的设置是使用3x3的感受野，步长为2。最大值汇聚的感受野尺寸很少有超过3的，因为汇聚操作过于激烈，易造成数据信息丢失，这通常会导致算法性能变差。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;qa&quot;&gt;Q&amp;amp;A&lt;/h1&gt;
&lt;h2 id=&quot;减少尺寸设置的问题&quot;&gt;减少尺寸设置的问题&lt;/h2&gt;
&lt;p&gt;上文中展示的两种设置是很好的，因为所有的卷积层都能保持其输入数据的空间尺寸，汇聚层只负责对数据体从空间维度进行降采样。如果使用的步长大于1并且不对卷积层的输入数据使用零填充，那么就必须非常仔细地监督输入数据体通过整个卷积神经网络结构的过程，确认所有的步长和滤波器都尺寸互相吻合，卷积神经网络的结构美妙对称地联系在一起。&lt;/p&gt;

&lt;h2 id=&quot;为什么在卷积层使用1的步长&quot;&gt;为什么在卷积层使用1的步长？&lt;/h2&gt;
&lt;p&gt;在实际应用中，更小的步长效果更好。上文也已经提过，步长为1可以让空间维度的降采样全部由汇聚层负责，卷积层只负责对输入数据体的深度进行变换。&lt;/p&gt;

&lt;h2 id=&quot;为何使用零填充&quot;&gt;为何使用零填充？&lt;/h2&gt;
&lt;p&gt;使用零填充除了前面提到的可以让卷积层的输出数据保持和输入数据在空间维度的不变，还可以提高算法性能。如果卷积层值进行卷积而不进行零填充，那么数据体的尺寸就会略微减小，那么图像边缘的信息就会过快地损失掉。&lt;/p&gt;

&lt;h2 id=&quot;因为内存限制所做的妥协&quot;&gt;因为内存限制所做的妥协&lt;/h2&gt;
&lt;p&gt;在某些案例（尤其是早期的卷积神经网络结构）中，基于前面的各种规则，内存的使用量迅速飙升。例如，使用64个尺寸为3x3的滤波器对224x224x3的图像进行卷积，零填充为1，得到的激活数据体尺寸是[224x224x64]。这个数量就是一千万的激活数据，或者就是72MB的内存（每张图就是这么多，激活函数和梯度都是）。因为GPU通常因为内存导致性能瓶颈，所以做出一些妥协是必须的。在实践中，人们倾向于在网络的第一个卷积层做出妥协。例如，可以妥协可能是在第一个卷积层使用步长为2，尺寸为7x7的滤波器（比如在ZFnet中）。在AlexNet中，滤波器的尺寸的11x11，步长为4。&lt;/p&gt;

&lt;h1 id=&quot;参考&quot;&gt;参考&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit&quot;&gt;(知乎) CS231n课程笔记翻译：卷积神经网络笔记&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://cs231n.github.io/convolutional-networks/&quot;&gt;(cs231n) Convolutional Neural Networks (CNNs / ConvNets)&lt;/a&gt;&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><category term="cs231n" /><summary type="html">卷积层 概述和直观介绍 卷积层的参数是有一些可学习的滤波器集合构成的。每个滤波器在空间上（宽度和高度）都比较小，但是深度和输入数据一致。举例来说，卷积神经网络第一层的一个典型的滤波器的尺寸可以是5x5x3（宽高都是5像素，深度是3是因为图像应为颜色通道，所以有3的深度）。在前向传播的时候，让每个滤波器都在输入数据的宽度和高度上滑动（更精确地说是卷积），然后计算整个滤波器和输入数据任一处的内积。当滤波器沿着输入数据的宽度和高度滑过后，会生成一个2维的激活图（activation map），激活图给出了在每个空间位置处滤波器的反应。 局部连接 在处理图像这样的高维度输入时，让每个神经元都与前一层中的所有神经元进行全连接是不现实的。相反，我们让每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的感受野（receptive field），它的尺寸是一个超参数（其实就是滤波器的空间尺寸）。在深度方向上，这个连接的大小总是和输入量的深度相等。需要再次强调的是，我们对待空间维度（宽和高）与深度维度是不同的：连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致。 例1：假设输入数据体尺寸为[32x32x3]（比如CIFAR-10的RGB图像），如果感受野（或滤波器尺寸）是5x5，那么卷积层中的每个神经元会有输入数据体中[5x5x3]区域的权重，共5x5x3=75个权重（还要加一个偏差参数）。注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。 空间排列 上文讲解了卷积层中每个神经元与输入数据体之间的连接方式，但是尚未讨论输出数据体中神经元的数量，以及它们的排列方式。3个超参数控制着输出数据体的尺寸：深度（depth），步长（stride）和零填充（zero-padding）。 输出数据体的深度是一个超参数：它和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西。举例来说，如果第一个卷积层的输入是原始图像，那么在深度维度上的不同神经元将可能被不同方向的边界，或者是颜色斑点激活。我们将这些沿着深度方向排列、感受野相同的神经元集合称为深度列（depth column），也有人使用纤维（fibre）来称呼它们。 在滑动滤波器的时候，必须指定步长。当步长为1，滤波器每次移动1个像素。当步长为2（或者不常用的3，或者更多，这些在实际中很少使用），滤波器滑动时每次移动2个像素。这个操作会让输出数据体在空间上变小。 这个零填充（zero-padding）的尺寸是一个超参数。零填充有一个良好性质，即可以控制输出数据体的空间尺寸（最常用的是用来保持输入数据体在空间上的尺寸，这样输入和输出的宽高都相等）。 参数共享 在卷积层中使用参数共享是用来控制参数的数量。 小结 我们总结一下卷积层的性质： 输入数据体的尺寸为 W1xH1xD1 4个超参数： 滤波器的数量K 滤波器的空间尺寸F 步长S 零填充数量P 输出数据体的尺寸为 W2xH2xD2 在输出数据体中，第d个深度切片，用第d个滤波器和输入数据进行有效卷积运算的结果（使用步长s），最后在加上第d个偏差。 用矩阵乘法实现 卷积运算本质上就是在滤波器和输入数据的局部区域间做点积。卷积层的常用实现方式就是利用这一点，将卷积层的前向传播变成一个巨大的矩阵乘法。 汇聚层 通常，在连续的卷积层之间会周期性地插入一个汇聚层。它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。汇聚层使用MAX操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。 不使用汇聚层 很多人不喜欢汇聚操作，认为可以不使用它。比如在Striving for Simplicity: The All Convolutional Net一文中，提出使用一种只有重复的卷积层组成的结构，抛弃汇聚层。通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为，在训练一个良好的生成模型时，弃用汇聚层也是很重要的。比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）。现在看起来，未来的卷积网络结构中，可能会很少使用甚至不使用汇聚层。 全连接层 在全连接层中，神经元对于前一层中的所有激活数据是全部连接的，这个常规神经网络中一样。它们的激活可以先用矩阵乘法，再加上偏差。更多细节请查看神经网络章节。 把全连接层转化成卷积层 全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在两类层中，神经元都是计算点积，所以它们的函数形式是一样的。因此，将此两者相互转化是可能的： 全连接层转化为卷积层 在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是224x224x3的图像，一系列的卷积层和汇聚层将图像数据变为尺寸为7x7x512的激活数据体（在AlexNet中就是这样，通过使用5个汇聚层来对输入数据进行空间上的降采样，每次尺寸下降一半，所以最终空间尺寸为224/2/2/2/2/2=7）。从这里可以看到，AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。 卷积神经网络的结构 卷积神经网络通常是由三种层构成：卷积层，汇聚层（除非特别说明，一般就是最大值汇聚）和全连接层（简称FC）。ReLU激活函数也应该算是是一层，它逐元素地进行激活函数操作。在本节中将讨论在卷积神经网络中这些层通常是如何组合在一起的。 层的排列规律 卷积神经网络最常见的形式就是将一些卷积层和ReLU层放在一起，其后紧跟汇聚层，然后重复如此直到图像在空间上被缩小到一个足够小的尺寸，在某个地方过渡成成全连接层也较为常见。最后的全连接层得到输出，比如分类评分等。换句话说，最常见的卷积神经网络结构如下： INPUT -&amp;gt; [[CONV -&amp;gt; RELU]*N -&amp;gt; POOL?]*M -&amp;gt; [FC -&amp;gt; RELU]*K -&amp;gt; FC 其中*指的是重复次数，POOL?指的是一个可选的汇聚层。其中N &amp;gt;=0,通常N&amp;lt;=3,M&amp;gt;=0,K&amp;gt;=0,通常K&amp;lt;3。 层的尺寸设置规律 输入层（包含图像的）应该能被2整除很多次。常用数字包括32（比如CIFAR-10），64，96（比如STL-10）或224（比如ImageNet卷积神经网络），384和512。 卷积层应该使用小尺寸滤波器（比如3x3或最多5x5），使用步长1。还有一点非常重要，就是对输入数据进行零填充，这样卷积层就不会改变输入数据在空间维度上的尺寸。 汇聚层负责对输入数据的空间维度进行降采样。最常用的设置是用用2x2感受野的最大值汇聚，步长为2。注意这一操作将会把输入数据中75%的激活数据丢弃（因为对宽度和高度都进行了2的降采样）。另一个不那么常用的设置是使用3x3的感受野，步长为2。最大值汇聚的感受野尺寸很少有超过3的，因为汇聚操作过于激烈，易造成数据信息丢失，这通常会导致算法性能变差。 Q&amp;amp;A 减少尺寸设置的问题 上文中展示的两种设置是很好的，因为所有的卷积层都能保持其输入数据的空间尺寸，汇聚层只负责对数据体从空间维度进行降采样。如果使用的步长大于1并且不对卷积层的输入数据使用零填充，那么就必须非常仔细地监督输入数据体通过整个卷积神经网络结构的过程，确认所有的步长和滤波器都尺寸互相吻合，卷积神经网络的结构美妙对称地联系在一起。 为什么在卷积层使用1的步长？ 在实际应用中，更小的步长效果更好。上文也已经提过，步长为1可以让空间维度的降采样全部由汇聚层负责，卷积层只负责对输入数据体的深度进行变换。 为何使用零填充？ 使用零填充除了前面提到的可以让卷积层的输出数据保持和输入数据在空间维度的不变，还可以提高算法性能。如果卷积层值进行卷积而不进行零填充，那么数据体的尺寸就会略微减小，那么图像边缘的信息就会过快地损失掉。 因为内存限制所做的妥协 在某些案例（尤其是早期的卷积神经网络结构）中，基于前面的各种规则，内存的使用量迅速飙升。例如，使用64个尺寸为3x3的滤波器对224x224x3的图像进行卷积，零填充为1，得到的激活数据体尺寸是[224x224x64]。这个数量就是一千万的激活数据，或者就是72MB的内存（每张图就是这么多，激活函数和梯度都是）。因为GPU通常因为内存导致性能瓶颈，所以做出一些妥协是必须的。在实践中，人们倾向于在网络的第一个卷积层做出妥协。例如，可以妥协可能是在第一个卷积层使用步长为2，尺寸为7x7的滤波器（比如在ZFnet中）。在AlexNet中，滤波器的尺寸的11x11，步长为4。 参考 (知乎) CS231n课程笔记翻译：卷积神经网络笔记 (cs231n) Convolutional Neural Networks (CNNs / ConvNets)</summary></entry><entry><title type="html">房思琪的初恋乐园</title><link href="http://localhost:4000/2020/05/20/%E6%88%BF%E6%80%9D%E7%90%AA%E7%9A%84%E5%88%9D%E6%81%8B%E4%B9%90%E5%9B%AD.html" rel="alternate" type="text/html" title="房思琪的初恋乐园" /><published>2020-05-20T00:00:00+09:00</published><updated>2020-05-20T00:00:00+09:00</updated><id>http://localhost:4000/2020/05/20/%E6%88%BF%E6%80%9D%E7%90%AA%E7%9A%84%E5%88%9D%E6%81%8B%E4%B9%90%E5%9B%AD</id><content type="html" xml:base="http://localhost:4000/2020/05/20/%E6%88%BF%E6%80%9D%E7%90%AA%E7%9A%84%E5%88%9D%E6%81%8B%E4%B9%90%E5%9B%AD.html">&lt;h1 id=&quot;书评&quot;&gt;书评&lt;/h1&gt;
&lt;h2 id=&quot;2020年05月20日-160729&quot;&gt;2020年05月20日 16:07:29&lt;/h2&gt;
&lt;p&gt;这本书读起来太吃力了，感觉心中承受了很大的压力与痛苦。&lt;/p&gt;

&lt;p&gt;仿佛我变成了李国华，曾经干过很过分的事情，对一个房思琪和很多人。&lt;br /&gt;
一瞬间我又变成了房思琪，看着伊纹姐承受暴力，又被憧憬的李老师以“爱”为名义肆意蹂躏。&lt;/p&gt;

&lt;p&gt;作者的文笔太好了，一个句子反反复复读几遍才能明白其中深意。&lt;br /&gt;
有的时候觉得脑子跟不上作者的文笔了。&lt;/p&gt;

&lt;p&gt;拉回现实的时候喘口气，还好只是小说。&lt;br /&gt;
我多么幸运，幸福。&lt;/p&gt;

&lt;h2 id=&quot;2020年05月21日-162208&quot;&gt;2020年05月21日 16:22:08&lt;/h2&gt;
&lt;p&gt;第二章失乐园快读完了，心里还是很压抑。&lt;br /&gt;
感觉作者的思路很乱，一段话需要反反复复的理解，值得深思。&lt;/p&gt;

&lt;p&gt;也许是我理解能力太差了，作者文笔还是太好了。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="读后感" /><summary type="html">书评 2020年05月20日 16:07:29 这本书读起来太吃力了，感觉心中承受了很大的压力与痛苦。 仿佛我变成了李国华，曾经干过很过分的事情，对一个房思琪和很多人。 一瞬间我又变成了房思琪，看着伊纹姐承受暴力，又被憧憬的李老师以“爱”为名义肆意蹂躏。 作者的文笔太好了，一个句子反反复复读几遍才能明白其中深意。 有的时候觉得脑子跟不上作者的文笔了。 拉回现实的时候喘口气，还好只是小说。 我多么幸运，幸福。 2020年05月21日 16:22:08 第二章失乐园快读完了，心里还是很压抑。 感觉作者的思路很乱，一段话需要反反复复的理解，值得深思。 也许是我理解能力太差了，作者文笔还是太好了。</summary></entry><entry><title type="html">深度学习用于计算机视觉</title><link href="http://localhost:4000/2020/05/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89.html" rel="alternate" type="text/html" title="深度学习用于计算机视觉" /><published>2020-05-18T00:00:00+09:00</published><updated>2020-05-18T00:00:00+09:00</updated><id>http://localhost:4000/2020/05/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89</id><content type="html" xml:base="http://localhost:4000/2020/05/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89.html">&lt;h1 id=&quot;卷积神经网络简介&quot;&gt;卷积神经网络简介&lt;/h1&gt;
&lt;h2 id=&quot;卷积运算&quot;&gt;卷积运算&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;密集连层：从输入特征空间中学到的是全局模式&lt;/li&gt;
  &lt;li&gt;卷积层：局部模式&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;卷积神经网络具有以下两个特性：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;学到的模式具有 &lt;strong&gt;平移不变性&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;可以学到 &lt;strong&gt;模式的空间层次结构&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;卷积由以下两个关键参数定义：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;从输入中提取的图块尺寸&lt;/li&gt;
  &lt;li&gt;输出特征图的深度：卷积所计算的过滤器的数量。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;输出的宽度和高度可能与输入的不同，原因：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;边界效应，可通过对输入特征图进行填充来抵消&lt;/li&gt;
  &lt;li&gt;使用了 &lt;strong&gt;步幅&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;最大池化运算&quot;&gt;最大池化运算&lt;/h2&gt;
&lt;p&gt;作用：对特征图进行下采样，与步进卷积类似。 &lt;br /&gt;
原因：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;减少需要处理的特征图的元素个数&lt;/li&gt;
  &lt;li&gt;通过让连续卷积层的观察窗口越来越大从而引入空间过滤器的层级结构&lt;/li&gt;
&lt;/ul&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><category term="Python深度学习" /><summary type="html">卷积神经网络简介 卷积运算 密集连层：从输入特征空间中学到的是全局模式 卷积层：局部模式 卷积神经网络具有以下两个特性： 学到的模式具有 平移不变性 可以学到 模式的空间层次结构 卷积由以下两个关键参数定义： 从输入中提取的图块尺寸 输出特征图的深度：卷积所计算的过滤器的数量。 输出的宽度和高度可能与输入的不同，原因： 边界效应，可通过对输入特征图进行填充来抵消 使用了 步幅 最大池化运算 作用：对特征图进行下采样，与步进卷积类似。 原因： 减少需要处理的特征图的元素个数 通过让连续卷积层的观察窗口越来越大从而引入空间过滤器的层级结构</summary></entry><entry><title type="html">你当像鸟飞向你的山</title><link href="http://localhost:4000/2020/04/25/%E4%BD%A0%E5%BD%93%E5%83%8F%E9%B8%9F%E9%A3%9E%E5%90%91%E4%BD%A0%E7%9A%84%E5%B1%B1.html" rel="alternate" type="text/html" title="你当像鸟飞向你的山" /><published>2020-04-25T00:00:00+09:00</published><updated>2020-04-25T00:00:00+09:00</updated><id>http://localhost:4000/2020/04/25/%E4%BD%A0%E5%BD%93%E5%83%8F%E9%B8%9F%E9%A3%9E%E5%90%91%E4%BD%A0%E7%9A%84%E5%B1%B1</id><content type="html" xml:base="http://localhost:4000/2020/04/25/%E4%BD%A0%E5%BD%93%E5%83%8F%E9%B8%9F%E9%A3%9E%E5%90%91%E4%BD%A0%E7%9A%84%E5%B1%B1.html"></content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="读后感" /><summary type="html"></summary></entry><entry><title type="html">陈凯旭，生日快乐！</title><link href="http://localhost:4000/2020/04/12/%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90.html" rel="alternate" type="text/html" title="陈凯旭，生日快乐！" /><published>2020-04-12T00:00:00+09:00</published><updated>2020-04-12T00:00:00+09:00</updated><id>http://localhost:4000/2020/04/12/%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90</id><content type="html" xml:base="http://localhost:4000/2020/04/12/%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90.html">&lt;p&gt;今年25岁，时间过得好快呀。&lt;/p&gt;

&lt;p&gt;过去的一岁，干过一些蠢事，受过一些伤痛。&lt;br /&gt;
但也是成长最快的一岁，让我明白了一些道理。&lt;/p&gt;

&lt;p&gt;祝我生日快乐。&lt;br /&gt;
加油生活。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="致自己" /><summary type="html">今年25岁，时间过得好快呀。 过去的一岁，干过一些蠢事，受过一些伤痛。 但也是成长最快的一岁，让我明白了一些道理。 祝我生日快乐。 加油生活。</summary></entry><entry><title type="html">致命女人</title><link href="http://localhost:4000/2020/04/12/%E8%87%B4%E5%91%BD%E5%A5%B3%E4%BA%BA.html" rel="alternate" type="text/html" title="致命女人" /><published>2020-04-12T00:00:00+09:00</published><updated>2020-04-12T00:00:00+09:00</updated><id>http://localhost:4000/2020/04/12/%E8%87%B4%E5%91%BD%E5%A5%B3%E4%BA%BA</id><content type="html" xml:base="http://localhost:4000/2020/04/12/%E8%87%B4%E5%91%BD%E5%A5%B3%E4%BA%BA.html">&lt;p&gt;之前在B站上看过这个美剧的短评，特意来看完全剧。&lt;/p&gt;

&lt;p&gt;编剧真的很厉害，影片拍摄手法也很喜欢。&lt;br /&gt;
三条故事线虽然发生在不同年代，却都发生在一个房子里。&lt;br /&gt;
并且前后主人都有会面。&lt;/p&gt;

&lt;p&gt;随着时间的推进，女性的地位显而易见的得到提高，看待婚姻的态度也不一样了。&lt;/p&gt;

&lt;p&gt;不过发生故事的房子就是一个凶宅啊，被下诅咒了吧。&lt;/p&gt;

&lt;p&gt;面对感情上的问题要保证足够的理智与冷静，千万不能冲动行事。&lt;br /&gt;
希望我永远都别碰到绿茶，真的玩不过。&lt;/p&gt;

&lt;p&gt;最后，千万别得罪女人。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="观后感" /><summary type="html">之前在B站上看过这个美剧的短评，特意来看完全剧。 编剧真的很厉害，影片拍摄手法也很喜欢。 三条故事线虽然发生在不同年代，却都发生在一个房子里。 并且前后主人都有会面。 随着时间的推进，女性的地位显而易见的得到提高，看待婚姻的态度也不一样了。 不过发生故事的房子就是一个凶宅啊，被下诅咒了吧。 面对感情上的问题要保证足够的理智与冷静，千万不能冲动行事。 希望我永远都别碰到绿茶，真的玩不过。 最后，千万别得罪女人。</summary></entry></feed>
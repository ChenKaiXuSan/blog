<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh"><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="zh" /><updated>2020-05-20T17:34:11+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">旭的小窝</title><subtitle>这里讲我的故事
</subtitle><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><entry><title type="html">卷积神经网络笔记</title><link href="http://localhost:4000/2020/05/20/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B0.html" rel="alternate" type="text/html" title="卷积神经网络笔记" /><published>2020-05-20T00:00:00+09:00</published><updated>2020-05-20T00:00:00+09:00</updated><id>http://localhost:4000/2020/05/20/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B0</id><content type="html" xml:base="http://localhost:4000/2020/05/20/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B0.html">&lt;h1 id=&quot;卷积层&quot;&gt;卷积层&lt;/h1&gt;
&lt;h2 id=&quot;概述和直观介绍&quot;&gt;概述和直观介绍&lt;/h2&gt;
&lt;p&gt;卷积层的参数是有一些可学习的滤波器集合构成的。每个滤波器在空间上（宽度和高度）都比较小，但是深度和输入数据一致。举例来说，卷积神经网络第一层的一个典型的滤波器的尺寸可以是5x5x3（宽高都是5像素，深度是3是因为图像应为颜色通道，所以有3的深度）。在前向传播的时候，让每个滤波器都在输入数据的宽度和高度上滑动（更精确地说是卷积），然后计算整个滤波器和输入数据任一处的内积。当滤波器沿着输入数据的宽度和高度滑过后，会生成一个2维的激活图（activation map），激活图给出了在每个空间位置处滤波器的反应。&lt;/p&gt;

&lt;h2 id=&quot;局部连接&quot;&gt;局部连接&lt;/h2&gt;
&lt;p&gt;在处理图像这样的高维度输入时，让每个神经元都与前一层中的所有神经元进行全连接是不现实的。相反，我们让每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的感受野（receptive field），它的尺寸是一个超参数（其实就是滤波器的空间尺寸）。在深度方向上，这个连接的大小总是和输入量的深度相等。需要再次强调的是，我们对待空间维度（宽和高）与深度维度是不同的：连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;例1：假设输入数据体尺寸为[32x32x3]（比如CIFAR-10的RGB图像），如果感受野（或滤波器尺寸）是5x5，那么卷积层中的每个神经元会有输入数据体中[5x5x3]区域的权重，共5x5x3=75个权重（还要加一个偏差参数）。注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;空间排列&quot;&gt;空间排列&lt;/h2&gt;
&lt;p&gt;上文讲解了卷积层中每个神经元与输入数据体之间的连接方式，但是尚未讨论输出数据体中神经元的数量，以及它们的排列方式。3个超参数控制着输出数据体的尺寸：深度（depth），步长（stride）和零填充（zero-padding）。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;输出数据体的深度是一个超参数：它和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西。举例来说，如果第一个卷积层的输入是原始图像，那么在深度维度上的不同神经元将可能被不同方向的边界，或者是颜色斑点激活。我们将这些沿着深度方向排列、感受野相同的神经元集合称为深度列（depth column），也有人使用纤维（fibre）来称呼它们。&lt;/li&gt;
  &lt;li&gt;在滑动滤波器的时候，必须指定步长。当步长为1，滤波器每次移动1个像素。当步长为2（或者不常用的3，或者更多，这些在实际中很少使用），滤波器滑动时每次移动2个像素。这个操作会让输出数据体在空间上变小。&lt;/li&gt;
  &lt;li&gt;这个零填充（zero-padding）的尺寸是一个超参数。零填充有一个良好性质，即可以控制输出数据体的空间尺寸（最常用的是用来保持输入数据体在空间上的尺寸，这样输入和输出的宽高都相等）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;参数共享&quot;&gt;参数共享&lt;/h2&gt;
&lt;p&gt;在卷积层中使用参数共享是用来控制参数的数量。&lt;/p&gt;

&lt;h2 id=&quot;小结&quot;&gt;小结&lt;/h2&gt;
&lt;p&gt;我们总结一下卷积层的性质：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;输入数据体的尺寸为 W1xH1xD1&lt;/li&gt;
  &lt;li&gt;4个超参数：
    &lt;ul&gt;
      &lt;li&gt;滤波器的数量K&lt;/li&gt;
      &lt;li&gt;滤波器的空间尺寸F&lt;/li&gt;
      &lt;li&gt;步长S&lt;/li&gt;
      &lt;li&gt;零填充数量P&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;输出数据体的尺寸为 W2xH2xD2&lt;/li&gt;
  &lt;li&gt;在输出数据体中，第d个深度切片，用第d个滤波器和输入数据进行有效卷积运算的结果（使用步长s），最后在加上第d个偏差。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;用矩阵乘法实现&quot;&gt;用矩阵乘法实现&lt;/h2&gt;
&lt;p&gt;卷积运算本质上就是在滤波器和输入数据的局部区域间做点积。卷积层的常用实现方式就是利用这一点，将卷积层的前向传播变成一个巨大的矩阵乘法。&lt;/p&gt;

&lt;h1 id=&quot;汇聚层&quot;&gt;汇聚层&lt;/h1&gt;
&lt;p&gt;通常，在连续的卷积层之间会周期性地插入一个汇聚层。它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。汇聚层使用MAX操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。&lt;/p&gt;

&lt;h2 id=&quot;不使用汇聚层&quot;&gt;不使用汇聚层&lt;/h2&gt;
&lt;p&gt;很多人不喜欢汇聚操作，认为可以不使用它。比如在Striving for Simplicity: The All Convolutional Net一文中，提出使用一种只有重复的卷积层组成的结构，抛弃汇聚层。通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为，在训练一个良好的生成模型时，弃用汇聚层也是很重要的。比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）。现在看起来，未来的卷积网络结构中，可能会很少使用甚至不使用汇聚层。&lt;/p&gt;

&lt;h1 id=&quot;全连接层&quot;&gt;全连接层&lt;/h1&gt;
&lt;p&gt;在全连接层中，神经元对于前一层中的所有激活数据是全部连接的，这个常规神经网络中一样。它们的激活可以先用矩阵乘法，再加上偏差。更多细节请查看神经网络章节。&lt;/p&gt;

&lt;h1 id=&quot;把全连接层转化成卷积层&quot;&gt;把全连接层转化成卷积层&lt;/h1&gt;
&lt;p&gt;全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在两类层中，神经元都是计算点积，所以它们的函数形式是一样的。因此，将此两者相互转化是可能的：&lt;/p&gt;

&lt;h2 id=&quot;全连接层转化为卷积层&quot;&gt;全连接层转化为卷积层&lt;/h2&gt;
&lt;p&gt;在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是224x224x3的图像，一系列的卷积层和汇聚层将图像数据变为尺寸为7x7x512的激活数据体（在AlexNet中就是这样，通过使用5个汇聚层来对输入数据进行空间上的降采样，每次尺寸下降一半，所以最终空间尺寸为224/2/2/2/2/2=7）。从这里可以看到，AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。&lt;/p&gt;

&lt;h1 id=&quot;卷积神经网络的结构&quot;&gt;卷积神经网络的结构&lt;/h1&gt;
&lt;p&gt;卷积神经网络通常是由三种层构成：卷积层，汇聚层（除非特别说明，一般就是最大值汇聚）和全连接层（简称FC）。ReLU激活函数也应该算是是一层，它逐元素地进行激活函数操作。在本节中将讨论在卷积神经网络中这些层通常是如何组合在一起的。&lt;/p&gt;

&lt;h2 id=&quot;层的排列规律&quot;&gt;层的排列规律&lt;/h2&gt;
&lt;p&gt;卷积神经网络最常见的形式就是将一些卷积层和ReLU层放在一起，其后紧跟汇聚层，然后重复如此直到图像在空间上被缩小到一个足够小的尺寸，在某个地方过渡成成全连接层也较为常见。最后的全连接层得到输出，比如分类评分等。换句话说，最常见的卷积神经网络结构如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;INPUT -&amp;gt; [[CONV -&amp;gt; RELU]*N -&amp;gt; POOL?]*M -&amp;gt; [FC -&amp;gt; RELU]*K -&amp;gt; FC
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;其中*指的是重复次数，POOL?指的是一个可选的汇聚层。其中N &amp;gt;=0,通常N&amp;lt;=3,M&amp;gt;=0,K&amp;gt;=0,通常K&amp;lt;3。&lt;/p&gt;

&lt;h2 id=&quot;层的尺寸设置规律&quot;&gt;层的尺寸设置规律&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;输入层（包含图像的）应该能被2整除很多次。常用数字包括32（比如CIFAR-10），64，96（比如STL-10）或224（比如ImageNet卷积神经网络），384和512。&lt;/li&gt;
  &lt;li&gt;卷积层应该使用小尺寸滤波器（比如3x3或最多5x5），使用步长1。还有一点非常重要，就是对输入数据进行零填充，这样卷积层就不会改变输入数据在空间维度上的尺寸。&lt;/li&gt;
  &lt;li&gt;汇聚层负责对输入数据的空间维度进行降采样。最常用的设置是用用2x2感受野的最大值汇聚，步长为2。注意这一操作将会把输入数据中75%的激活数据丢弃（因为对宽度和高度都进行了2的降采样）。另一个不那么常用的设置是使用3x3的感受野，步长为2。最大值汇聚的感受野尺寸很少有超过3的，因为汇聚操作过于激烈，易造成数据信息丢失，这通常会导致算法性能变差。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;qa&quot;&gt;Q&amp;amp;A&lt;/h1&gt;
&lt;h2 id=&quot;减少尺寸设置的问题&quot;&gt;减少尺寸设置的问题&lt;/h2&gt;
&lt;p&gt;上文中展示的两种设置是很好的，因为所有的卷积层都能保持其输入数据的空间尺寸，汇聚层只负责对数据体从空间维度进行降采样。如果使用的步长大于1并且不对卷积层的输入数据使用零填充，那么就必须非常仔细地监督输入数据体通过整个卷积神经网络结构的过程，确认所有的步长和滤波器都尺寸互相吻合，卷积神经网络的结构美妙对称地联系在一起。&lt;/p&gt;

&lt;h2 id=&quot;为什么在卷积层使用1的步长&quot;&gt;为什么在卷积层使用1的步长？&lt;/h2&gt;
&lt;p&gt;在实际应用中，更小的步长效果更好。上文也已经提过，步长为1可以让空间维度的降采样全部由汇聚层负责，卷积层只负责对输入数据体的深度进行变换。&lt;/p&gt;

&lt;h2 id=&quot;为何使用零填充&quot;&gt;为何使用零填充？&lt;/h2&gt;
&lt;p&gt;使用零填充除了前面提到的可以让卷积层的输出数据保持和输入数据在空间维度的不变，还可以提高算法性能。如果卷积层值进行卷积而不进行零填充，那么数据体的尺寸就会略微减小，那么图像边缘的信息就会过快地损失掉。&lt;/p&gt;

&lt;h2 id=&quot;因为内存限制所做的妥协&quot;&gt;因为内存限制所做的妥协&lt;/h2&gt;
&lt;p&gt;在某些案例（尤其是早期的卷积神经网络结构）中，基于前面的各种规则，内存的使用量迅速飙升。例如，使用64个尺寸为3x3的滤波器对224x224x3的图像进行卷积，零填充为1，得到的激活数据体尺寸是[224x224x64]。这个数量就是一千万的激活数据，或者就是72MB的内存（每张图就是这么多，激活函数和梯度都是）。因为GPU通常因为内存导致性能瓶颈，所以做出一些妥协是必须的。在实践中，人们倾向于在网络的第一个卷积层做出妥协。例如，可以妥协可能是在第一个卷积层使用步长为2，尺寸为7x7的滤波器（比如在ZFnet中）。在AlexNet中，滤波器的尺寸的11x11，步长为4。&lt;/p&gt;

&lt;h1 id=&quot;参考&quot;&gt;参考&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit&quot;&gt;(知乎) CS231n课程笔记翻译：卷积神经网络笔记&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://cs231n.github.io/convolutional-networks/&quot;&gt;(cs231n) Convolutional Neural Networks (CNNs / ConvNets)&lt;/a&gt;&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><category term="cs231n" /><summary type="html">卷积层 概述和直观介绍 卷积层的参数是有一些可学习的滤波器集合构成的。每个滤波器在空间上（宽度和高度）都比较小，但是深度和输入数据一致。举例来说，卷积神经网络第一层的一个典型的滤波器的尺寸可以是5x5x3（宽高都是5像素，深度是3是因为图像应为颜色通道，所以有3的深度）。在前向传播的时候，让每个滤波器都在输入数据的宽度和高度上滑动（更精确地说是卷积），然后计算整个滤波器和输入数据任一处的内积。当滤波器沿着输入数据的宽度和高度滑过后，会生成一个2维的激活图（activation map），激活图给出了在每个空间位置处滤波器的反应。 局部连接 在处理图像这样的高维度输入时，让每个神经元都与前一层中的所有神经元进行全连接是不现实的。相反，我们让每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的感受野（receptive field），它的尺寸是一个超参数（其实就是滤波器的空间尺寸）。在深度方向上，这个连接的大小总是和输入量的深度相等。需要再次强调的是，我们对待空间维度（宽和高）与深度维度是不同的：连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致。 例1：假设输入数据体尺寸为[32x32x3]（比如CIFAR-10的RGB图像），如果感受野（或滤波器尺寸）是5x5，那么卷积层中的每个神经元会有输入数据体中[5x5x3]区域的权重，共5x5x3=75个权重（还要加一个偏差参数）。注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。 空间排列 上文讲解了卷积层中每个神经元与输入数据体之间的连接方式，但是尚未讨论输出数据体中神经元的数量，以及它们的排列方式。3个超参数控制着输出数据体的尺寸：深度（depth），步长（stride）和零填充（zero-padding）。 输出数据体的深度是一个超参数：它和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西。举例来说，如果第一个卷积层的输入是原始图像，那么在深度维度上的不同神经元将可能被不同方向的边界，或者是颜色斑点激活。我们将这些沿着深度方向排列、感受野相同的神经元集合称为深度列（depth column），也有人使用纤维（fibre）来称呼它们。 在滑动滤波器的时候，必须指定步长。当步长为1，滤波器每次移动1个像素。当步长为2（或者不常用的3，或者更多，这些在实际中很少使用），滤波器滑动时每次移动2个像素。这个操作会让输出数据体在空间上变小。 这个零填充（zero-padding）的尺寸是一个超参数。零填充有一个良好性质，即可以控制输出数据体的空间尺寸（最常用的是用来保持输入数据体在空间上的尺寸，这样输入和输出的宽高都相等）。 参数共享 在卷积层中使用参数共享是用来控制参数的数量。 小结 我们总结一下卷积层的性质： 输入数据体的尺寸为 W1xH1xD1 4个超参数： 滤波器的数量K 滤波器的空间尺寸F 步长S 零填充数量P 输出数据体的尺寸为 W2xH2xD2 在输出数据体中，第d个深度切片，用第d个滤波器和输入数据进行有效卷积运算的结果（使用步长s），最后在加上第d个偏差。 用矩阵乘法实现 卷积运算本质上就是在滤波器和输入数据的局部区域间做点积。卷积层的常用实现方式就是利用这一点，将卷积层的前向传播变成一个巨大的矩阵乘法。 汇聚层 通常，在连续的卷积层之间会周期性地插入一个汇聚层。它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。汇聚层使用MAX操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。 不使用汇聚层 很多人不喜欢汇聚操作，认为可以不使用它。比如在Striving for Simplicity: The All Convolutional Net一文中，提出使用一种只有重复的卷积层组成的结构，抛弃汇聚层。通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为，在训练一个良好的生成模型时，弃用汇聚层也是很重要的。比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）。现在看起来，未来的卷积网络结构中，可能会很少使用甚至不使用汇聚层。 全连接层 在全连接层中，神经元对于前一层中的所有激活数据是全部连接的，这个常规神经网络中一样。它们的激活可以先用矩阵乘法，再加上偏差。更多细节请查看神经网络章节。 把全连接层转化成卷积层 全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在两类层中，神经元都是计算点积，所以它们的函数形式是一样的。因此，将此两者相互转化是可能的： 全连接层转化为卷积层 在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是224x224x3的图像，一系列的卷积层和汇聚层将图像数据变为尺寸为7x7x512的激活数据体（在AlexNet中就是这样，通过使用5个汇聚层来对输入数据进行空间上的降采样，每次尺寸下降一半，所以最终空间尺寸为224/2/2/2/2/2=7）。从这里可以看到，AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。 卷积神经网络的结构 卷积神经网络通常是由三种层构成：卷积层，汇聚层（除非特别说明，一般就是最大值汇聚）和全连接层（简称FC）。ReLU激活函数也应该算是是一层，它逐元素地进行激活函数操作。在本节中将讨论在卷积神经网络中这些层通常是如何组合在一起的。 层的排列规律 卷积神经网络最常见的形式就是将一些卷积层和ReLU层放在一起，其后紧跟汇聚层，然后重复如此直到图像在空间上被缩小到一个足够小的尺寸，在某个地方过渡成成全连接层也较为常见。最后的全连接层得到输出，比如分类评分等。换句话说，最常见的卷积神经网络结构如下： INPUT -&amp;gt; [[CONV -&amp;gt; RELU]*N -&amp;gt; POOL?]*M -&amp;gt; [FC -&amp;gt; RELU]*K -&amp;gt; FC 其中*指的是重复次数，POOL?指的是一个可选的汇聚层。其中N &amp;gt;=0,通常N&amp;lt;=3,M&amp;gt;=0,K&amp;gt;=0,通常K&amp;lt;3。 层的尺寸设置规律 输入层（包含图像的）应该能被2整除很多次。常用数字包括32（比如CIFAR-10），64，96（比如STL-10）或224（比如ImageNet卷积神经网络），384和512。 卷积层应该使用小尺寸滤波器（比如3x3或最多5x5），使用步长1。还有一点非常重要，就是对输入数据进行零填充，这样卷积层就不会改变输入数据在空间维度上的尺寸。 汇聚层负责对输入数据的空间维度进行降采样。最常用的设置是用用2x2感受野的最大值汇聚，步长为2。注意这一操作将会把输入数据中75%的激活数据丢弃（因为对宽度和高度都进行了2的降采样）。另一个不那么常用的设置是使用3x3的感受野，步长为2。最大值汇聚的感受野尺寸很少有超过3的，因为汇聚操作过于激烈，易造成数据信息丢失，这通常会导致算法性能变差。 Q&amp;amp;A 减少尺寸设置的问题 上文中展示的两种设置是很好的，因为所有的卷积层都能保持其输入数据的空间尺寸，汇聚层只负责对数据体从空间维度进行降采样。如果使用的步长大于1并且不对卷积层的输入数据使用零填充，那么就必须非常仔细地监督输入数据体通过整个卷积神经网络结构的过程，确认所有的步长和滤波器都尺寸互相吻合，卷积神经网络的结构美妙对称地联系在一起。 为什么在卷积层使用1的步长？ 在实际应用中，更小的步长效果更好。上文也已经提过，步长为1可以让空间维度的降采样全部由汇聚层负责，卷积层只负责对输入数据体的深度进行变换。 为何使用零填充？ 使用零填充除了前面提到的可以让卷积层的输出数据保持和输入数据在空间维度的不变，还可以提高算法性能。如果卷积层值进行卷积而不进行零填充，那么数据体的尺寸就会略微减小，那么图像边缘的信息就会过快地损失掉。 因为内存限制所做的妥协 在某些案例（尤其是早期的卷积神经网络结构）中，基于前面的各种规则，内存的使用量迅速飙升。例如，使用64个尺寸为3x3的滤波器对224x224x3的图像进行卷积，零填充为1，得到的激活数据体尺寸是[224x224x64]。这个数量就是一千万的激活数据，或者就是72MB的内存（每张图就是这么多，激活函数和梯度都是）。因为GPU通常因为内存导致性能瓶颈，所以做出一些妥协是必须的。在实践中，人们倾向于在网络的第一个卷积层做出妥协。例如，可以妥协可能是在第一个卷积层使用步长为2，尺寸为7x7的滤波器（比如在ZFnet中）。在AlexNet中，滤波器的尺寸的11x11，步长为4。 参考 (知乎) CS231n课程笔记翻译：卷积神经网络笔记 (cs231n) Convolutional Neural Networks (CNNs / ConvNets)</summary></entry><entry><title type="html">房思琪的初恋乐园</title><link href="http://localhost:4000/2020/05/20/%E6%88%BF%E6%80%9D%E7%90%AA%E7%9A%84%E5%88%9D%E6%81%8B%E4%B9%90%E5%9B%AD.html" rel="alternate" type="text/html" title="房思琪的初恋乐园" /><published>2020-05-20T00:00:00+09:00</published><updated>2020-05-20T00:00:00+09:00</updated><id>http://localhost:4000/2020/05/20/%E6%88%BF%E6%80%9D%E7%90%AA%E7%9A%84%E5%88%9D%E6%81%8B%E4%B9%90%E5%9B%AD</id><content type="html" xml:base="http://localhost:4000/2020/05/20/%E6%88%BF%E6%80%9D%E7%90%AA%E7%9A%84%E5%88%9D%E6%81%8B%E4%B9%90%E5%9B%AD.html">&lt;h1 id=&quot;书评&quot;&gt;书评&lt;/h1&gt;
&lt;h2 id=&quot;2020年05月20日-160729&quot;&gt;2020年05月20日 16:07:29&lt;/h2&gt;
&lt;p&gt;这本书读起来太吃力了，感觉心中承受了很大的压力与痛苦。&lt;/p&gt;

&lt;p&gt;仿佛我变成了李国华，曾经干过很过分的事情，对一个房思琪和很多人。&lt;br /&gt;
一瞬间我又变成了房思琪，看着伊纹姐承受暴力，又被憧憬的李老师以“爱”为名义肆意蹂躏。&lt;/p&gt;

&lt;p&gt;作者的文笔太好了，一个句子反反复复读几遍才能明白其中深意。&lt;br /&gt;
有的时候觉得脑子跟不上作者的文笔了。&lt;/p&gt;

&lt;p&gt;拉回现实的时候喘口气，还好只是小说。&lt;br /&gt;
我多么幸运，幸福。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="读后感" /><summary type="html">书评 2020年05月20日 16:07:29 这本书读起来太吃力了，感觉心中承受了很大的压力与痛苦。 仿佛我变成了李国华，曾经干过很过分的事情，对一个房思琪和很多人。 一瞬间我又变成了房思琪，看着伊纹姐承受暴力，又被憧憬的李老师以“爱”为名义肆意蹂躏。 作者的文笔太好了，一个句子反反复复读几遍才能明白其中深意。 有的时候觉得脑子跟不上作者的文笔了。 拉回现实的时候喘口气，还好只是小说。 我多么幸运，幸福。</summary></entry><entry><title type="html">深度学习用于计算机视觉</title><link href="http://localhost:4000/2020/05/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89.html" rel="alternate" type="text/html" title="深度学习用于计算机视觉" /><published>2020-05-18T00:00:00+09:00</published><updated>2020-05-18T00:00:00+09:00</updated><id>http://localhost:4000/2020/05/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89</id><content type="html" xml:base="http://localhost:4000/2020/05/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89.html">&lt;h1 id=&quot;卷积神经网络简介&quot;&gt;卷积神经网络简介&lt;/h1&gt;
&lt;h2 id=&quot;卷积运算&quot;&gt;卷积运算&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;密集连层：从输入特征空间中学到的是全局模式&lt;/li&gt;
  &lt;li&gt;卷积层：局部模式&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;卷积神经网络具有以下两个特性：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;学到的模式具有 &lt;strong&gt;平移不变性&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;可以学到 &lt;strong&gt;模式的空间层次结构&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;卷积由以下两个关键参数定义：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;从输入中提取的图块尺寸&lt;/li&gt;
  &lt;li&gt;输出特征图的深度：卷积所计算的过滤器的数量。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;输出的宽度和高度可能与输入的不同，原因：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;边界效应，可通过对输入特征图进行填充来抵消&lt;/li&gt;
  &lt;li&gt;使用了 &lt;strong&gt;步幅&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;最大池化运算&quot;&gt;最大池化运算&lt;/h2&gt;
&lt;p&gt;作用：对特征图进行下采样，与步进卷积类似。 &lt;br /&gt;
原因：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;减少需要处理的特征图的元素个数&lt;/li&gt;
  &lt;li&gt;通过让连续卷积层的观察窗口越来越大从而引入空间过滤器的层级结构&lt;/li&gt;
&lt;/ul&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><category term="Python深度学习" /><summary type="html">卷积神经网络简介 卷积运算 密集连层：从输入特征空间中学到的是全局模式 卷积层：局部模式 卷积神经网络具有以下两个特性： 学到的模式具有 平移不变性 可以学到 模式的空间层次结构 卷积由以下两个关键参数定义： 从输入中提取的图块尺寸 输出特征图的深度：卷积所计算的过滤器的数量。 输出的宽度和高度可能与输入的不同，原因： 边界效应，可通过对输入特征图进行填充来抵消 使用了 步幅 最大池化运算 作用：对特征图进行下采样，与步进卷积类似。 原因： 减少需要处理的特征图的元素个数 通过让连续卷积层的观察窗口越来越大从而引入空间过滤器的层级结构</summary></entry><entry><title type="html">你当像鸟飞向你的山</title><link href="http://localhost:4000/2020/04/25/%E4%BD%A0%E5%BD%93%E5%83%8F%E9%B8%9F%E9%A3%9E%E5%90%91%E4%BD%A0%E7%9A%84%E5%B1%B1.html" rel="alternate" type="text/html" title="你当像鸟飞向你的山" /><published>2020-04-25T00:00:00+09:00</published><updated>2020-04-25T00:00:00+09:00</updated><id>http://localhost:4000/2020/04/25/%E4%BD%A0%E5%BD%93%E5%83%8F%E9%B8%9F%E9%A3%9E%E5%90%91%E4%BD%A0%E7%9A%84%E5%B1%B1</id><content type="html" xml:base="http://localhost:4000/2020/04/25/%E4%BD%A0%E5%BD%93%E5%83%8F%E9%B8%9F%E9%A3%9E%E5%90%91%E4%BD%A0%E7%9A%84%E5%B1%B1.html"></content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="读后感" /><summary type="html"></summary></entry><entry><title type="html">陈凯旭，生日快乐！</title><link href="http://localhost:4000/2020/04/12/%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90.html" rel="alternate" type="text/html" title="陈凯旭，生日快乐！" /><published>2020-04-12T00:00:00+09:00</published><updated>2020-04-12T00:00:00+09:00</updated><id>http://localhost:4000/2020/04/12/%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90</id><content type="html" xml:base="http://localhost:4000/2020/04/12/%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90.html">&lt;p&gt;今年25岁，时间过得好快呀。&lt;/p&gt;

&lt;p&gt;过去的一岁，干过一些蠢事，受过一些伤痛。&lt;br /&gt;
但也是成长最快的一岁，让我明白了一些道理。&lt;/p&gt;

&lt;p&gt;祝我生日快乐。&lt;br /&gt;
加油生活。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="致自己" /><summary type="html">今年25岁，时间过得好快呀。 过去的一岁，干过一些蠢事，受过一些伤痛。 但也是成长最快的一岁，让我明白了一些道理。 祝我生日快乐。 加油生活。</summary></entry><entry><title type="html">致命女人</title><link href="http://localhost:4000/2020/04/12/%E8%87%B4%E5%91%BD%E5%A5%B3%E4%BA%BA.html" rel="alternate" type="text/html" title="致命女人" /><published>2020-04-12T00:00:00+09:00</published><updated>2020-04-12T00:00:00+09:00</updated><id>http://localhost:4000/2020/04/12/%E8%87%B4%E5%91%BD%E5%A5%B3%E4%BA%BA</id><content type="html" xml:base="http://localhost:4000/2020/04/12/%E8%87%B4%E5%91%BD%E5%A5%B3%E4%BA%BA.html">&lt;p&gt;之前在B站上看过这个美剧的短评，特意来看完全剧。&lt;/p&gt;

&lt;p&gt;编剧真的很厉害，影片拍摄手法也很喜欢。&lt;br /&gt;
三条故事线虽然发生在不同年代，却都发生在一个房子里。&lt;br /&gt;
并且前后主人都有会面。&lt;/p&gt;

&lt;p&gt;随着时间的推进，女性的地位显而易见的得到提高，看待婚姻的态度也不一样了。&lt;/p&gt;

&lt;p&gt;不过发生故事的房子就是一个凶宅啊，被下诅咒了吧。&lt;/p&gt;

&lt;p&gt;面对感情上的问题要保证足够的理智与冷静，千万不能冲动行事。&lt;br /&gt;
希望我永远都别碰到绿茶，真的玩不过。&lt;/p&gt;

&lt;p&gt;最后，千万别得罪女人。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="观后感" /><summary type="html">之前在B站上看过这个美剧的短评，特意来看完全剧。 编剧真的很厉害，影片拍摄手法也很喜欢。 三条故事线虽然发生在不同年代，却都发生在一个房子里。 并且前后主人都有会面。 随着时间的推进，女性的地位显而易见的得到提高，看待婚姻的态度也不一样了。 不过发生故事的房子就是一个凶宅啊，被下诅咒了吧。 面对感情上的问题要保证足够的理智与冷静，千万不能冲动行事。 希望我永远都别碰到绿茶，真的玩不过。 最后，千万别得罪女人。</summary></entry><entry xml:lang="zh"><title type="html">每日一笔</title><link href="http://localhost:4000/2020/04/04/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94.html" rel="alternate" type="text/html" title="每日一笔" /><published>2020-04-04T00:00:00+09:00</published><updated>2020-04-04T00:00:00+09:00</updated><id>http://localhost:4000/2020/04/04/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94</id><content type="html" xml:base="http://localhost:4000/2020/04/04/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94.html">&lt;h2 id=&quot;每日一笔记录生活&quot;&gt;每日一笔，记录生活&lt;/h2&gt;
&lt;h3 id=&quot;2020年04月04日-232648&quot;&gt;2020年04月04日 23:26:48&lt;/h3&gt;
&lt;p&gt;四月生活开始，天气也变得越来越暖和。&lt;br /&gt;
不过金泽偶尔下的小雨依旧很烦人。&lt;br /&gt;
今天清明节，国内举行全国默哀。&lt;br /&gt;
希望疫情能快点结束。 &lt;br /&gt;
所有事情变得越来越好。&lt;/p&gt;

&lt;p&gt;打工的时候脑子里瞎想。&lt;br /&gt;
我这个人还是一直比较相信因果的。&lt;br /&gt;
正所谓种什么因的什么果。感情上也是一样的。&lt;br /&gt;
希望如此吧，哈哈。&lt;/p&gt;

&lt;p&gt;四月是樱花开放的季节，是时候拿着我新买的相机出去拍樱花了呀。&lt;/p&gt;

&lt;h3 id=&quot;2020年04月14日-013719&quot;&gt;2020年04月14日 01:37:19&lt;/h3&gt;
&lt;p&gt;幸福都是别人的，努力才是自己的。&lt;/p&gt;

&lt;p&gt;干了这碗毒鸡汤。&lt;/p&gt;

&lt;p&gt;电子海洛因真上头。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="每日一笔" /><summary type="html">每日一笔，记录生活 2020年04月04日 23:26:48 四月生活开始，天气也变得越来越暖和。 不过金泽偶尔下的小雨依旧很烦人。 今天清明节，国内举行全国默哀。 希望疫情能快点结束。 所有事情变得越来越好。 打工的时候脑子里瞎想。 我这个人还是一直比较相信因果的。 正所谓种什么因的什么果。感情上也是一样的。 希望如此吧，哈哈。 四月是樱花开放的季节，是时候拿着我新买的相机出去拍樱花了呀。 2020年04月14日 01:37:19 幸福都是别人的，努力才是自己的。 干了这碗毒鸡汤。 电子海洛因真上头。</summary></entry><entry><title type="html">我来金泽这一年</title><link href="http://localhost:4000/2020/03/31/%E6%88%91%E6%9D%A5%E9%87%91%E6%B3%BD%E8%BF%99%E4%B8%80%E5%B9%B4.html" rel="alternate" type="text/html" title="我来金泽这一年" /><published>2020-03-31T00:00:00+09:00</published><updated>2020-03-31T00:00:00+09:00</updated><id>http://localhost:4000/2020/03/31/%E6%88%91%E6%9D%A5%E9%87%91%E6%B3%BD%E8%BF%99%E4%B8%80%E5%B9%B4</id><content type="html" xml:base="http://localhost:4000/2020/03/31/%E6%88%91%E6%9D%A5%E9%87%91%E6%B3%BD%E8%BF%99%E4%B8%80%E5%B9%B4.html">&lt;p&gt;看看日历，不知不觉我到日本已经有一年的时间了。&lt;/p&gt;

&lt;p&gt;恍惚还记得去年的这个时间，我应该刚下飞机，提着我的两个箱子住进了大阪国际机场里的FirstCarbon胶囊旅馆。&lt;br /&gt;
也还记得当时坐在狭小的胶囊旅馆中拿出我的小电脑在印象笔记上写下了来日本后的第一篇日记。&lt;br /&gt;
哈哈，这一切都仿佛是在昨天发生的一样，现在还记忆深刻。&lt;br /&gt;
可惜这一切确实是发生在过去的，留下的也只是记忆罢了。&lt;/p&gt;

&lt;p&gt;也还记得第一次见赵哥的时候，觉得他像个越南人，哈哈。&lt;br /&gt;
第一次见游哥的时候是在来学校的bus上，给我记忆最深刻的场景就是游哥抱着ps4走过我面前，那个时候我就知道这又是一个资深宅男了。哈哈。&lt;/p&gt;

&lt;p&gt;那个时候还遇到了越南的小姐姐ジェンさん，我在大巴上坐在她旁边。&lt;br /&gt;
她应该是我来日本之后认识的第一个人吧，一开始还以为是个中国人，脑海里还YY了各种场景才去搭讪的，没想到聊过之后是个越南人，哈哈，真想捂脸🤦‍。&lt;br /&gt;
长得也太像国人了，不过以我的审美来看还是很漂亮的，就算是越南人也值了。&lt;br /&gt;
可惜了，哈哈。&lt;/p&gt;

&lt;p&gt;之后也认识了猪哥，并且四个人组成了北陆四君子。&lt;br /&gt;
真的是每天在一起就知道搞哲学，还好我意志比较坚定，哈哈。&lt;br /&gt;
在之后不知道什么时候认识的平老哥，第一印象的话感觉很成熟一人，后来才知道原来成熟的原因是在岁数上吧。&lt;br /&gt;
虽然平老哥的心态很能保持童真，平时聊天也完全不会感到有代沟。&lt;/p&gt;

&lt;p&gt;然后就到十月份的时候研究室新来了一个研究生——高姐姐。&lt;br /&gt;
第一次和比我高的女生玩的比较熟，有的时候走在她身边真的感觉自己压力山大。&lt;br /&gt;
对于研究室新来的这个研究生，我也算是尽了我这半个学长的责任——把所有的复习资料都给她了。&lt;br /&gt;
以至于后来别的研究室的研究生来找我要题的时候我也只是让他们拍照，不给原件。&lt;br /&gt;
毕竟肥水不流外人田，哈哈。&lt;/p&gt;

&lt;p&gt;虽然最后她的成绩不尽人意，现在已经回国了。但我对她投入的期望还是蛮大的。&lt;br /&gt;
不过人各有志，也许追求的东西不一样吧。&lt;br /&gt;
我相信她以后在别的学校还能继续发光发采吧。我一直觉得她挺优秀的。&lt;/p&gt;

&lt;p&gt;这一年于我而言，对我自身的成长不可谓不大。 &lt;br /&gt;
我一直觉得我的人生需要体验不同的事情，不管是好的坏的，这对我而说都是宝贵的经验。&lt;/p&gt;

&lt;p&gt;但没想到有些东西自己是不是真的能经受的住。&lt;br /&gt;
这一年，艰难的时候也是真的难，甚至现在都感到后悔。&lt;br /&gt;
但是忘了从哪里看到的一句话了。当你觉得生活过得艰难的时候，一定就是成长最快的时候。&lt;/p&gt;

&lt;p&gt;以前从来没有考虑过的人际交往在这一年中变得格外的艰难，心中对朋友有所愧疚，甚至还得罪了与我生活并不怎么会产生交集的一个女生。&lt;br /&gt;
也曾经处在八卦的中心，成为别人茶余饭后的谈资。&lt;/p&gt;

&lt;p&gt;我自认我的性格八面玲珑，不会刻意的讨好什么人，也不会得罪什么人。&lt;br /&gt;
我也不会记仇报复，也不会装可怜博取同情。&lt;br /&gt;
周围的朋友能聚在一起都是真心相处的，在人际交往中也是认真相处，从来没有算计过什么得失。&lt;br /&gt;
但现实告诉我，我还是年少轻狂，在处理一些事情上面还是太冲动了。&lt;br /&gt;
这样的后果就是自食其果。而且这样的影响估计要陪伴我在这里的两年了吧。&lt;br /&gt;
不过经过这次事情之后也是深刻的认识到，在冲动的时候不要做任何事情，自己一个人呆着就是最理智的决定。&lt;/p&gt;

&lt;p&gt;有这么一段时间，变得不像自己。做了一些自己不会做的事情，自己的底线也一次次被打破。&lt;br /&gt;
最后仿佛破罐子破摔一样放弃了自己坚守的东西。&lt;br /&gt;
这样不好，不管什么时候，都不应该改变自己。应该一直坚持自己所坚持的事情，做自己想做的事情。&lt;br /&gt;
而不是去附和别人的兴趣爱好，这样只是自私的觉得自己真正的“付出了”。&lt;/p&gt;

&lt;p&gt;我想现在的状态只适合一个人，有太多情绪需要自己整理清楚了，也有太多事情需要一个人去做了。&lt;br /&gt;
当初和张旭分手的时候我就没有想好是否还会再谈恋爱。&lt;br /&gt;
当然也害怕自己单身，也很羡慕恩爱的情侣成双入对。&lt;br /&gt;
但我目前的状态根本没办法对一个人好，像以前一样。&lt;br /&gt;
从来没想到会这么快的开始，然后以这么尴尬的结局收尾。&lt;/p&gt;

&lt;p&gt;当然这一年开心的事情也有很多。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;顺利的在金泽大学入学，这一点真的很值得我高兴，也为我付出的努力得到了认可。&lt;/li&gt;
  &lt;li&gt;教授不错，对我们都很好。&lt;/li&gt;
  &lt;li&gt;还和朋友去了很多地方旅游，虽然有些地方去过了，但还是值得高兴。&lt;/li&gt;
  &lt;li&gt;电子产品买了不少，新的兴趣爱好也增加了。&lt;/li&gt;
  &lt;li&gt;买了酷酷的山地车，离骑行的距离又近了一步。&lt;/li&gt;
  &lt;li&gt;搬到了方便的地方住，还顺利的找到了打工。&lt;/li&gt;
  &lt;li&gt;认识了很多新朋友，至少自己不孤单了。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;不过这一年，学习的时间少，更多的时候都是在无所事事瞎玩。&lt;br /&gt;
现在也处在迷茫的状态中，不知道应该怎么学习，用什么方法学习比较好。&lt;/p&gt;

&lt;p&gt;明天就相当于是新的在日本新的一年啦。&lt;br /&gt;
希望在这一年里，我能实现自己心中计划的内容，然后可以好好的研究。&lt;br /&gt;
不荒废时间，努力锻炼身体。&lt;br /&gt;
首先恢复以前的生活，然后接着自律起来。&lt;br /&gt;
新的兴趣爱好希望可以坚持下去，我不算有天分，唯一有的也只有那份努力的坚持。&lt;/p&gt;

&lt;p&gt;翻了翻日记，还能看到去年的这一天我在日记本上写下：&lt;br /&gt;
只愿我以后的生活能够一切顺利，也想我能够前途光明。&lt;br /&gt;
不忘初心，方得始终。&lt;/p&gt;

&lt;p&gt;希望接下来的时间，更幸运，也更幸福。&lt;/p&gt;

&lt;p&gt;下面就是找出来的日记啦。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;再一次到达日本——关西机场之夜&lt;/p&gt;

  &lt;p&gt;终于又一次抵达了日本，喝上了心心念的的伊藤园乌龙茶。&lt;br /&gt;
现在时间是2019年3月31日19:08:54，今天一天时间都是在路上度过的。还好昨天睡得比较早，现在虽然感觉有一些疲惫但还算是比较有精神的。&lt;br /&gt;
这几天一直在路上奔波，刚才微信安全中心提示我，我在短短一周之内在三个不同的地方登陆过微信，看来以为是我的账号出现什么意外了吧。&lt;br /&gt;
这次进入日本唯一担心的还是我带的那些药，不过过关的时候比我想象的还要简单很多，行李也只打开了小的那个查看了一下是不是有需要缴税的物品。看来这种药品也不是什么违禁药物，根本不会过多询问吧。&lt;br /&gt;
今天在机场里面地铁口问路的时候，说了半天日语结果对面是个中国人让我说中文。一下子就感觉很尴尬，不过中文当然说起来还是比较顺口的。&lt;br /&gt;
抽时间去看了看明天坐巴士的地方，应该是能正确的找到巴士了，标志什么的还是挺明显的。&lt;br /&gt;
今天新东方的老师休息，本来想让他再给我发一份关于巴士相关的说明，结果并没有回复我。最后还是自己在邮箱里面找到了巴士的信息，不过说回来这本来也是我自己的事情。&lt;br /&gt;
刚才去洗了个澡，然后在自动贩卖机上买了瓶饮料。自动贩卖机上的东西好贵啊。&lt;br /&gt;
胶囊旅馆一切都挺好的，但是唯一一点需要吐槽的地方就是实在是太热了。还是因为地方太小了，虽然一直开着空调（其实只是换气罢了），穿着半袖，但还是感觉非常的闷热。空气不流通也没什么办法，只能将就的睡一晚上了。主要还是价格便宜，然后位置也比较好。&lt;br /&gt;
到了日本之后终于可以用谷歌而不用再用百度了。刚才试了一下，现在的小米mix3是可以直接使用谷歌框架的，折腾了一番之后下了一个tiktok，抖音国际版。上来看一看日本的小姐姐，果然不管在什么地方大家拍摄的内容都是差不多的啊。日本的小姐姐真可爱。在日本比较难受的就是哔哩哔哩和大部分音乐软件都不能使用了。这样以后听歌看番剧都挺麻烦的，主要是没有弹幕看了，不能再看大家吐槽了。&lt;br /&gt;
明天应该就能顺利到学校了，不过坐巴士还要做5，6个小时。学校的位置距离机场还挺远的，以后回家也不会太方便了吧。希望明天住的宿舍条件能好一点，毕竟每个月的房租也很贵。到了之后也应该把笔记本买上了，虽然这个小笔记本目前用的感觉足够。但是还是太小了，不是太方便。&lt;br /&gt;
后天去见导师，现在的日语水平也还是很烂，也就只能在日常生活中简单交流使用了。&lt;br /&gt;
作为第一篇再一次到日本写的日记，零零碎碎的写了一些，希望一起顺利。&lt;/p&gt;

  &lt;p&gt;2019年3月31日23:30:21    记载于关西国际机场胶囊旅馆中&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="日记" /><summary type="html">看看日历，不知不觉我到日本已经有一年的时间了。 恍惚还记得去年的这个时间，我应该刚下飞机，提着我的两个箱子住进了大阪国际机场里的FirstCarbon胶囊旅馆。 也还记得当时坐在狭小的胶囊旅馆中拿出我的小电脑在印象笔记上写下了来日本后的第一篇日记。 哈哈，这一切都仿佛是在昨天发生的一样，现在还记忆深刻。 可惜这一切确实是发生在过去的，留下的也只是记忆罢了。 也还记得第一次见赵哥的时候，觉得他像个越南人，哈哈。 第一次见游哥的时候是在来学校的bus上，给我记忆最深刻的场景就是游哥抱着ps4走过我面前，那个时候我就知道这又是一个资深宅男了。哈哈。 那个时候还遇到了越南的小姐姐ジェンさん，我在大巴上坐在她旁边。 她应该是我来日本之后认识的第一个人吧，一开始还以为是个中国人，脑海里还YY了各种场景才去搭讪的，没想到聊过之后是个越南人，哈哈，真想捂脸🤦‍。 长得也太像国人了，不过以我的审美来看还是很漂亮的，就算是越南人也值了。 可惜了，哈哈。 之后也认识了猪哥，并且四个人组成了北陆四君子。 真的是每天在一起就知道搞哲学，还好我意志比较坚定，哈哈。 在之后不知道什么时候认识的平老哥，第一印象的话感觉很成熟一人，后来才知道原来成熟的原因是在岁数上吧。 虽然平老哥的心态很能保持童真，平时聊天也完全不会感到有代沟。 然后就到十月份的时候研究室新来了一个研究生——高姐姐。 第一次和比我高的女生玩的比较熟，有的时候走在她身边真的感觉自己压力山大。 对于研究室新来的这个研究生，我也算是尽了我这半个学长的责任——把所有的复习资料都给她了。 以至于后来别的研究室的研究生来找我要题的时候我也只是让他们拍照，不给原件。 毕竟肥水不流外人田，哈哈。 虽然最后她的成绩不尽人意，现在已经回国了。但我对她投入的期望还是蛮大的。 不过人各有志，也许追求的东西不一样吧。 我相信她以后在别的学校还能继续发光发采吧。我一直觉得她挺优秀的。 这一年于我而言，对我自身的成长不可谓不大。 我一直觉得我的人生需要体验不同的事情，不管是好的坏的，这对我而说都是宝贵的经验。 但没想到有些东西自己是不是真的能经受的住。 这一年，艰难的时候也是真的难，甚至现在都感到后悔。 但是忘了从哪里看到的一句话了。当你觉得生活过得艰难的时候，一定就是成长最快的时候。 以前从来没有考虑过的人际交往在这一年中变得格外的艰难，心中对朋友有所愧疚，甚至还得罪了与我生活并不怎么会产生交集的一个女生。 也曾经处在八卦的中心，成为别人茶余饭后的谈资。 我自认我的性格八面玲珑，不会刻意的讨好什么人，也不会得罪什么人。 我也不会记仇报复，也不会装可怜博取同情。 周围的朋友能聚在一起都是真心相处的，在人际交往中也是认真相处，从来没有算计过什么得失。 但现实告诉我，我还是年少轻狂，在处理一些事情上面还是太冲动了。 这样的后果就是自食其果。而且这样的影响估计要陪伴我在这里的两年了吧。 不过经过这次事情之后也是深刻的认识到，在冲动的时候不要做任何事情，自己一个人呆着就是最理智的决定。 有这么一段时间，变得不像自己。做了一些自己不会做的事情，自己的底线也一次次被打破。 最后仿佛破罐子破摔一样放弃了自己坚守的东西。 这样不好，不管什么时候，都不应该改变自己。应该一直坚持自己所坚持的事情，做自己想做的事情。 而不是去附和别人的兴趣爱好，这样只是自私的觉得自己真正的“付出了”。 我想现在的状态只适合一个人，有太多情绪需要自己整理清楚了，也有太多事情需要一个人去做了。 当初和张旭分手的时候我就没有想好是否还会再谈恋爱。 当然也害怕自己单身，也很羡慕恩爱的情侣成双入对。 但我目前的状态根本没办法对一个人好，像以前一样。 从来没想到会这么快的开始，然后以这么尴尬的结局收尾。 当然这一年开心的事情也有很多。 顺利的在金泽大学入学，这一点真的很值得我高兴，也为我付出的努力得到了认可。 教授不错，对我们都很好。 还和朋友去了很多地方旅游，虽然有些地方去过了，但还是值得高兴。 电子产品买了不少，新的兴趣爱好也增加了。 买了酷酷的山地车，离骑行的距离又近了一步。 搬到了方便的地方住，还顺利的找到了打工。 认识了很多新朋友，至少自己不孤单了。 不过这一年，学习的时间少，更多的时候都是在无所事事瞎玩。 现在也处在迷茫的状态中，不知道应该怎么学习，用什么方法学习比较好。 明天就相当于是新的在日本新的一年啦。 希望在这一年里，我能实现自己心中计划的内容，然后可以好好的研究。 不荒废时间，努力锻炼身体。 首先恢复以前的生活，然后接着自律起来。 新的兴趣爱好希望可以坚持下去，我不算有天分，唯一有的也只有那份努力的坚持。 翻了翻日记，还能看到去年的这一天我在日记本上写下： 只愿我以后的生活能够一切顺利，也想我能够前途光明。 不忘初心，方得始终。 希望接下来的时间，更幸运，也更幸福。 下面就是找出来的日记啦。 再一次到达日本——关西机场之夜 终于又一次抵达了日本，喝上了心心念的的伊藤园乌龙茶。 现在时间是2019年3月31日19:08:54，今天一天时间都是在路上度过的。还好昨天睡得比较早，现在虽然感觉有一些疲惫但还算是比较有精神的。 这几天一直在路上奔波，刚才微信安全中心提示我，我在短短一周之内在三个不同的地方登陆过微信，看来以为是我的账号出现什么意外了吧。 这次进入日本唯一担心的还是我带的那些药，不过过关的时候比我想象的还要简单很多，行李也只打开了小的那个查看了一下是不是有需要缴税的物品。看来这种药品也不是什么违禁药物，根本不会过多询问吧。 今天在机场里面地铁口问路的时候，说了半天日语结果对面是个中国人让我说中文。一下子就感觉很尴尬，不过中文当然说起来还是比较顺口的。 抽时间去看了看明天坐巴士的地方，应该是能正确的找到巴士了，标志什么的还是挺明显的。 今天新东方的老师休息，本来想让他再给我发一份关于巴士相关的说明，结果并没有回复我。最后还是自己在邮箱里面找到了巴士的信息，不过说回来这本来也是我自己的事情。 刚才去洗了个澡，然后在自动贩卖机上买了瓶饮料。自动贩卖机上的东西好贵啊。 胶囊旅馆一切都挺好的，但是唯一一点需要吐槽的地方就是实在是太热了。还是因为地方太小了，虽然一直开着空调（其实只是换气罢了），穿着半袖，但还是感觉非常的闷热。空气不流通也没什么办法，只能将就的睡一晚上了。主要还是价格便宜，然后位置也比较好。 到了日本之后终于可以用谷歌而不用再用百度了。刚才试了一下，现在的小米mix3是可以直接使用谷歌框架的，折腾了一番之后下了一个tiktok，抖音国际版。上来看一看日本的小姐姐，果然不管在什么地方大家拍摄的内容都是差不多的啊。日本的小姐姐真可爱。在日本比较难受的就是哔哩哔哩和大部分音乐软件都不能使用了。这样以后听歌看番剧都挺麻烦的，主要是没有弹幕看了，不能再看大家吐槽了。 明天应该就能顺利到学校了，不过坐巴士还要做5，6个小时。学校的位置距离机场还挺远的，以后回家也不会太方便了吧。希望明天住的宿舍条件能好一点，毕竟每个月的房租也很贵。到了之后也应该把笔记本买上了，虽然这个小笔记本目前用的感觉足够。但是还是太小了，不是太方便。 后天去见导师，现在的日语水平也还是很烂，也就只能在日常生活中简单交流使用了。 作为第一篇再一次到日本写的日记，零零碎碎的写了一些，希望一起顺利。 2019年3月31日23:30:21 记载于关西国际机场胶囊旅馆中</summary></entry><entry><title type="html">机器学习基础</title><link href="http://localhost:4000/2020/03/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.html" rel="alternate" type="text/html" title="机器学习基础" /><published>2020-03-13T00:00:00+09:00</published><updated>2020-03-13T00:00:00+09:00</updated><id>http://localhost:4000/2020/03/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80</id><content type="html" xml:base="http://localhost:4000/2020/03/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.html">&lt;h1 id=&quot;机器学习的四个分支&quot;&gt;机器学习的四个分支&lt;/h1&gt;
&lt;h2 id=&quot;监督学习&quot;&gt;监督学习&lt;/h2&gt;
&lt;p&gt;监督学习是目前最常见的机器学习类型。&lt;br /&gt;
变体：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;序列生成：给定一张图像，预测描述图像的文字。&lt;/li&gt;
  &lt;li&gt;语法树预测：给定一个句子，预测其分解生成的语法树。&lt;/li&gt;
  &lt;li&gt;目标检测：给定一张图像，在图中特定目标的周围画一个边界框。&lt;/li&gt;
  &lt;li&gt;图像分割：给定一张图像，在特定物体上画一个象素级的mask。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;无监督学习&quot;&gt;无监督学习&lt;/h2&gt;
&lt;p&gt;是指在没有目标的情况下寻找输入数据的有趣变换。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;降维&lt;/li&gt;
  &lt;li&gt;聚类&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;自监督学习&quot;&gt;自监督学习&lt;/h2&gt;
&lt;p&gt;没有人类参与的监督学习。&lt;br /&gt;
标签是从输入数据中生成的，通常是使用启发式算法生成的。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;自编码器&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;强化学习&quot;&gt;强化学习&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;智能体 
还没有工业化，未来？&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;评估机器学习模型&quot;&gt;评估机器学习模型&lt;/h1&gt;
&lt;h2 id=&quot;训练集验证集和测试集&quot;&gt;训练集，验证集和测试集&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;超参数–选择层数或每层的的大小&lt;/li&gt;
  &lt;li&gt;参数–权重&lt;/li&gt;
  &lt;li&gt;信息泄露&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果数据很少：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;简单的留出验证&lt;/li&gt;
  &lt;li&gt;K折验证&lt;/li&gt;
  &lt;li&gt;重复K折验证&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;简单的留出验证&quot;&gt;简单的留出验证&lt;/h3&gt;
&lt;p&gt;留出一定比例的数据作为测试集。&lt;/p&gt;

&lt;h3 id=&quot;k折验证&quot;&gt;k折验证&lt;/h3&gt;
&lt;p&gt;将数据划分为大小相同的k个分区。&lt;/p&gt;

&lt;h3 id=&quot;带有打乱数据的重复k折验证&quot;&gt;带有打乱数据的重复k折验证&lt;/h3&gt;
&lt;p&gt;计算代价很大&lt;/p&gt;

&lt;h2 id=&quot;评估模型的注意事项&quot;&gt;评估模型的注意事项&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;数据代表性&lt;/li&gt;
  &lt;li&gt;时间箭头&lt;/li&gt;
  &lt;li&gt;数据冗余&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;数据预处理特征工程和特征学习&quot;&gt;数据预处理，特征工程和特征学习&lt;/h1&gt;
&lt;h2 id=&quot;神经网络的数据预处理&quot;&gt;神经网络的数据预处理&lt;/h2&gt;
&lt;p&gt;目的是使原始数据更适于用神经网络处理&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;向量化&lt;/li&gt;
  &lt;li&gt;值标准化&lt;br /&gt;
  输入数据应具以下特征：
    &lt;ul&gt;
      &lt;li&gt;取值较小&lt;/li&gt;
      &lt;li&gt;同质性&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;处理缺失值&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;特征工程&quot;&gt;特征工程&lt;/h2&gt;
&lt;p&gt;用更简单的方式表述问题，从而使问题变得很容易。&lt;/p&gt;

&lt;h1 id=&quot;过拟合与欠拟合&quot;&gt;过拟合与欠拟合&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;优化&lt;/li&gt;
  &lt;li&gt;泛化&lt;/li&gt;
  &lt;li&gt;正则化: 降低过拟合的方法&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;正则化方法：&lt;/p&gt;
&lt;h2 id=&quot;减小网络大小&quot;&gt;减小网络大小&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;容量&lt;/strong&gt;：模型中可学习参数的个数&lt;/p&gt;
&lt;h2 id=&quot;添加权重正则化&quot;&gt;添加权重正则化&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;奥卡姆剃刀原理&lt;br /&gt;
一种常见的降低过拟合的方法：强制使模型权重只能选择较小的值，从而限制模型的复杂度，使得分布更加的规则。&lt;br /&gt;
实现方法是向网络损失函数中添加与较大权重值相关的成本。&lt;br /&gt;
两种形式：&lt;/li&gt;
  &lt;li&gt;L1正则化：成本与权重系数的绝对值成正比&lt;/li&gt;
  &lt;li&gt;L2正则化（权重衰减）：成本与权重系数的平方成正比
    &lt;h2 id=&quot;添加dropout正则化&quot;&gt;添加dropout正则化&lt;/h2&gt;
    &lt;p&gt;对某一层使用，就是在训练过程中随机将该层的一些输出特征舍弃（设置为0）&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;总结&quot;&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;防止神经网络过拟合的常用方法：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;获得更多的训练数据&lt;/li&gt;
  &lt;li&gt;减小网络容量&lt;/li&gt;
  &lt;li&gt;添加权重正则化&lt;/li&gt;
  &lt;li&gt;添加dropout&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;机器学习的通用工作流程&quot;&gt;机器学习的通用工作流程&lt;/h1&gt;
&lt;h2 id=&quot;定义问题收集数据集&quot;&gt;定义问题，收集数据集&lt;/h2&gt;
&lt;p&gt;非平稳问题&lt;/p&gt;

&lt;h2 id=&quot;选择衡量成功的指标&quot;&gt;选择衡量成功的指标&lt;/h2&gt;

&lt;h2 id=&quot;确定评估方法&quot;&gt;确定评估方法&lt;/h2&gt;
&lt;h2 id=&quot;准备数据&quot;&gt;准备数据&lt;/h2&gt;
&lt;h2 id=&quot;开发比基准更好的模型&quot;&gt;开发比基准更好的模型&lt;/h2&gt;
&lt;p&gt;目标是获得 &lt;strong&gt;统计功效&lt;/strong&gt;&lt;br /&gt;
选择三个关键参数构建第一个工作模型：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;最后一层的激活&lt;/li&gt;
  &lt;li&gt;损失函数&lt;/li&gt;
  &lt;li&gt;优化配置&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;扩大模型规模开发过拟合的模型&quot;&gt;扩大模型规模：开发过拟合的模型&lt;/h2&gt;
&lt;p&gt;理想的模型是刚好在欠拟合和过拟合的界线上，在容量不足和容量过大的界线上。&lt;/p&gt;
&lt;h2 id=&quot;模型正则化与调节超参数&quot;&gt;模型正则化与调节超参数&lt;/h2&gt;
&lt;p&gt;应该尝试以下几项：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;添加dropout&lt;/li&gt;
  &lt;li&gt;尝试不同的架构：增加或减少层数&lt;/li&gt;
  &lt;li&gt;添加L1或L2正则化&lt;/li&gt;
  &lt;li&gt;尝试不同的超参数，以找到最佳的配置&lt;/li&gt;
  &lt;li&gt;（可选）反复做特征工程：添加新特征或删除没有信息量的特征&lt;/li&gt;
&lt;/ul&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><category term="Python深度学习" /><summary type="html">机器学习的四个分支 监督学习 监督学习是目前最常见的机器学习类型。 变体： 序列生成：给定一张图像，预测描述图像的文字。 语法树预测：给定一个句子，预测其分解生成的语法树。 目标检测：给定一张图像，在图中特定目标的周围画一个边界框。 图像分割：给定一张图像，在特定物体上画一个象素级的mask。 无监督学习 是指在没有目标的情况下寻找输入数据的有趣变换。 降维 聚类 自监督学习 没有人类参与的监督学习。 标签是从输入数据中生成的，通常是使用启发式算法生成的。 自编码器 强化学习 智能体 还没有工业化，未来？ 评估机器学习模型 训练集，验证集和测试集 超参数–选择层数或每层的的大小 参数–权重 信息泄露 如果数据很少： 简单的留出验证 K折验证 重复K折验证 简单的留出验证 留出一定比例的数据作为测试集。 k折验证 将数据划分为大小相同的k个分区。 带有打乱数据的重复k折验证 计算代价很大 评估模型的注意事项 数据代表性 时间箭头 数据冗余 数据预处理，特征工程和特征学习 神经网络的数据预处理 目的是使原始数据更适于用神经网络处理 向量化 值标准化 输入数据应具以下特征： 取值较小 同质性 处理缺失值 特征工程 用更简单的方式表述问题，从而使问题变得很容易。 过拟合与欠拟合 优化 泛化 正则化: 降低过拟合的方法 正则化方法： 减小网络大小 容量：模型中可学习参数的个数 添加权重正则化 奥卡姆剃刀原理 一种常见的降低过拟合的方法：强制使模型权重只能选择较小的值，从而限制模型的复杂度，使得分布更加的规则。 实现方法是向网络损失函数中添加与较大权重值相关的成本。 两种形式： L1正则化：成本与权重系数的绝对值成正比 L2正则化（权重衰减）：成本与权重系数的平方成正比 添加dropout正则化 对某一层使用，就是在训练过程中随机将该层的一些输出特征舍弃（设置为0） 总结 防止神经网络过拟合的常用方法： 获得更多的训练数据 减小网络容量 添加权重正则化 添加dropout 机器学习的通用工作流程 定义问题，收集数据集 非平稳问题 选择衡量成功的指标 确定评估方法 准备数据 开发比基准更好的模型 目标是获得 统计功效 选择三个关键参数构建第一个工作模型： 最后一层的激活 损失函数 优化配置 扩大模型规模：开发过拟合的模型 理想的模型是刚好在欠拟合和过拟合的界线上，在容量不足和容量过大的界线上。 模型正则化与调节超参数 应该尝试以下几项： 添加dropout 尝试不同的架构：增加或减少层数 添加L1或L2正则化 尝试不同的超参数，以找到最佳的配置 （可选）反复做特征工程：添加新特征或删除没有信息量的特征</summary></entry><entry><title type="html">博弈论基础</title><link href="http://localhost:4000/2020/03/13/%E5%8D%9A%E5%BC%88%E8%AE%BA%E5%9F%BA%E7%A1%80.html" rel="alternate" type="text/html" title="博弈论基础" /><published>2020-03-13T00:00:00+09:00</published><updated>2020-03-13T00:00:00+09:00</updated><id>http://localhost:4000/2020/03/13/%E5%8D%9A%E5%BC%88%E8%AE%BA%E5%9F%BA%E7%A1%80</id><content type="html" xml:base="http://localhost:4000/2020/03/13/%E5%8D%9A%E5%BC%88%E8%AE%BA%E5%9F%BA%E7%A1%80.html">&lt;h1 id=&quot;完全信息静态博弈&quot;&gt;完全信息静态博弈&lt;/h1&gt;
&lt;h2 id=&quot;博弈的标准式表达&quot;&gt;博弈的标准式表达&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;囚徒困境
    &lt;h2 id=&quot;重复剔除严格劣战略&quot;&gt;重复剔除严格劣战略&lt;/h2&gt;
  &lt;/li&gt;
  &lt;li&gt;严格劣战略
    &lt;h2 id=&quot;纳什均衡的导出和定义&quot;&gt;纳什均衡的导出和定义&lt;/h2&gt;
  &lt;/li&gt;
  &lt;li&gt;纳什均衡&lt;/li&gt;
  &lt;li&gt;性别战博弈&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;应用举例&quot;&gt;应用举例&lt;/h1&gt;
&lt;h2 id=&quot;古诺的双头垄断模型&quot;&gt;古诺的双头垄断模型&lt;/h2&gt;
&lt;h2 id=&quot;贝特兰德的双头垄断模型&quot;&gt;贝特兰德的双头垄断模型&lt;/h2&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="读后感" /><summary type="html">完全信息静态博弈 博弈的标准式表达 囚徒困境 重复剔除严格劣战略 严格劣战略 纳什均衡的导出和定义 纳什均衡 性别战博弈 应用举例 古诺的双头垄断模型 贝特兰德的双头垄断模型</summary></entry></feed>
<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh"><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="zh" /><updated>2020-02-26T13:59:20+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">旭的小窝</title><subtitle>这里讲我的故事
</subtitle><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><entry><title type="html">网络迷踪</title><link href="http://localhost:4000/2020/02/24/%E7%BD%91%E7%BB%9C%E8%BF%B7%E8%B8%AA.html" rel="alternate" type="text/html" title="网络迷踪" /><published>2020-02-24T00:00:00+09:00</published><updated>2020-02-24T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/24/%E3%80%8A%E7%BD%91%E7%BB%9C%E8%BF%B7%E8%B8%AA%E3%80%8B</id><content type="html" xml:base="http://localhost:4000/2020/02/24/%E7%BD%91%E7%BB%9C%E8%BF%B7%E8%B8%AA.html">&lt;p&gt;2018年的高分电影，悬疑片。&lt;br /&gt;
昨天无意之间找到了这部电影，貌似还是2018年的高分悬疑电影。&lt;/p&gt;

&lt;h2 id=&quot;故事内容&quot;&gt;故事内容&lt;/h2&gt;

&lt;h2 id=&quot;感想&quot;&gt;感想&lt;/h2&gt;
&lt;p&gt;千万不要轻易的死去，要不然平时和你不熟悉的人一下就成了和你交情要好的朋友。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="观后感" /><summary type="html">2018年的高分电影，悬疑片。 昨天无意之间找到了这部电影，貌似还是2018年的高分悬疑电影。 故事内容 感想 千万不要轻易的死去，要不然平时和你不熟悉的人一下就成了和你交情要好的朋友。</summary></entry><entry><title type="html">Anaconda3教程</title><link href="http://localhost:4000/2020/02/21/Anaconda3%E5%AE%89%E8%A3%85.html" rel="alternate" type="text/html" title="Anaconda3教程" /><published>2020-02-21T00:00:00+09:00</published><updated>2020-02-21T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/21/Anaconda3%E5%AE%89%E8%A3%85</id><content type="html" xml:base="http://localhost:4000/2020/02/21/Anaconda3%E5%AE%89%E8%A3%85.html">&lt;h1 id=&quot;什么是anaconda&quot;&gt;什么是Anaconda？&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://www.anaconda.com/&quot;&gt;Anaconda&lt;/a&gt;就是可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda包含了conda、Python在内的超过180个科学包及其依赖项。&lt;/p&gt;

&lt;p class=&quot;info&quot;&gt;本教程在wsl下安装anaconda及使用&lt;/p&gt;

&lt;h1 id=&quot;在window的wsl系统下安装anaconda3&quot;&gt;在window的wsl系统下安装anaconda3&lt;/h1&gt;
&lt;h2 id=&quot;steps-to-install-anaconda-on-windows-ubuntu-terminal&quot;&gt;Steps to Install Anaconda on Windows Ubuntu Terminal&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Go to https://repo.continuum.io/archive to find the list of Anaconda releases&lt;/li&gt;
  &lt;li&gt;Select the release you want.下载.bash&lt;/li&gt;
  &lt;li&gt;通过wsl终端访问/mnt/*(下载的地方)&lt;/li&gt;
  &lt;li&gt;运行安装脚本
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash Anaconda3-2019.10-Linux-x86_64.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Optionally install VS Code when prompted&lt;/li&gt;
  &lt;li&gt;Close the terminal and reopen it to reload .bash configs.&lt;/li&gt;
  &lt;li&gt;To test that it worked, run $ which python. It should print a path that has anaconda in it.&lt;/li&gt;
  &lt;li&gt;To open jupyter, type $ jupyter notebook –no-browser.&lt;br /&gt;
The no browser flag will still run Jupyter on port 8888, but it won’t pop it open automatically. it’s necessary since you don’t have a browser (probably) in your subsystem. In the terminal, it will give you a link to paste into your browser. If it worked, you should see your notebooks!&lt;/li&gt;
  &lt;li&gt;I made an alias for the juypter command by putting this command in my .bash_aliases: alias jup=’jupyter notebook –no-browser’.&lt;br /&gt;
Restart the terminal for this new command to take effect.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;the-conda-command&quot;&gt;The conda command&lt;/h1&gt;
&lt;h2 id=&quot;install-tensorflow&quot;&gt;install tensorflow&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda install tensorflow
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;验证conda已被安装&quot;&gt;验证conda已被安装&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda --version
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;更新conda至最新版本&quot;&gt;更新conda至最新版本&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda update conda
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;执行命令后，conda将会对版本进行比较并列出可以升级的版本。同时，也会告知用户其他相关包也会升级到相应版本。&lt;/p&gt;

&lt;p&gt;当较新的版本可以用于升级时，终端会显示 Proceed ([y]/n)? ，此时输入 y 即可进行升级。&lt;/p&gt;

&lt;h2 id=&quot;查看conda帮助信息&quot;&gt;查看conda帮助信息&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda --help
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;或&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda -h
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h1 id=&quot;管理环境&quot;&gt;管理环境&lt;/h1&gt;
&lt;h2 id=&quot;创建新环境&quot;&gt;创建新环境&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda create --name &amp;lt;env_name&amp;gt; &amp;lt;package_names&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;Attention:
    &lt;ul&gt;
      &lt;li&gt;
        &lt;env_name&gt; 即创建的环境名。建议以英文命名，且不加空格，名称两边不加尖括号“&amp;lt;&amp;gt;”。
&lt;/env_name&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;package_names&gt; 即安装在环境中的包名。名称两边不加尖括号“&amp;lt;&amp;gt;”。
&lt;/package_names&gt;
      &lt;/li&gt;
      &lt;li&gt;如果要安装指定的版本号，则只需要在包名后面以 = 和版本号的形式执行。如： conda create –name python2 python=2.7 ，即创建一个名为“python2”的环境，环境中安装版本为2.7的python。&lt;/li&gt;
      &lt;li&gt;–name 同样可以替换为 -n 。&lt;/li&gt;
      &lt;li&gt;提示：默认情况下，新创建的环境将会被保存在 /Users/&lt;user_name&gt;/anaconda3/env 目录下，其中， &lt;user_name&gt; 为当前用户的用户名。&lt;/user_name&gt;&lt;/user_name&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;切换环境&quot;&gt;切换环境&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;source activate &amp;lt;env_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;或&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda activate &amp;lt;env_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;当成功切换环境之后，在该行行首将以“(env_name)”或“[env_name]”开头。其中，“env_name”为切换到的环境名。如：在macOS系统中执行 source active python2 ，即切换至名为“python2”的环境，则行首将会以(python2)开头。&lt;/p&gt;

&lt;h2 id=&quot;退出环境至root&quot;&gt;退出环境至root&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;source deactivate
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;或&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda deactivate
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;当执行退出当前环境，回到root环境命令后，原本行首以“(env_name)”或“[env_name]”开头的字符将不再显示。&lt;/p&gt;

&lt;h2 id=&quot;显示已创建环境&quot;&gt;显示已创建环境&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda info --envs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;或&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda info -e
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;或&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda env list
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;结果中星号“*”所在行即为当前所在环境。&lt;/p&gt;

&lt;h2 id=&quot;复制环境&quot;&gt;复制环境&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda create --name &amp;lt;new_env_name&amp;gt; --clone &amp;lt;copied_env_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;Attention:
    &lt;ul&gt;
      &lt;li&gt;
        &lt;copied_env_name&gt; 即为被复制/克隆环境名。环境名两边不加尖括号“&amp;lt;&amp;gt;”。
&lt;/copied_env_name&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;new_env_name&gt; 即为复制之后新环境的名称。环境名两边不加尖括号“&amp;lt;&amp;gt;”。
&lt;/new_env_name&gt;
      &lt;/li&gt;
      &lt;li&gt;conda create –name py2 –clone python2 ，即为克隆名为“python2”的环境，克隆后的新环境名为“py2”。此时，环境中将同时存在“python2”和“py2”环境，且两个环境的配置相同。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;删除环境&quot;&gt;删除环境&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda remove --name &amp;lt;env_name&amp;gt; --all
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;管理包&quot;&gt;管理包&lt;/h1&gt;
&lt;h2 id=&quot;查找可供安装的包版本&quot;&gt;查找可供安装的包版本&lt;/h2&gt;
&lt;h3 id=&quot;精确查找&quot;&gt;精确查找&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda search --full-name &amp;lt;package_full_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;Attention:
    &lt;ul&gt;
      &lt;li&gt;–full-name 为精确查找的参数。&lt;/li&gt;
      &lt;li&gt;
        &lt;package_full_name&gt; 是被查找包的全名。包名两边不加尖括号“&amp;lt;&amp;gt;”。
&lt;/package_full_name&gt;
      &lt;/li&gt;
      &lt;li&gt;例如： conda search –full-name python 即查找全名为“python”的包有哪些版本可供安装。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;模糊查找&quot;&gt;模糊查找&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda search &amp;lt;text&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;获取当前环境中已安装的包信息&quot;&gt;获取当前环境中已安装的包信息&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda list
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;执行上述命令后将在终端显示当前环境已安装包的包名及其版本号。&lt;/p&gt;

&lt;h2 id=&quot;安装包&quot;&gt;安装包&lt;/h2&gt;
&lt;h3 id=&quot;在指定环境中安装包&quot;&gt;在指定环境中安装包&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda install --name &amp;lt;env_name&amp;gt; &amp;lt;package_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;例如： conda install –name python2 pandas 即在名为“python2”的环境中安装pandas包。&lt;/p&gt;

&lt;h3 id=&quot;在当前环境中安装包&quot;&gt;在当前环境中安装包&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda install &amp;lt;package_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;例如： conda install pandas 即在当前环境中安装pandas包。&lt;/p&gt;

&lt;h3 id=&quot;使用pip安装包&quot;&gt;使用pip安装包&lt;/h3&gt;
&lt;h4 id=&quot;使用场景&quot;&gt;使用场景&lt;/h4&gt;
&lt;p&gt;当使用 conda install 无法进行安装时，可以使用pip进行安装。例如：see包。&lt;/p&gt;

&lt;h2 id=&quot;卸载包&quot;&gt;卸载包&lt;/h2&gt;
&lt;h3 id=&quot;卸载指定环境中的包&quot;&gt;卸载指定环境中的包&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda remove --name &amp;lt;env_name&amp;gt; &amp;lt;package_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;例如： conda remove –name python2 pandas 即卸载名为“python2”中的pandas包。&lt;/p&gt;

&lt;h3 id=&quot;卸载当前环境中的包&quot;&gt;卸载当前环境中的包&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda remove &amp;lt;package_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;例如： conda remove pandas 即在当前环境中卸载pandas包。&lt;/p&gt;

&lt;h2 id=&quot;更新包&quot;&gt;更新包&lt;/h2&gt;
&lt;h3 id=&quot;更新所有包&quot;&gt;更新所有包&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda update --all
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;或&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda upgrade --all
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;更新指定包&quot;&gt;更新指定包&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda update &amp;lt;package_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;或&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda upgrade &amp;lt;package_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;更新多个指定包，则包名以空格隔开，向后排列。&lt;br /&gt;
如： conda update pandas numpy matplotlib 即更新pandas、numpy、matplotlib包。&lt;/p&gt;

&lt;h1 id=&quot;参考资料&quot;&gt;参考资料&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32925500&quot;&gt;Anaconda介绍、安装及使用教程&lt;/a&gt;&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="Anaconda3" /><category term="ComputerVision" /><summary type="html">什么是Anaconda？ Anaconda就是可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda包含了conda、Python在内的超过180个科学包及其依赖项。 本教程在wsl下安装anaconda及使用 在window的wsl系统下安装anaconda3 Steps to Install Anaconda on Windows Ubuntu Terminal Go to https://repo.continuum.io/archive to find the list of Anaconda releases Select the release you want.下载.bash 通过wsl终端访问/mnt/*(下载的地方) 运行安装脚本 bash Anaconda3-2019.10-Linux-x86_64.sh Optionally install VS Code when prompted Close the terminal and reopen it to reload .bash configs. To test that it worked, run $ which python. It should print a path that has anaconda in it. To open jupyter, type $ jupyter notebook –no-browser. The no browser flag will still run Jupyter on port 8888, but it won’t pop it open automatically. it’s necessary since you don’t have a browser (probably) in your subsystem. In the terminal, it will give you a link to paste into your browser. If it worked, you should see your notebooks! I made an alias for the juypter command by putting this command in my .bash_aliases: alias jup=’jupyter notebook –no-browser’. Restart the terminal for this new command to take effect. The conda command install tensorflow conda install tensorflow 验证conda已被安装 conda --version 更新conda至最新版本 conda update conda 执行命令后，conda将会对版本进行比较并列出可以升级的版本。同时，也会告知用户其他相关包也会升级到相应版本。 当较新的版本可以用于升级时，终端会显示 Proceed ([y]/n)? ，此时输入 y 即可进行升级。 查看conda帮助信息 conda --help 或 conda -h 管理环境 创建新环境 conda create --name &amp;lt;env_name&amp;gt; &amp;lt;package_names&amp;gt; Attention: 即创建的环境名。建议以英文命名，且不加空格，名称两边不加尖括号“&amp;lt;&amp;gt;”。 即安装在环境中的包名。名称两边不加尖括号“&amp;lt;&amp;gt;”。 如果要安装指定的版本号，则只需要在包名后面以 = 和版本号的形式执行。如： conda create –name python2 python=2.7 ，即创建一个名为“python2”的环境，环境中安装版本为2.7的python。 –name 同样可以替换为 -n 。 提示：默认情况下，新创建的环境将会被保存在 /Users//anaconda3/env 目录下，其中， 为当前用户的用户名。 切换环境 source activate &amp;lt;env_name&amp;gt; 或 conda activate &amp;lt;env_name&amp;gt; 当成功切换环境之后，在该行行首将以“(env_name)”或“[env_name]”开头。其中，“env_name”为切换到的环境名。如：在macOS系统中执行 source active python2 ，即切换至名为“python2”的环境，则行首将会以(python2)开头。 退出环境至root source deactivate 或 conda deactivate 当执行退出当前环境，回到root环境命令后，原本行首以“(env_name)”或“[env_name]”开头的字符将不再显示。 显示已创建环境 conda info --envs 或 conda info -e 或 conda env list 结果中星号“*”所在行即为当前所在环境。 复制环境 conda create --name &amp;lt;new_env_name&amp;gt; --clone &amp;lt;copied_env_name&amp;gt; Attention: 即为被复制/克隆环境名。环境名两边不加尖括号“&amp;lt;&amp;gt;”。 即为复制之后新环境的名称。环境名两边不加尖括号“&amp;lt;&amp;gt;”。 conda create –name py2 –clone python2 ，即为克隆名为“python2”的环境，克隆后的新环境名为“py2”。此时，环境中将同时存在“python2”和“py2”环境，且两个环境的配置相同。 删除环境 conda remove --name &amp;lt;env_name&amp;gt; --all 管理包 查找可供安装的包版本 精确查找 conda search --full-name &amp;lt;package_full_name&amp;gt; Attention: –full-name 为精确查找的参数。 是被查找包的全名。包名两边不加尖括号“&amp;lt;&amp;gt;”。 例如： conda search –full-name python 即查找全名为“python”的包有哪些版本可供安装。 模糊查找 conda search &amp;lt;text&amp;gt; 获取当前环境中已安装的包信息 conda list 执行上述命令后将在终端显示当前环境已安装包的包名及其版本号。 安装包 在指定环境中安装包 conda install --name &amp;lt;env_name&amp;gt; &amp;lt;package_name&amp;gt; 例如： conda install –name python2 pandas 即在名为“python2”的环境中安装pandas包。 在当前环境中安装包 conda install &amp;lt;package_name&amp;gt; 例如： conda install pandas 即在当前环境中安装pandas包。 使用pip安装包 使用场景 当使用 conda install 无法进行安装时，可以使用pip进行安装。例如：see包。 卸载包 卸载指定环境中的包 conda remove --name &amp;lt;env_name&amp;gt; &amp;lt;package_name&amp;gt; 例如： conda remove –name python2 pandas 即卸载名为“python2”中的pandas包。 卸载当前环境中的包 conda remove &amp;lt;package_name&amp;gt; 例如： conda remove pandas 即在当前环境中卸载pandas包。 更新包 更新所有包 conda update --all 或 conda upgrade --all 更新指定包 conda update &amp;lt;package_name&amp;gt; 或 conda upgrade &amp;lt;package_name&amp;gt; 更新多个指定包，则包名以空格隔开，向后排列。 如： conda update pandas numpy matplotlib 即更新pandas、numpy、matplotlib包。 参考资料 Anaconda介绍、安装及使用教程</summary></entry><entry><title type="html">神经网络入门</title><link href="http://localhost:4000/2020/02/21/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8.html" rel="alternate" type="text/html" title="神经网络入门" /><published>2020-02-21T00:00:00+09:00</published><updated>2020-02-21T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/21/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8</id><content type="html" xml:base="http://localhost:4000/2020/02/21/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8.html">&lt;h1 id=&quot;神经网络刨析&quot;&gt;神经网络刨析&lt;/h1&gt;
&lt;p&gt;训练神经网络主要围绕一下几个方面：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;层&lt;/strong&gt;，多个层组合成网络（模型）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;输入数据&lt;/strong&gt;和相应的&lt;strong&gt;目标&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;损失函数&lt;/strong&gt;，用于学习的反馈信号&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;优化器&lt;/strong&gt;，决定学习过程如何进行&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;层深度学习的基础组件&quot;&gt;层：深度学习的基础组件&lt;/h2&gt;
&lt;p&gt;将一个或多个输入张量转换为一个或多个输出张量。&lt;br /&gt;
层的&lt;strong&gt;权重&lt;/strong&gt;是层的状态。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;简单的向量数据-2D张量-&lt;strong&gt;密集连接层（densely connected lyaer）或全连接层（fully connected layer)或密集层（dense layer）&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;序列数据-3D张量-&lt;strong&gt;循环层（recurrent layer）&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;图像数据-4D张量-&lt;strong&gt;二维卷积层&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;模型层构成的网络&quot;&gt;模型：层构成的网络&lt;/h2&gt;
&lt;p&gt;常见的网络拓扑结构：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;双分支网络（two-branch）&lt;/li&gt;
  &lt;li&gt;多头网络（multihead）&lt;/li&gt;
  &lt;li&gt;Inception模块&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;网络的拓扑结构定义了一个&lt;strong&gt;假设空间（hypothesis space）&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&quot;损失函数与优化器配置学习过程的关键&quot;&gt;损失函数与优化器：配置学习过程的关键&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;损失函数（目标函数）：在训练过程中需要将其最小化。它能够衡量当前任务是否已经成功完成。&lt;/li&gt;
  &lt;li&gt;优化器：决定如何基于损失函数对网络进行更新。它执行的是随机梯度下降（SGD）的某个变体&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;梯度下降过程必须基于&lt;strong&gt;单个&lt;/strong&gt;标量损失值。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><category term="Python深度学习" /><summary type="html">神经网络刨析 训练神经网络主要围绕一下几个方面： 层，多个层组合成网络（模型） 输入数据和相应的目标 损失函数，用于学习的反馈信号 优化器，决定学习过程如何进行 层：深度学习的基础组件 将一个或多个输入张量转换为一个或多个输出张量。 层的权重是层的状态。 简单的向量数据-2D张量-密集连接层（densely connected lyaer）或全连接层（fully connected layer)或密集层（dense layer） 序列数据-3D张量-循环层（recurrent layer） 图像数据-4D张量-二维卷积层 模型：层构成的网络 常见的网络拓扑结构： 双分支网络（two-branch） 多头网络（multihead） Inception模块 网络的拓扑结构定义了一个假设空间（hypothesis space）。 损失函数与优化器：配置学习过程的关键 损失函数（目标函数）：在训练过程中需要将其最小化。它能够衡量当前任务是否已经成功完成。 优化器：决定如何基于损失函数对网络进行更新。它执行的是随机梯度下降（SGD）的某个变体 梯度下降过程必须基于单个标量损失值。</summary></entry><entry><title type="html">神经网络基础</title><link href="http://localhost:4000/2020/02/18/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80.html" rel="alternate" type="text/html" title="神经网络基础" /><published>2020-02-18T00:00:00+09:00</published><updated>2020-02-18T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/18/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80</id><content type="html" xml:base="http://localhost:4000/2020/02/18/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80.html">&lt;h1 id=&quot;神经网络的数据表示&quot;&gt;神经网络的数据表示&lt;/h1&gt;

&lt;h2 id=&quot;张量tensor&quot;&gt;张量（tensor）&lt;/h2&gt;
&lt;p&gt;使用张量作为基本数据结构。&lt;/p&gt;

&lt;p&gt;是一个数据容器。&lt;br /&gt;
它包含的数据几乎总是数值数据，因此它是数字的容器。&lt;br /&gt;
矩阵是二维张量。&lt;/p&gt;

&lt;p&gt;张量是矩阵向任意维度的推广。
张量的 &lt;strong&gt;维度（dimension)&lt;/strong&gt; 叫做 &lt;strong&gt;轴（axis）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;标量（0D张量）&lt;br /&gt;
仅包含一个数字的张量叫做 &lt;strong&gt;标量(scalar)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;向量（1D张量）&lt;br /&gt;
数字组成的数组叫做 &lt;strong&gt;向量(vector)&lt;/strong&gt; 或 &lt;strong&gt;一维张量（1D张量）&lt;/strong&gt;&lt;br /&gt;
一维张量只有一个轴。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;矩阵（2D张量）&lt;br /&gt;
向量组成的数组叫做 &lt;strong&gt;矩阵（matrix）&lt;/strong&gt; 或 &lt;strong&gt;二维张量（2D张量）&lt;/strong&gt;&lt;br /&gt;
矩阵有两个轴（行和列）&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;3D张量与更高维度的张量
将多个矩阵组合成一个新的数组，可以得到一个3D张量。&lt;br /&gt;
直观的理解为数字组成的立方体。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;高维度张量以此类推。&lt;/p&gt;

&lt;h2 id=&quot;关键属性&quot;&gt;关键属性&lt;/h2&gt;
&lt;p&gt;张量由以下三个关键属性来定义。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;轴的个数（阶）&lt;br /&gt;
3D张量有3个轴，矩阵有2个轴。&lt;/li&gt;
  &lt;li&gt;形状&lt;br /&gt;
张量沿每个轴的维度大小（元素个数）。&lt;/li&gt;
  &lt;li&gt;数据类型&lt;br /&gt;
张量中所包含数据的类型。&lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;warning&quot;&gt;很多库中不存在字符串张量。&lt;/p&gt;

&lt;h2 id=&quot;数据批量&quot;&gt;数据批量&lt;/h2&gt;
&lt;p&gt;数据张量的第一个轴（0轴）都是&lt;strong&gt;样本轴（sample axis，样本维度）&lt;/strong&gt;。
深度学习模型不会同时处理整个数据集，而是将数据拆分成小批量。&lt;br /&gt;
对于这种批量张量，第一个轴（0轴)叫做&lt;strong&gt;批量轴（batch axis）&lt;/strong&gt;或&lt;strong&gt;批量维度（batch dimension）&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&quot;现实世界中的数据张量&quot;&gt;现实世界中的数据张量&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;向量数据：2D张量&lt;/li&gt;
  &lt;li&gt;时间序列数据 或 序列数据：3D张量&lt;/li&gt;
  &lt;li&gt;图像：4D张量&lt;/li&gt;
  &lt;li&gt;视频：5D张量&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;张量运算&quot;&gt;张量运算&lt;/h1&gt;
&lt;h2 id=&quot;逐元素运算&quot;&gt;逐元素运算&lt;/h2&gt;
&lt;h2 id=&quot;广播&quot;&gt;广播&lt;/h2&gt;
&lt;p&gt;两个形状不同的张量进行操作，会发生什么？&lt;/p&gt;

&lt;p&gt;较小的张量会被&lt;strong&gt;广播（broadcast）&lt;/strong&gt;，以匹配较大张量的形状。&lt;/p&gt;

&lt;p&gt;广播包含以下两个步骤：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;向较小的张量添加轴（叫做&lt;strong&gt;广播轴&lt;/strong&gt;）。&lt;/li&gt;
  &lt;li&gt;将较小的张量沿着新轴重复，使其形状与较大张量相同。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;张量点积&quot;&gt;张量点积&lt;/h2&gt;
&lt;p&gt;点积运算，也叫&lt;strong&gt;张量积（tensor product）&lt;/strong&gt;。&lt;br /&gt;
它将输入张量的元素合并在一起。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;z = np.dot(x,y)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;数学符号中&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;z = x.y
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p class=&quot;warning&quot;&gt;两个向量之间的点积是一个标量，而且只有元素个数相同的向量之间才能做点积。&lt;/p&gt;

&lt;h2 id=&quot;张量变形tensor-reshaping&quot;&gt;张量变形（tensor reshaping）&lt;/h2&gt;
&lt;p&gt;张量变形是指改变张量的行和列，以得到想要的形状。&lt;/p&gt;

&lt;p&gt;特殊的张量变形：转置（transposition）。&lt;/p&gt;

&lt;h1 id=&quot;基于梯度的优化&quot;&gt;基于梯度的优化&lt;/h1&gt;
&lt;h2 id=&quot;训练循环training-loop&quot;&gt;训练循环（training loop)&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;抽取训练样本x和对应目标y组成的数据批量&lt;/li&gt;
  &lt;li&gt;在x上运行网络[前向传播（forward pass）]，得到预测值y_pred&lt;/li&gt;
  &lt;li&gt;计算网络在这批数据上的损失，用于衡量y_pred和y之间的距离。&lt;/li&gt;
  &lt;li&gt;更新网络的所有权重，使网络在这批数据上的损失略微下降。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;对于第四步，更好的方法是利用网络中所有运算都是 &lt;strong&gt;可微（differentiable）&lt;/strong&gt; 的，计算损失相对于网络系数的 &lt;strong&gt;梯度（gradient）&lt;/strong&gt;，然后向梯度的反方向改变系数，从而使损失降低。&lt;/p&gt;

&lt;p class=&quot;info&quot;&gt;可微的意思是“可以被求导”&lt;/p&gt;

&lt;h2 id=&quot;张量运算的导数梯度&quot;&gt;张量运算的导数：梯度&lt;/h2&gt;
&lt;h2 id=&quot;随机梯度下降&quot;&gt;随机梯度下降&lt;/h2&gt;
&lt;h3 id=&quot;小批量随机梯度下降mini-batch-stochastic-gradient-descent&quot;&gt;小批量随机梯度下降（mini-batch stochastic gradient descent）&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;抽取训练样本x和对应目标y组成的数据批量&lt;/li&gt;
  &lt;li&gt;在x上运行网络，得到预测值y_pred&lt;/li&gt;
  &lt;li&gt;计算网络在这批数据上的损失，用于衡量y_pred和y之间的距离。&lt;/li&gt;
  &lt;li&gt;计算损失相对于网络参数的梯度【一次**反向传播（backward pass）】。&lt;/li&gt;
  &lt;li&gt;将参数沿着梯度的反方向移动一点，从而使这批数据上的损失减小一点。&lt;/li&gt;
&lt;/ol&gt;

&lt;p class=&quot;info&quot;&gt;&lt;strong&gt;随机（stochastic）&lt;/strong&gt;是指每批数据都是随机抽取的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;真SGD&lt;/strong&gt;：每次迭代时只抽取一个样本和目标。&lt;br /&gt;
&lt;strong&gt;批量SGD&lt;/strong&gt;：每一次迭代都在所有数据上运行。这样每次更新都更加准确，但计算代价也高很多。&lt;/p&gt;

&lt;p&gt;SGD还有很多种变体，带动量的SGD等变体，被称为&lt;strong&gt;优化方法（optimization method）&lt;/strong&gt;或 &lt;strong&gt;优化器（optimizer）&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;动量方法的实现过程是，每一步都移动小球，不仅要考虑当前的斜率值（当前的加速度），还要考虑当前的速度（来自于之前的加速度）。&lt;br /&gt;
在神经网络中，更新参数w不仅要考虑当前的梯度值，还要考虑上一次的参数更新。&lt;/p&gt;

&lt;h2 id=&quot;链式求导反向传播算法&quot;&gt;链式求导：反向传播算法&lt;/h2&gt;

&lt;h2 id=&quot;tips&quot;&gt;Tips：&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;学习&lt;/strong&gt;是指找到一组模型参数，使得在给定的训练数据样本和对应目标值上的损失函数最小化。&lt;/li&gt;
  &lt;li&gt;学习的过程：随机选取包含数据样本及其目标值的批量，并计算批量损失相对于网络参数的梯度。随后将网络参数沿着梯度的反方向稍稍移动。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;损失&lt;/strong&gt;是在训练过程中需要最小化额量，它能够衡量当前任务是否已成功解决。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;优化器&lt;/strong&gt;是使用损失梯度更新参数的具体方式。&lt;/li&gt;
&lt;/ul&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><category term="Python深度学习" /><summary type="html">神经网络的数据表示 张量（tensor） 使用张量作为基本数据结构。 是一个数据容器。 它包含的数据几乎总是数值数据，因此它是数字的容器。 矩阵是二维张量。 张量是矩阵向任意维度的推广。 张量的 维度（dimension) 叫做 轴（axis） 标量（0D张量） 仅包含一个数字的张量叫做 标量(scalar) 向量（1D张量） 数字组成的数组叫做 向量(vector) 或 一维张量（1D张量） 一维张量只有一个轴。 矩阵（2D张量） 向量组成的数组叫做 矩阵（matrix） 或 二维张量（2D张量） 矩阵有两个轴（行和列） 3D张量与更高维度的张量 将多个矩阵组合成一个新的数组，可以得到一个3D张量。 直观的理解为数字组成的立方体。 高维度张量以此类推。 关键属性 张量由以下三个关键属性来定义。 轴的个数（阶） 3D张量有3个轴，矩阵有2个轴。 形状 张量沿每个轴的维度大小（元素个数）。 数据类型 张量中所包含数据的类型。 很多库中不存在字符串张量。 数据批量 数据张量的第一个轴（0轴）都是样本轴（sample axis，样本维度）。 深度学习模型不会同时处理整个数据集，而是将数据拆分成小批量。 对于这种批量张量，第一个轴（0轴)叫做批量轴（batch axis）或批量维度（batch dimension）。 现实世界中的数据张量 向量数据：2D张量 时间序列数据 或 序列数据：3D张量 图像：4D张量 视频：5D张量 张量运算 逐元素运算 广播 两个形状不同的张量进行操作，会发生什么？ 较小的张量会被广播（broadcast），以匹配较大张量的形状。 广播包含以下两个步骤： 向较小的张量添加轴（叫做广播轴）。 将较小的张量沿着新轴重复，使其形状与较大张量相同。 张量点积 点积运算，也叫张量积（tensor product）。 它将输入张量的元素合并在一起。 z = np.dot(x,y) 数学符号中 z = x.y 两个向量之间的点积是一个标量，而且只有元素个数相同的向量之间才能做点积。 张量变形（tensor reshaping） 张量变形是指改变张量的行和列，以得到想要的形状。 特殊的张量变形：转置（transposition）。 基于梯度的优化 训练循环（training loop) 抽取训练样本x和对应目标y组成的数据批量 在x上运行网络[前向传播（forward pass）]，得到预测值y_pred 计算网络在这批数据上的损失，用于衡量y_pred和y之间的距离。 更新网络的所有权重，使网络在这批数据上的损失略微下降。 对于第四步，更好的方法是利用网络中所有运算都是 可微（differentiable） 的，计算损失相对于网络系数的 梯度（gradient），然后向梯度的反方向改变系数，从而使损失降低。 可微的意思是“可以被求导” 张量运算的导数：梯度 随机梯度下降 小批量随机梯度下降（mini-batch stochastic gradient descent） 抽取训练样本x和对应目标y组成的数据批量 在x上运行网络，得到预测值y_pred 计算网络在这批数据上的损失，用于衡量y_pred和y之间的距离。 计算损失相对于网络参数的梯度【一次**反向传播（backward pass）】。 将参数沿着梯度的反方向移动一点，从而使这批数据上的损失减小一点。 随机（stochastic）是指每批数据都是随机抽取的。 真SGD：每次迭代时只抽取一个样本和目标。 批量SGD：每一次迭代都在所有数据上运行。这样每次更新都更加准确，但计算代价也高很多。 SGD还有很多种变体，带动量的SGD等变体，被称为优化方法（optimization method）或 优化器（optimizer）。 动量方法的实现过程是，每一步都移动小球，不仅要考虑当前的斜率值（当前的加速度），还要考虑当前的速度（来自于之前的加速度）。 在神经网络中，更新参数w不仅要考虑当前的梯度值，还要考虑上一次的参数更新。 链式求导：反向传播算法 Tips： 学习是指找到一组模型参数，使得在给定的训练数据样本和对应目标值上的损失函数最小化。 学习的过程：随机选取包含数据样本及其目标值的批量，并计算批量损失相对于网络参数的梯度。随后将网络参数沿着梯度的反方向稍稍移动。 损失是在训练过程中需要最小化额量，它能够衡量当前任务是否已成功解决。 优化器是使用损失梯度更新参数的具体方式。</summary></entry><entry><title type="html">最优化笔记</title><link href="http://localhost:4000/2020/02/18/%E6%9C%80%E4%BC%98%E5%8C%96.html" rel="alternate" type="text/html" title="最优化笔记" /><published>2020-02-18T00:00:00+09:00</published><updated>2020-02-18T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/18/%E6%9C%80%E4%BC%98%E5%8C%96</id><content type="html" xml:base="http://localhost:4000/2020/02/18/%E6%9C%80%E4%BC%98%E5%8C%96.html">&lt;h1 id=&quot;cs231n-optimization&quot;&gt;cs231n optimization&lt;/h1&gt;
&lt;p&gt;Optimization is the process of finding the set of parameters W that minimize the loss function.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit&quot;&gt;(知乎)最优化笔记&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;http://cs231n.github.io/optimization-1/&quot;&gt;cs231n&lt;/a&gt;&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><category term="cs231n" /><summary type="html">cs231n optimization Optimization is the process of finding the set of parameters W that minimize the loss function. (知乎)最优化笔记 cs231n</summary></entry><entry><title type="html">PyTorch张量</title><link href="http://localhost:4000/2020/02/17/Pytorch.html" rel="alternate" type="text/html" title="PyTorch张量" /><published>2020-02-17T00:00:00+09:00</published><updated>2020-02-17T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/17/Pytorch</id><content type="html" xml:base="http://localhost:4000/2020/02/17/Pytorch.html">&lt;h1 id=&quot;基本数据类型&quot;&gt;基本数据类型&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;标量（0维）&lt;br /&gt;
一维长度为1的张量确实也可以表示张量
为了语义更清晰，在版本PyTorch0.3之后，两个概念从形式上加以区分，并增加了长度为零的 tensor。&lt;/li&gt;
  &lt;li&gt;张量（1维） Bias or Linear input （单张图片输入）&lt;/li&gt;
  &lt;li&gt;张量（2维） Linear input batch （多张图片输入）&lt;/li&gt;
  &lt;li&gt;张量（3维） RNN input Batch （循环神经网络批量输入）
[word,sentence,feature]&lt;/li&gt;
  &lt;li&gt;张量（4维） CNN input Batch （卷积神经网络批量输入）
[batch,channel,height,width] ’r’,’g’,’b’三原色通道&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;张量-tensors&quot;&gt;张量 Tensors&lt;/h1&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from __future__ import print_function
import torch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;引入pytorch&lt;/p&gt;

&lt;h3 id=&quot;创建一个-5x3-矩阵-但是未初始化&quot;&gt;创建一个 5x3 矩阵, 但是未初始化:&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = torch.empty(5, 3)
print(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;创建一个随机初始化的矩阵&quot;&gt;创建一个随机初始化的矩阵:&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = torch.rand(5, 3)
print(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;创建一个0填充的矩阵数据类型为long&quot;&gt;创建一个0填充的矩阵，数据类型为long:&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = torch.zeros(5, 3, dtype=torch.long)
print(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;创建tensor并使用现有数据初始化&quot;&gt;创建tensor并使用现有数据初始化:&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = torch.tensor([5.5, 3])
print(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;根据现有的张量创建张量&quot;&gt;根据现有的张量创建张量:&lt;/h3&gt;
&lt;p&gt;这些方法将重用输入张量的属性，例如， dtype，除非设置新的值进行覆盖&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = x.new_ones(5, 3, dtype=torch.double)      # new_* 方法来创建对象
print(x)

x = torch.randn_like(x, dtype=torch.float)    # 覆盖 dtype!
print(x)                                      #  对象的size 是相同的，只是值和类型发生了变化
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;获取size&quot;&gt;获取size&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print(x.size())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;加法&quot;&gt;加法&lt;/h1&gt;
&lt;h3 id=&quot;加法1&quot;&gt;加法1&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;y = torch.rand(5, 3)
print(x + y)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tensor([[ 0.2218,  0.8329, -1.3406],
        [-0.2737,  0.8382,  1.4644],
        [-0.3806,  0.2332, -0.4300],
        [-0.6603,  1.8713,  1.9648],
        [ 1.4351, -0.6195, -0.5985]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;加法2&quot;&gt;加法2&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print(torch.add(x, y))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;提供输出tensor作为参数&quot;&gt;提供输出tensor作为参数&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;result = torch.empty(5, 3)
torch.add(x, y, out=result)
print(result)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;替换&quot;&gt;替换&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# adds x to y
y.add_(x)
print(y)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p class=&quot;warning&quot;&gt;任何 以&lt;code class=&quot;highlighter-rouge&quot;&gt;_&lt;/code&gt; 结尾的操作都会用结果替换原变量. 例如: &lt;code class=&quot;highlighter-rouge&quot;&gt;x.copy_(y)&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;x.t_()&lt;/code&gt;, 都会改变 &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;使用索引操作张量&quot;&gt;使用索引操作张量&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print(x[:, 1])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tensor([ 1.6401,  0.3637,  1.5745, -1.9971,  1.2926])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;torchview-改变张量的维度和大小&quot;&gt;torch.view 改变张量的维度和大小&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8)  #  size -1 从其他维度推断
print(x.size(), y.size(), z.size())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;如果你有只有一个元素的张量&quot;&gt;如果你有只有一个元素的张量&lt;/h3&gt;
&lt;p&gt;使用.item()来得到Python数据类型的数值&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = torch.randn(1)
print(x)
print(x.item())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tensor([-0.4353])
-0.43528521060943604
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><category term="PyTorch" /><summary type="html">基本数据类型 标量（0维） 一维长度为1的张量确实也可以表示张量 为了语义更清晰，在版本PyTorch0.3之后，两个概念从形式上加以区分，并增加了长度为零的 tensor。 张量（1维） Bias or Linear input （单张图片输入） 张量（2维） Linear input batch （多张图片输入） 张量（3维） RNN input Batch （循环神经网络批量输入） [word,sentence,feature] 张量（4维） CNN input Batch （卷积神经网络批量输入） [batch,channel,height,width] ’r’,’g’,’b’三原色通道 张量 Tensors from __future__ import print_function import torch 引入pytorch 创建一个 5x3 矩阵, 但是未初始化: x = torch.empty(5, 3) print(x) 创建一个随机初始化的矩阵: x = torch.rand(5, 3) print(x) 创建一个0填充的矩阵，数据类型为long: x = torch.zeros(5, 3, dtype=torch.long) print(x) 创建tensor并使用现有数据初始化: x = torch.tensor([5.5, 3]) print(x) 根据现有的张量创建张量: 这些方法将重用输入张量的属性，例如， dtype，除非设置新的值进行覆盖 x = x.new_ones(5, 3, dtype=torch.double) # new_* 方法来创建对象 print(x) x = torch.randn_like(x, dtype=torch.float) # 覆盖 dtype! print(x) # 对象的size 是相同的，只是值和类型发生了变化 获取size print(x.size()) 加法 加法1 y = torch.rand(5, 3) print(x + y) tensor([[ 0.2218, 0.8329, -1.3406], [-0.2737, 0.8382, 1.4644], [-0.3806, 0.2332, -0.4300], [-0.6603, 1.8713, 1.9648], [ 1.4351, -0.6195, -0.5985]]) 加法2 print(torch.add(x, y)) 提供输出tensor作为参数 result = torch.empty(5, 3) torch.add(x, y, out=result) print(result) 替换 # adds x to y y.add_(x) print(y) 任何 以_ 结尾的操作都会用结果替换原变量. 例如: x.copy_(y), x.t_(), 都会改变 x. 使用索引操作张量 print(x[:, 1]) tensor([ 1.6401, 0.3637, 1.5745, -1.9971, 1.2926]) torch.view 改变张量的维度和大小 x = torch.randn(4, 4) y = x.view(16) z = x.view(-1, 8) # size -1 从其他维度推断 print(x.size(), y.size(), z.size()) 如果你有只有一个元素的张量 使用.item()来得到Python数据类型的数值 x = torch.randn(1) print(x) print(x.item()) tensor([-0.4353]) -0.43528521060943604</summary></entry><entry><title type="html">自动求导</title><link href="http://localhost:4000/2020/02/17/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC.html" rel="alternate" type="text/html" title="自动求导" /><published>2020-02-17T00:00:00+09:00</published><updated>2020-02-17T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/17/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC</id><content type="html" xml:base="http://localhost:4000/2020/02/17/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC.html">&lt;h1 id=&quot;autograd-自动求导机制&quot;&gt;Autograd: 自动求导机制&lt;/h1&gt;
&lt;p&gt;PyTorch 中所有神经网络的核心是 autograd 包。&lt;/p&gt;

&lt;p&gt;autograd包为张量上的所有操作提供了自动求导。 它是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。&lt;/p&gt;

&lt;h2 id=&quot;张量torchtensor&quot;&gt;张量（torch.Tensor）&lt;/h2&gt;
&lt;p&gt;torch.Tensor是这个包的核心类。&lt;/p&gt;

&lt;p&gt;如果设置 .requires_grad 为 True，那么将会追踪所有对于该张量的操作。&lt;br /&gt;
当完成计算后通过调用 .backward()，自动计算所有的梯度，这个张量的所有梯度将会自动积累到 .grad 属性。&lt;/p&gt;

&lt;p&gt;要阻止张量跟踪历史记录，可以调用.detach()方法将其与计算历史记录分离，并禁止跟踪它将来的计算记录。&lt;/p&gt;

&lt;p&gt;在自动梯度计算中还有另外一个重要的类Function.&lt;/p&gt;

&lt;p&gt;如果需要计算导数，你可以在Tensor上调用.backward()。&lt;br /&gt;
如果Tensor是一个标量（即它包含一个元素数据）则不需要为backward()指定任何参数，但是如果它有更多的元素，你需要指定一个gradient 参数来匹配张量的形状。&lt;/p&gt;

&lt;h3 id=&quot;过程&quot;&gt;过程&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;导入包
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;创建一个张量并设置 requires_grad=True 用来追踪他的计算历史
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = torch.ones(2, 2, requires_grad=True)
print(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;对张量进行操作:
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;y = x + 2
print(y)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;结果y已经被计算出来了，所以，grad_fn已经被自动生成了。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print(y.grad_fn)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;对y进行一个操作
```
out = z.mean()&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;print(z, out)&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;6. .requires_grad_( ... ) 
.requires_grad_( ... ) 可以改变现有张量的 requires_grad属性。 如果没有指定的话，默认输入的flag是 False。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;a = torch.randn(2, 2)
a = ((a * 3) / (a - 1))
print(a.requires_grad)
a.requires_grad_(True)
print(a.requires_grad)
b = (a * a).sum()
print(b.grad_fn)&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;False
True
&amp;lt;SumBackward0 object at 0x000002004F7D5608&amp;gt;
```&lt;/p&gt;
&lt;h2 id=&quot;梯度&quot;&gt;梯度&lt;/h2&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><category term="PyTorch" /><summary type="html">Autograd: 自动求导机制 PyTorch 中所有神经网络的核心是 autograd 包。 autograd包为张量上的所有操作提供了自动求导。 它是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。 张量（torch.Tensor） torch.Tensor是这个包的核心类。 如果设置 .requires_grad 为 True，那么将会追踪所有对于该张量的操作。 当完成计算后通过调用 .backward()，自动计算所有的梯度，这个张量的所有梯度将会自动积累到 .grad 属性。 要阻止张量跟踪历史记录，可以调用.detach()方法将其与计算历史记录分离，并禁止跟踪它将来的计算记录。 在自动梯度计算中还有另外一个重要的类Function. 如果需要计算导数，你可以在Tensor上调用.backward()。 如果Tensor是一个标量（即它包含一个元素数据）则不需要为backward()指定任何参数，但是如果它有更多的元素，你需要指定一个gradient 参数来匹配张量的形状。 过程 导入包 import torch 创建一个张量并设置 requires_grad=True 用来追踪他的计算历史 x = torch.ones(2, 2, requires_grad=True) print(x) 对张量进行操作: y = x + 2 print(y) 结果y已经被计算出来了，所以，grad_fn已经被自动生成了。 print(y.grad_fn) 对y进行一个操作 ``` out = z.mean() print(z, out) 6. .requires_grad_( ... ) .requires_grad_( ... ) 可以改变现有张量的 requires_grad属性。 如果没有指定的话，默认输入的flag是 False。 a = torch.randn(2, 2) a = ((a * 3) / (a - 1)) print(a.requires_grad) a.requires_grad_(True) print(a.requires_grad) b = (a * a).sum() print(b.grad_fn) False True &amp;lt;SumBackward0 object at 0x000002004F7D5608&amp;gt; ``` 梯度</summary></entry><entry><title type="html">彭寒薇，生日快乐！</title><link href="http://localhost:4000/2020/02/17/%E5%BD%AD%E5%AF%92%E8%96%87%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90.html" rel="alternate" type="text/html" title="彭寒薇，生日快乐！" /><published>2020-02-17T00:00:00+09:00</published><updated>2020-02-17T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/17/%E5%BD%AD%E5%AF%92%E8%96%87%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90!</id><content type="html" xml:base="http://localhost:4000/2020/02/17/%E5%BD%AD%E5%AF%92%E8%96%87%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90.html">&lt;p&gt;彭寒薇，生日快乐！&lt;/p&gt;

&lt;p&gt;这个生日一定是你过得最特别的一个，也是我给别人过得最特别的一个。&lt;/p&gt;

&lt;p&gt;祝你天天快乐，每天开心！&lt;/p&gt;

&lt;div&gt;&lt;div class=&quot;extensions extensions--video&quot;&gt;
  &lt;iframe src=&quot;//player.bilibili.com/player.html?aid=89520991&amp;amp;page=1&quot; frameborder=&quot;no&quot; scrolling=&quot;no&quot; allowfullscreen=&quot;true&quot;&gt;
  &lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="写给谁的情书" /><summary type="html">彭寒薇，生日快乐！ 这个生日一定是你过得最特别的一个，也是我给别人过得最特别的一个。 祝你天天快乐，每天开心！</summary></entry><entry><title type="html">请回答1988</title><link href="http://localhost:4000/2020/02/17/%E8%AF%B7%E5%9B%9E%E7%AD%941988.html" rel="alternate" type="text/html" title="请回答1988" /><published>2020-02-17T00:00:00+09:00</published><updated>2020-02-17T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/17/%E8%AF%B7%E5%9B%9E%E7%AD%941988</id><content type="html" xml:base="http://localhost:4000/2020/02/17/%E8%AF%B7%E5%9B%9E%E7%AD%941988.html">&lt;h1 id=&quot;写在开头&quot;&gt;写在开头&lt;/h1&gt;
&lt;p&gt;又开坑了这个电视剧，这次是二刷。姑且是下载好了，之后需要多长时间才能看完就不知道了。毕竟这个电视剧每一集的时间还是很长的，而我现在也没有那么多时间都话在看电视剧上面。&lt;/p&gt;

&lt;p&gt;慢慢看，慢慢写感想吧。&lt;/p&gt;

&lt;h3 id=&quot;2020年02月17日-164957&quot;&gt;2020年02月17日 16:49:57&lt;/h3&gt;
&lt;p&gt;发现我真的是一个多情的人，今天看了三集《请回答1988》。被剧中的剧情有所触动，哭的稀里哗啦的。说好的男儿有泪不轻弹呢，到我这边就不起效果了呀。&lt;/p&gt;

&lt;p&gt;虽然是第二遍看这个电视剧了，但是时隔这么多年再看感概还是那么的真实。&lt;br /&gt;
虽然细节的剧情已经全部都忘记了，但是演员塑造的人物性格还是记着一清二楚的。&lt;/p&gt;

&lt;p&gt;我想，我喜欢这个电视剧就是因为它讲的故事特别的真实吧。&lt;br /&gt;
其实也不是什么轰轰烈烈的故事，只不过是一个胡同，一个时代背景下，普普通通的三口人家的生活。但是在我看来，确实能治愈我的心灵。&lt;br /&gt;
让我明白一些道理，也让我知道大家都是怎么生活的。&lt;/p&gt;

&lt;h3 id=&quot;2020年02月21日-153207&quot;&gt;2020年02月21日 15:32:07&lt;/h3&gt;
&lt;p&gt;果然不管是什么时间看，什么岁数看，都是很让人感动的故事啊。&lt;br /&gt;
都是些生活的小事，却显得格外的真实。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="观后感" /><summary type="html">写在开头 又开坑了这个电视剧，这次是二刷。姑且是下载好了，之后需要多长时间才能看完就不知道了。毕竟这个电视剧每一集的时间还是很长的，而我现在也没有那么多时间都话在看电视剧上面。 慢慢看，慢慢写感想吧。 2020年02月17日 16:49:57 发现我真的是一个多情的人，今天看了三集《请回答1988》。被剧中的剧情有所触动，哭的稀里哗啦的。说好的男儿有泪不轻弹呢，到我这边就不起效果了呀。 虽然是第二遍看这个电视剧了，但是时隔这么多年再看感概还是那么的真实。 虽然细节的剧情已经全部都忘记了，但是演员塑造的人物性格还是记着一清二楚的。 我想，我喜欢这个电视剧就是因为它讲的故事特别的真实吧。 其实也不是什么轰轰烈烈的故事，只不过是一个胡同，一个时代背景下，普普通通的三口人家的生活。但是在我看来，确实能治愈我的心灵。 让我明白一些道理，也让我知道大家都是怎么生活的。 2020年02月21日 15:32:07 果然不管是什么时间看，什么岁数看，都是很让人感动的故事啊。 都是些生活的小事，却显得格外的真实。</summary></entry><entry xml:lang="zh"><title type="html">每日一笔</title><link href="http://localhost:4000/2020/02/09/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94.html" rel="alternate" type="text/html" title="每日一笔" /><published>2020-02-09T00:00:00+09:00</published><updated>2020-02-09T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/09/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94</id><content type="html" xml:base="http://localhost:4000/2020/02/09/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94.html">&lt;h2 id=&quot;每日一笔记录生活&quot;&gt;每日一笔，记录生活&lt;/h2&gt;
&lt;h3 id=&quot;2020年02月09日-114045&quot;&gt;2020年02月09日 11:40:45&lt;/h3&gt;
&lt;p&gt;今天早上偶然找到了戴胜的微博，点进去看了看之后，很是震惊。&lt;br /&gt;
原来在我努力讨好一个人的时候，我的情敌正在默默努力提高自己。&lt;br /&gt;
看来这就是敌我差距啊。不被人喜欢也是正常的事情啦。&lt;br /&gt;
好好加油啊，陈。&lt;/p&gt;

&lt;p&gt;今天开始收拾东西，明天搬家。&lt;br /&gt;
东西越收拾越多，已经有两个大箱子了。&lt;br /&gt;
最可怕的是搬家的时候找不到人帮忙，看来我做人真的是有问题的吧。&lt;/p&gt;

&lt;p&gt;加油收拾家，努力生活。
今日一笔结束。&lt;/p&gt;

&lt;h3 id=&quot;2020年02月11日-001111&quot;&gt;2020年02月11日 00:11:11&lt;/h3&gt;
&lt;p&gt;说是每日一笔，其实现在已经是第二天的凌晨了。&lt;/p&gt;

&lt;p&gt;今天一天过得特别忙碌，没什么时间写东西，也没什么时间拍照片呀。&lt;br /&gt;
忙是忙了些，感觉上还是很充实的。&lt;br /&gt;
今天打工的时候想，拿出珍藏已久的口琴吹一吹。&lt;br /&gt;
又想到吹太大声音的话，会不会影响到周围的邻居啊。别过几天再被人举报了。&lt;br /&gt;
那就太尴尬了。😅&lt;/p&gt;

&lt;p&gt;话说是不是应该剪视频了，之前的素材还存着挺多的呢。
努力生活呀，陈。&lt;/p&gt;

&lt;p&gt;今日一笔结束。&lt;/p&gt;

&lt;h3 id=&quot;2020年02月12日-110949&quot;&gt;2020年02月12日 11:09:49&lt;/h3&gt;
&lt;p&gt;搬家之后特别的忙，有很多事情等着去处理。&lt;/p&gt;

&lt;p&gt;其实已经不知道第几次觉得自己时间不够用了。&lt;br /&gt;
也不知道是不是时间利用的效率不够高。每天起床的时间都挺早的，可惜还是觉得时间很紧张。要是一天能多一点时间给我就好啦！&lt;/p&gt;

&lt;p&gt;今天早上走上山，感觉还不错。&lt;br /&gt;
犹记得以前的自己就很爱散步，中午休息的时间都会去公司周围的公园走上四十分钟。到了日本之后也许是住的离学校比较近，每天运动的时间都很少了。&lt;br /&gt;
趁着这个机会，以后正好可以多运动一下。虽然上下学的时候不方便，但是能运动就是很好的机会啦！而且这边环境着实不错，多运动运动有益身心健康。&lt;br /&gt;
跑步的事情差不多也要继续开始了。前几天下的小雪也消融了，正好是一个重新开始的好机会。&lt;/p&gt;

&lt;p&gt;加油生活呀，陈。&lt;/p&gt;

&lt;h3 id=&quot;2020年02月13日-151008&quot;&gt;2020年02月13日 15:10:08&lt;/h3&gt;
&lt;p&gt;昨天晚上出去跑步了。&lt;br /&gt;
这几天金泽的天气实在太不给力了，才刚跑了一会儿就开始下雨了。虽然不是瓢泼大雨，但还是有一些影响。&lt;br /&gt;
然后很长时间没有进行运动，体力下降的太快了。昨天才跑了2km多一点，晚上的时候就觉得大腿内侧有一些疼痛。顺便一说，昨天的配速有5.5min/km，虽然跑的有些快，但也不应该腿会这么疼。&lt;br /&gt;
之前买了一个迪卡侬的夜跑灯，昨天用了一下之后感觉太好用啦实在是，简直就是夜跑的利器啊！&lt;br /&gt;
昨天还买了红酒喝！感觉自己的生活越来越懂得享受了。晚上的时候喝一杯刚好睡觉，很享受微醺的感觉。&lt;br /&gt;
今天中午第一次在会馆做饭，认识了很多新的同学。&lt;/p&gt;

&lt;p&gt;我要先做好自己，然后再做其他的事情。加油！&lt;/p&gt;

&lt;h3 id=&quot;2020年02月14日-095432&quot;&gt;2020年02月14日 09:54:32&lt;/h3&gt;
&lt;p&gt;昨天继续跑步，3km，天气很给力没有闹小矛盾。&lt;br /&gt;
侥幸买到了两盒口罩，还买了一包暖宝宝。&lt;br /&gt;
今天继续录视频，在找人帮忙的路上充满了坎坷。应该仔细反思是不是自己为人处事出现了问题。&lt;br /&gt;
准备好了情人节巧克力，在イオン纠结了半天，巧克力的种类实在是太多了，头秃的很。&lt;/p&gt;

&lt;p&gt;突然发现，就我个人来说。有压力，有对手的时候进步最快。以前也是这样的，考完修士之后感觉自己失去了目标，然后又是各种乱七八糟的事情搞得每天不知道自己在干什么。一些好的习惯都渐渐的忘掉了，也降低了对自己的要求。&lt;br /&gt;
现在看到了对手，也看到了未来。虽然明确的方向还是没有，但是比起计划总是能先行动起来了。我相信只要保持下去，就能找回原来的自己。&lt;br /&gt;
原来，自己和自己奋斗，也可以变成一个有趣的人。&lt;/p&gt;

&lt;h3 id=&quot;2020年02月16日-233210&quot;&gt;2020年02月16日 23:32:10&lt;/h3&gt;
&lt;p&gt;又是连着两天没有没有跑步。&lt;br /&gt;
昨天晚上打工，今天下了一整天的小雨。总是想跑步的时候，总是有一些额外的事情出来干扰我的计划。&lt;br /&gt;
心烦。&lt;/p&gt;

&lt;p&gt;今天和黄同学收了一个小太阳。虽然最近的温度有所回升，但是晚上的时候还是能感受到屋子里面的温度明显低了很多。有了小太阳之后，也感觉不是那么的寒冷了。&lt;br /&gt;
想起来在上海的时候，那个时候心疼电费，冬天不舍得开空调。洗完澡的时候也是最难熬的时候，因为冷的透彻心扉。那个时候就靠着一个小小的小太阳度过每个难熬的夜晚。那段时间的经历真的让人记忆深刻。&lt;br /&gt;
晚上睡觉的时候开一会儿电热毯才能入睡，关掉之后必须马上进入梦乡，要不然特别的冷。&lt;br /&gt;
距离那个时候过去了差不多一年的时间，有时会想起那时的生活。每天虽然很辛苦，但是能感受到在真真正正的奋斗着。一个人在陌生的城市，既孤独又拼搏。&lt;/p&gt;

&lt;p&gt;年轻时候的人生就是这样的吧。&lt;br /&gt;
每日一笔结束。&lt;/p&gt;

&lt;h3 id=&quot;2020年02月18日-142225&quot;&gt;2020年02月18日 14:22:25&lt;/h3&gt;
&lt;p&gt;中午的时候去图书馆坐了一会儿，就是上次那个位置。&lt;br /&gt;
上次去过一次之后就爱上了，视野很开阔，能看到很不错风景。&lt;br /&gt;
今天中午的时候又飘了一会儿雪花，景色真的很不错。&lt;/p&gt;

&lt;p&gt;要走的时候回头看到一个人趴在桌子上，和你很像。&lt;br /&gt;
路过的时候仔细看了看，趴在桌子上也看不到正面。&lt;br /&gt;
但是衣服，裤子，鞋都很像。&lt;br /&gt;
也许是我看错了，我想世界上应该没有两个人，穿衣打扮都这么相似的吧。&lt;br /&gt;
而且这个时候你应该还在做实验。之后才想到，其实应该看看袜子像不像的吧。&lt;/p&gt;

&lt;p&gt;后来还是默默的走开了，回去的路上在想，是不是应该买一杯咖啡放在桌子上。&lt;br /&gt;
但是又觉得太自作多情了，这种剧情应该只存在于电影故事之中吧。&lt;/p&gt;

&lt;p&gt;昨天买了布鲁斯口琴，明天就能到啦！&lt;br /&gt;
加油生活呀！&lt;/p&gt;

&lt;h3 id=&quot;2020年02月21日-225039&quot;&gt;2020年02月21日 22:50:39&lt;/h3&gt;
&lt;p&gt;许就不更新每日一笔了。&lt;br /&gt;
每天的生活都很忙碌，虽然没有干什么有意义的事情，但都过得很充实。&lt;/p&gt;

&lt;p&gt;今天去吃了一家德国料理。其实是一家居酒屋，主营的也是德国啤酒。&lt;br /&gt;
还记得上次喝到这样的啤酒是在7月份的啤酒节上，当时留下的印象很深刻。&lt;br /&gt;
今天点了一份蘑菇酱的意大利面，味道很浓厚。香肠和猪肘的味道也还不错，啤酒也很好喝。&lt;br /&gt;
就是价钱稍微有些偏贵，做为二次会的地方是一个很不错的去处。&lt;br /&gt;
店内的装潢别具一格，很有德国的感觉。&lt;br /&gt;
以后应该还是会再来一次的吧，为了蘑菇酱的意大利面。&lt;/p&gt;

&lt;h3 id=&quot;2020年02月24日-122825&quot;&gt;2020年02月24日 12:28:25&lt;/h3&gt;
&lt;p&gt;每日一笔，隔天更新。&lt;br /&gt;
高姐姐终究还是回国了，昨天大家一起去送得她离开金泽。&lt;br /&gt;
成人世界的竞争有的时候就是这么的激烈，成王败寇。&lt;br /&gt;
合格就留下来，不合格就离开。只是没想到教授这次这么的严厉，没有一点回旋的余地。 
今后共勉。&lt;/p&gt;

&lt;p&gt;昨天去一家拉面店吃饭的时候，走进店里刚坐下来说了两句话，旁边一桌的日本人估计是看到我们是中国人就去和店员说想换位置，之后就换到了其他距离我们比较远的一桌。&lt;br /&gt;
我想我应该是受到了歧视，因为这次病毒而受到的歧视。&lt;br /&gt;
这件事说大不大，说小不小。但给我的感触还是挺大的。&lt;/p&gt;

&lt;p&gt;我能理解这样的行为，但是心里终究还是会有不舒服的感觉。&lt;br /&gt;
而且目前看来，日本要比国内危险多了。走在大街上，戴口罩的日本人寥寥无几，大部分小心翼翼的都是国人。&lt;br /&gt;
之前看新闻也能看到在不同的地方都会或多或少的发生歧视行为。&lt;/p&gt;

&lt;p&gt;但我想，病毒面前人人平等。歧视并不是一个能解决问题的方法。就像这次很多人都会去评论武汉人。但我想没必要这样，因为大家都很无辜。在这种突发性事件面前，我们每个人都是受害者。&lt;br /&gt;
也没有一个人能拿在这样的条件下独善其身。所以我们应该保护好自己，然后再帮助其他人。&lt;/p&gt;

&lt;p&gt;希望这次疫情快些过去吧。共勉。&lt;br /&gt;
每日一笔结束。&lt;/p&gt;

&lt;h3 id=&quot;2020年02月25日-115959&quot;&gt;2020年02月25日 11:59:59&lt;/h3&gt;
&lt;p&gt;今天忽然想到，如果这件事情发生到我身上我应该怎么面对呢？&lt;br /&gt;
就是我喜欢的女孩，我追求过的女孩，在我对她好的时候，她正躺在别人的怀里。&lt;br /&gt;
最可怕的是我一点都不知道，她一直把我当成朋友，但还接受我对她所有的好意。言语之间都透露着给我机会的希望，但是心里真实的想法确实和我当朋友。&lt;br /&gt;
也许在女生的角度来看确实已经明确的表达过拒绝的语言了。所以这就是一个通行证，可以接受所有的好意和付出。&lt;/p&gt;

&lt;p&gt;信息不对等的话真的太可怕了。虽然这次的主角不是我，我更多的是感到不值和可怜，为他的付出。但是我想未来的我也无法避免这样的事情发生在自己身上，到了那个时候我应该怎么做呢？&lt;br /&gt;
我一直不是一个能狠下心的人，我想我应该也会接着当舔狗吧。&lt;br /&gt;
就算知道残忍的真相之后。&lt;br /&gt;
真实可悲呢，你。&lt;/p&gt;

&lt;p&gt;不过以上都是假设，是幻想的。过好自己的生活吧。&lt;br /&gt;
每日一笔结束。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><summary type="html">每日一笔，记录生活 2020年02月09日 11:40:45 今天早上偶然找到了戴胜的微博，点进去看了看之后，很是震惊。 原来在我努力讨好一个人的时候，我的情敌正在默默努力提高自己。 看来这就是敌我差距啊。不被人喜欢也是正常的事情啦。 好好加油啊，陈。 今天开始收拾东西，明天搬家。 东西越收拾越多，已经有两个大箱子了。 最可怕的是搬家的时候找不到人帮忙，看来我做人真的是有问题的吧。 加油收拾家，努力生活。 今日一笔结束。 2020年02月11日 00:11:11 说是每日一笔，其实现在已经是第二天的凌晨了。 今天一天过得特别忙碌，没什么时间写东西，也没什么时间拍照片呀。 忙是忙了些，感觉上还是很充实的。 今天打工的时候想，拿出珍藏已久的口琴吹一吹。 又想到吹太大声音的话，会不会影响到周围的邻居啊。别过几天再被人举报了。 那就太尴尬了。😅 话说是不是应该剪视频了，之前的素材还存着挺多的呢。 努力生活呀，陈。 今日一笔结束。 2020年02月12日 11:09:49 搬家之后特别的忙，有很多事情等着去处理。 其实已经不知道第几次觉得自己时间不够用了。 也不知道是不是时间利用的效率不够高。每天起床的时间都挺早的，可惜还是觉得时间很紧张。要是一天能多一点时间给我就好啦！ 今天早上走上山，感觉还不错。 犹记得以前的自己就很爱散步，中午休息的时间都会去公司周围的公园走上四十分钟。到了日本之后也许是住的离学校比较近，每天运动的时间都很少了。 趁着这个机会，以后正好可以多运动一下。虽然上下学的时候不方便，但是能运动就是很好的机会啦！而且这边环境着实不错，多运动运动有益身心健康。 跑步的事情差不多也要继续开始了。前几天下的小雪也消融了，正好是一个重新开始的好机会。 加油生活呀，陈。 2020年02月13日 15:10:08 昨天晚上出去跑步了。 这几天金泽的天气实在太不给力了，才刚跑了一会儿就开始下雨了。虽然不是瓢泼大雨，但还是有一些影响。 然后很长时间没有进行运动，体力下降的太快了。昨天才跑了2km多一点，晚上的时候就觉得大腿内侧有一些疼痛。顺便一说，昨天的配速有5.5min/km，虽然跑的有些快，但也不应该腿会这么疼。 之前买了一个迪卡侬的夜跑灯，昨天用了一下之后感觉太好用啦实在是，简直就是夜跑的利器啊！ 昨天还买了红酒喝！感觉自己的生活越来越懂得享受了。晚上的时候喝一杯刚好睡觉，很享受微醺的感觉。 今天中午第一次在会馆做饭，认识了很多新的同学。 我要先做好自己，然后再做其他的事情。加油！ 2020年02月14日 09:54:32 昨天继续跑步，3km，天气很给力没有闹小矛盾。 侥幸买到了两盒口罩，还买了一包暖宝宝。 今天继续录视频，在找人帮忙的路上充满了坎坷。应该仔细反思是不是自己为人处事出现了问题。 准备好了情人节巧克力，在イオン纠结了半天，巧克力的种类实在是太多了，头秃的很。 突然发现，就我个人来说。有压力，有对手的时候进步最快。以前也是这样的，考完修士之后感觉自己失去了目标，然后又是各种乱七八糟的事情搞得每天不知道自己在干什么。一些好的习惯都渐渐的忘掉了，也降低了对自己的要求。 现在看到了对手，也看到了未来。虽然明确的方向还是没有，但是比起计划总是能先行动起来了。我相信只要保持下去，就能找回原来的自己。 原来，自己和自己奋斗，也可以变成一个有趣的人。 2020年02月16日 23:32:10 又是连着两天没有没有跑步。 昨天晚上打工，今天下了一整天的小雨。总是想跑步的时候，总是有一些额外的事情出来干扰我的计划。 心烦。 今天和黄同学收了一个小太阳。虽然最近的温度有所回升，但是晚上的时候还是能感受到屋子里面的温度明显低了很多。有了小太阳之后，也感觉不是那么的寒冷了。 想起来在上海的时候，那个时候心疼电费，冬天不舍得开空调。洗完澡的时候也是最难熬的时候，因为冷的透彻心扉。那个时候就靠着一个小小的小太阳度过每个难熬的夜晚。那段时间的经历真的让人记忆深刻。 晚上睡觉的时候开一会儿电热毯才能入睡，关掉之后必须马上进入梦乡，要不然特别的冷。 距离那个时候过去了差不多一年的时间，有时会想起那时的生活。每天虽然很辛苦，但是能感受到在真真正正的奋斗着。一个人在陌生的城市，既孤独又拼搏。 年轻时候的人生就是这样的吧。 每日一笔结束。 2020年02月18日 14:22:25 中午的时候去图书馆坐了一会儿，就是上次那个位置。 上次去过一次之后就爱上了，视野很开阔，能看到很不错风景。 今天中午的时候又飘了一会儿雪花，景色真的很不错。 要走的时候回头看到一个人趴在桌子上，和你很像。 路过的时候仔细看了看，趴在桌子上也看不到正面。 但是衣服，裤子，鞋都很像。 也许是我看错了，我想世界上应该没有两个人，穿衣打扮都这么相似的吧。 而且这个时候你应该还在做实验。之后才想到，其实应该看看袜子像不像的吧。 后来还是默默的走开了，回去的路上在想，是不是应该买一杯咖啡放在桌子上。 但是又觉得太自作多情了，这种剧情应该只存在于电影故事之中吧。 昨天买了布鲁斯口琴，明天就能到啦！ 加油生活呀！ 2020年02月21日 22:50:39 许就不更新每日一笔了。 每天的生活都很忙碌，虽然没有干什么有意义的事情，但都过得很充实。 今天去吃了一家德国料理。其实是一家居酒屋，主营的也是德国啤酒。 还记得上次喝到这样的啤酒是在7月份的啤酒节上，当时留下的印象很深刻。 今天点了一份蘑菇酱的意大利面，味道很浓厚。香肠和猪肘的味道也还不错，啤酒也很好喝。 就是价钱稍微有些偏贵，做为二次会的地方是一个很不错的去处。 店内的装潢别具一格，很有德国的感觉。 以后应该还是会再来一次的吧，为了蘑菇酱的意大利面。 2020年02月24日 12:28:25 每日一笔，隔天更新。 高姐姐终究还是回国了，昨天大家一起去送得她离开金泽。 成人世界的竞争有的时候就是这么的激烈，成王败寇。 合格就留下来，不合格就离开。只是没想到教授这次这么的严厉，没有一点回旋的余地。 今后共勉。 昨天去一家拉面店吃饭的时候，走进店里刚坐下来说了两句话，旁边一桌的日本人估计是看到我们是中国人就去和店员说想换位置，之后就换到了其他距离我们比较远的一桌。 我想我应该是受到了歧视，因为这次病毒而受到的歧视。 这件事说大不大，说小不小。但给我的感触还是挺大的。 我能理解这样的行为，但是心里终究还是会有不舒服的感觉。 而且目前看来，日本要比国内危险多了。走在大街上，戴口罩的日本人寥寥无几，大部分小心翼翼的都是国人。 之前看新闻也能看到在不同的地方都会或多或少的发生歧视行为。 但我想，病毒面前人人平等。歧视并不是一个能解决问题的方法。就像这次很多人都会去评论武汉人。但我想没必要这样，因为大家都很无辜。在这种突发性事件面前，我们每个人都是受害者。 也没有一个人能拿在这样的条件下独善其身。所以我们应该保护好自己，然后再帮助其他人。 希望这次疫情快些过去吧。共勉。 每日一笔结束。 2020年02月25日 11:59:59 今天忽然想到，如果这件事情发生到我身上我应该怎么面对呢？ 就是我喜欢的女孩，我追求过的女孩，在我对她好的时候，她正躺在别人的怀里。 最可怕的是我一点都不知道，她一直把我当成朋友，但还接受我对她所有的好意。言语之间都透露着给我机会的希望，但是心里真实的想法确实和我当朋友。 也许在女生的角度来看确实已经明确的表达过拒绝的语言了。所以这就是一个通行证，可以接受所有的好意和付出。 信息不对等的话真的太可怕了。虽然这次的主角不是我，我更多的是感到不值和可怜，为他的付出。但是我想未来的我也无法避免这样的事情发生在自己身上，到了那个时候我应该怎么做呢？ 我一直不是一个能狠下心的人，我想我应该也会接着当舔狗吧。 就算知道残忍的真相之后。 真实可悲呢，你。 不过以上都是假设，是幻想的。过好自己的生活吧。 每日一笔结束。</summary></entry></feed>
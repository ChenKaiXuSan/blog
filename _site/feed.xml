<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh"><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="zh" /><updated>2020-09-17T18:49:46+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">旭的小窝</title><subtitle>这里讲我的故事
</subtitle><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><entry><title type="html">NIPS 2016 Tutorial:Generative Adversarial Networks</title><link href="http://localhost:4000/2020/09/17/NIPS_GAN_Tutorial.html" rel="alternate" type="text/html" title="NIPS 2016 Tutorial:Generative Adversarial Networks" /><published>2020-09-17T00:00:00+09:00</published><updated>2020-09-17T00:00:00+09:00</updated><id>http://localhost:4000/2020/09/17/NIPS_GAN_Tutorial</id><content type="html" xml:base="http://localhost:4000/2020/09/17/NIPS_GAN_Tutorial.html">&lt;h2 id=&quot;摘要&quot;&gt;摘要&lt;/h2&gt;
&lt;p&gt;这篇论文总结了在NIPS2016上作者提出的向导。&lt;br /&gt;
这个向导主要包括：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;为什么生成模型值得研究&lt;/li&gt;
  &lt;li&gt;生成模型怎么工作，以及GANs与其他生成模型的比较&lt;/li&gt;
  &lt;li&gt;GANs工作的一些细节&lt;/li&gt;
  &lt;li&gt;GAN相关的研究课题&lt;/li&gt;
  &lt;li&gt;结合GAN与其他发放的最先进图像模型
最后，这篇向导为读者提供了3个练习，以及练习的答案。&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;介绍&quot;&gt;介绍&lt;/h1&gt;
&lt;p&gt;这个报告总结了在2016NIPS大会上关于GANs的内容。为了确保这个向导对观众最有用，这个向导主要面向去确定在大会初期由观众提出的问题。这个向导不计划全面的介绍GANs的领域；很多出彩的论文没有在这里被提及，只是因为这些论文没有对频繁出现的问题作出解释，另外因为这个向导被设置为2个小时的口述演讲，也没有无限的时间覆盖所有的研究内容。&lt;/p&gt;

&lt;p&gt;这个向导将介绍：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;为什么生成模型是一个值得研究的题目&lt;/li&gt;
  &lt;li&gt;生成模型是如何工作的，以及GAN与其他生成模型的区别&lt;/li&gt;
  &lt;li&gt;GANs工作的细节&lt;/li&gt;
  &lt;li&gt;GANs的相关研究课题&lt;/li&gt;
  &lt;li&gt;最先进的包含GANs和其他方法的图像模型
最后，这个向导包含3个练习给读者完成，同时提供了这3个练习的答案。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;此向导的幻灯片可以查看在PDF和Keynote格式在下面的链接中：
http://www.iangoodfellow.com/slides/2016-12-04-NIPS.pdf 
http://www.iangoodfellow.com/slides/2016-12-04-NIPS.key&lt;/p&gt;

&lt;p&gt;视频被NIPS基金会录制，可以在稍后进行观看。
https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Generative-Adversarial-Networks&lt;/p&gt;

&lt;p class=&quot;border&quot;&gt;生成对抗网络是生成模型的一个分类。生成模型在很多方面有应用。在这个向导中，这个模型指的是用某种方式去学习并表达针对一个训练数据集分布的估计，也就是$p_{data}$,然后学习描绘一个估计对于一些分布。结果是一个概率分布$p_{model}$。在一些例子中，模型明确的估计$p_{model}$，就像图一那样。在其他情况下，模型只可能从$p_{model}$中生成样本，如图2。一些模型可以两个都做。GANs主要关注在样本生成，尽管可以设计GANs都做两者。  &lt;br /&gt;
&lt;img src=&quot;/image/gan/201704/28/fig01.png&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;border&quot;&gt;Figure1:有些生成模型通过密度估计。这些模型通过从未知的数据生成分布$p_{data}$来获取训练数据样本，然后得到此未知分布的估计。$p_{model}$估计可以给一个特殊的x值进行估计以获得对于真实密度$p_{model}(x)$的$p_{model}(x)$估计。这个图片展示了一个一维数据收集样本的过程，和一个高斯模型。
&lt;img src=&quot;/image/gan/201704/28/fig02.png&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure2:一些生成模型可以从模型分布中生成样本。此图介绍的过程中，我们展示了ImageNet数据集中的样本。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><summary type="html">摘要 这篇论文总结了在NIPS2016上作者提出的向导。 这个向导主要包括： 为什么生成模型值得研究 生成模型怎么工作，以及GANs与其他生成模型的比较 GANs工作的一些细节 GAN相关的研究课题 结合GAN与其他发放的最先进图像模型 最后，这篇向导为读者提供了3个练习，以及练习的答案。 介绍 这个报告总结了在2016NIPS大会上关于GANs的内容。为了确保这个向导对观众最有用，这个向导主要面向去确定在大会初期由观众提出的问题。这个向导不计划全面的介绍GANs的领域；很多出彩的论文没有在这里被提及，只是因为这些论文没有对频繁出现的问题作出解释，另外因为这个向导被设置为2个小时的口述演讲，也没有无限的时间覆盖所有的研究内容。 这个向导将介绍： 为什么生成模型是一个值得研究的题目 生成模型是如何工作的，以及GAN与其他生成模型的区别 GANs工作的细节 GANs的相关研究课题 最先进的包含GANs和其他方法的图像模型 最后，这个向导包含3个练习给读者完成，同时提供了这3个练习的答案。 此向导的幻灯片可以查看在PDF和Keynote格式在下面的链接中： http://www.iangoodfellow.com/slides/2016-12-04-NIPS.pdf http://www.iangoodfellow.com/slides/2016-12-04-NIPS.key 视频被NIPS基金会录制，可以在稍后进行观看。 https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Generative-Adversarial-Networks 生成对抗网络是生成模型的一个分类。生成模型在很多方面有应用。在这个向导中，这个模型指的是用某种方式去学习并表达针对一个训练数据集分布的估计，也就是$p_{data}$,然后学习描绘一个估计对于一些分布。结果是一个概率分布$p_{model}$。在一些例子中，模型明确的估计$p_{model}$，就像图一那样。在其他情况下，模型只可能从$p_{model}$中生成样本，如图2。一些模型可以两个都做。GANs主要关注在样本生成，尽管可以设计GANs都做两者。 Figure1:有些生成模型通过密度估计。这些模型通过从未知的数据生成分布$p_{data}$来获取训练数据样本，然后得到此未知分布的估计。$p_{model}$估计可以给一个特殊的x值进行估计以获得对于真实密度$p_{model}(x)$的$p_{model}(x)$估计。这个图片展示了一个一维数据收集样本的过程，和一个高斯模型。 Figure2:一些生成模型可以从模型分布中生成样本。此图介绍的过程中，我们展示了ImageNet数据集中的样本。</summary></entry><entry xml:lang="zh"><title type="html">每日一笔</title><link href="http://localhost:4000/2020/08/03/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94.html" rel="alternate" type="text/html" title="每日一笔" /><published>2020-08-03T00:00:00+09:00</published><updated>2020-08-03T00:00:00+09:00</updated><id>http://localhost:4000/2020/08/03/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94</id><content type="html" xml:base="http://localhost:4000/2020/08/03/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94.html">&lt;h1 id=&quot;2020年08月03日-181609&quot;&gt;2020年08月03日 18:16:09&lt;/h1&gt;
&lt;p&gt;八月伊始，天气依旧是那么闷热。&lt;/p&gt;

&lt;p&gt;用训练好的VGG16直接跑数据，速度要来的快很多。&lt;br /&gt;
毕竟只需要跑后面的两个dense层，前面的卷积基都是封装在模型中的。&lt;br /&gt;
今天也深入了解了以下dropout的作用。虽然还不知道数学上的实现，但大致是干什么用的心中还是有一点意识了。&lt;/p&gt;

&lt;p&gt;老李和老谢的论文过了，虽然gcce很水，但心中多少还是有羡慕的情绪的。&lt;br /&gt;
年底的时候我要是能发一篇论文出来就好了啊。&lt;br /&gt;
也许不是那么难的吧。&lt;/p&gt;

&lt;p&gt;基础看了有一半了吧，差不多该开始看论文了我觉得。&lt;br /&gt;
就从过去的论文开始看吧。&lt;/p&gt;

&lt;h1 id=&quot;2020年08月06日-134306&quot;&gt;2020年08月06日 13:43:06&lt;/h1&gt;
&lt;p&gt;天气依旧是这么的炎热啊。&lt;br /&gt;
啥时候才能进入秋天呢，喜欢凉爽的感觉。&lt;/p&gt;

&lt;h1 id=&quot;2020年08月10日-171759&quot;&gt;2020年08月10日 17:17:59&lt;/h1&gt;
&lt;p&gt;梅雨季节应该过去了，最近几天一直都是大晴天的样子。&lt;/p&gt;

&lt;p&gt;tensorflow真的没有想象中的那么好用，总是会和cuda和cudnn发生版本的冲突。&lt;br /&gt;
每次都无法调用显卡，调试还要半天。&lt;br /&gt;
看看pytorch，实在不行用这个吧。&lt;br /&gt;
现在新论文里面用pytorch的貌似人还很多。&lt;br /&gt;
不过两个工具应该都是大同小异的感觉吧。&lt;/p&gt;

&lt;h1 id=&quot;2020年08月13日-162725&quot;&gt;2020年08月13日 16:27:25&lt;/h1&gt;
&lt;p&gt;今天去驾校练了车。&lt;br /&gt;
感觉有很长时间没有开车了吧，有一年的时间了。&lt;br /&gt;
练车的价格是真的贵，而且时间还很短。 &lt;br /&gt;
希望能尽快把驾照换出来吧。&lt;br /&gt;
不过这几次去驾校的时候都会下雨，今天也是小雨就一直没有停过。&lt;br /&gt;
不过骑自行车过去的话速度还是挺快的，本来距离就没有多远，坐公交的话实在是太浪费时间了。&lt;/p&gt;

&lt;p&gt;金泽的天气真的说下雨就下雨，完全没有一点预兆。&lt;br /&gt;
开始使用pytorch了。&lt;/p&gt;

&lt;p&gt;最近有点想要搬家了。&lt;/p&gt;

&lt;h1 id=&quot;2020年08月18日-181643&quot;&gt;2020年08月18日 18:16:43&lt;/h1&gt;
&lt;p&gt;今天居然记错了上课的时间，最后一节英语课。&lt;br /&gt;
没想到一个学期都没有翘过的英语课，最后一节就这么的放弃了。&lt;br /&gt;
说实话这个课太无聊了，而且日本的大家说英语多少是有点听不懂。日式英语可是太难理解了啊。&lt;br /&gt;
不过从明天开始这学期所有的课就都结束啦，可以花更多的时间来看书了，就看自己的一直如何了。&lt;/p&gt;

&lt;p&gt;昨天看了这季度的新番，《租界女友》还是很新颖的。&lt;br /&gt;
然后今天又看了《月色真美》。&lt;/p&gt;

&lt;p&gt;真好啊，青春的青春时光，看着的时候觉得自己的青春又回来了啊。&lt;br /&gt;
这可真的是爷青回啊。&lt;/p&gt;

&lt;p&gt;青春太棒了啊，可恶(〃＞目＜)&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="每日一笔" /><summary type="html">2020年08月03日 18:16:09 八月伊始，天气依旧是那么闷热。 用训练好的VGG16直接跑数据，速度要来的快很多。 毕竟只需要跑后面的两个dense层，前面的卷积基都是封装在模型中的。 今天也深入了解了以下dropout的作用。虽然还不知道数学上的实现，但大致是干什么用的心中还是有一点意识了。 老李和老谢的论文过了，虽然gcce很水，但心中多少还是有羡慕的情绪的。 年底的时候我要是能发一篇论文出来就好了啊。 也许不是那么难的吧。 基础看了有一半了吧，差不多该开始看论文了我觉得。 就从过去的论文开始看吧。 2020年08月06日 13:43:06 天气依旧是这么的炎热啊。 啥时候才能进入秋天呢，喜欢凉爽的感觉。 2020年08月10日 17:17:59 梅雨季节应该过去了，最近几天一直都是大晴天的样子。 tensorflow真的没有想象中的那么好用，总是会和cuda和cudnn发生版本的冲突。 每次都无法调用显卡，调试还要半天。 看看pytorch，实在不行用这个吧。 现在新论文里面用pytorch的貌似人还很多。 不过两个工具应该都是大同小异的感觉吧。 2020年08月13日 16:27:25 今天去驾校练了车。 感觉有很长时间没有开车了吧，有一年的时间了。 练车的价格是真的贵，而且时间还很短。 希望能尽快把驾照换出来吧。 不过这几次去驾校的时候都会下雨，今天也是小雨就一直没有停过。 不过骑自行车过去的话速度还是挺快的，本来距离就没有多远，坐公交的话实在是太浪费时间了。 金泽的天气真的说下雨就下雨，完全没有一点预兆。 开始使用pytorch了。 最近有点想要搬家了。 2020年08月18日 18:16:43 今天居然记错了上课的时间，最后一节英语课。 没想到一个学期都没有翘过的英语课，最后一节就这么的放弃了。 说实话这个课太无聊了，而且日本的大家说英语多少是有点听不懂。日式英语可是太难理解了啊。 不过从明天开始这学期所有的课就都结束啦，可以花更多的时间来看书了，就看自己的一直如何了。 昨天看了这季度的新番，《租界女友》还是很新颖的。 然后今天又看了《月色真美》。 真好啊，青春的青春时光，看着的时候觉得自己的青春又回来了啊。 这可真的是爷青回啊。 青春太棒了啊，可恶(〃＞目＜)</summary></entry><entry xml:lang="zh"><title type="html">每日一笔</title><link href="http://localhost:4000/2020/07/13/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94.html" rel="alternate" type="text/html" title="每日一笔" /><published>2020-07-13T00:00:00+09:00</published><updated>2020-07-13T00:00:00+09:00</updated><id>http://localhost:4000/2020/07/13/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94</id><content type="html" xml:base="http://localhost:4000/2020/07/13/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94.html">&lt;h3 id=&quot;2020年07月03日-161231&quot;&gt;2020年07月03日 16:12:31&lt;/h3&gt;
&lt;p&gt;许就没有来研究室了。
今天来的人还算很多，没有见到教授。&lt;/p&gt;

&lt;p&gt;免费的空调太舒服了啊，哈哈哈。&lt;/p&gt;

&lt;h3 id=&quot;2020年07月07日-163111&quot;&gt;2020年07月07日 16:31:11&lt;/h3&gt;
&lt;p&gt;今天上了三个小时的英语课，听着日本人说着日式英语觉得还挺好玩的这节课。&lt;br /&gt;
虽然上课的时间有点长就是了。&lt;/p&gt;

&lt;p&gt;然后接着来研究室蹭空调吧，不过大家都没来貌似。&lt;br /&gt;
一个人坐在这里也挺安心的。&lt;/p&gt;

&lt;h3 id=&quot;2020年07月09日-181418&quot;&gt;2020年07月09日 18:14:18&lt;/h3&gt;
&lt;p&gt;要学的东西太多了，感觉压力很大。&lt;br /&gt;
这学期的报告也太多了，有些都是没什么意义的报告，但为了学分也要写。&lt;/p&gt;

&lt;h3 id=&quot;2020年07月10日-154612&quot;&gt;2020年07月10日 15:46:12&lt;/h3&gt;
&lt;p&gt;今天去把驾照翻译了，马上要开始换驾照啦。&lt;br /&gt;
还久违的去逛了香林坊，真的疫情开始之后就再也没有去过了啊。&lt;/p&gt;

&lt;p&gt;下了一天的下雨，真的是梅雨季节啊。&lt;/p&gt;

&lt;h3 id=&quot;2020年07月13日-180734&quot;&gt;2020年07月13日 18:07:34&lt;/h3&gt;
&lt;p&gt;金泽的小雨还是一直下，梅雨天气一直持续着。&lt;/p&gt;

&lt;p&gt;最近看电视剧太多了，有点太疯狂了。&lt;br /&gt;
现在人总是不能接受延迟满足，总是寻求一时的刺激获得满足。&lt;br /&gt;
真是变得越来越没有耐心了。&lt;/p&gt;

&lt;h3 id=&quot;2020年07月16日-144817&quot;&gt;2020年07月16日 14:48:17&lt;/h3&gt;
&lt;p&gt;终于不下雨了，不过今天也实在是太热了吧！&lt;/p&gt;

&lt;p&gt;昨天买到了半价羊肉，还买了半价牛肉，真是太难得了啊。&lt;br /&gt;
不过今天的牛肉套料放多了，现在嘴里还有一股调料味。🤦‍&lt;/p&gt;

&lt;p&gt;不下雨太好了，今天终于能跑步了。&lt;br /&gt;
加油看论文吧。&lt;/p&gt;

&lt;h3 id=&quot;2020年07月20日-170602&quot;&gt;2020年07月20日 17:06:02&lt;/h3&gt;
&lt;p&gt;难得连续几天的时间这里都没有下雨，一直都是晴天的状态。&lt;br /&gt;
虽然天气很热，但是没有雨真的是太好了啊。&lt;/p&gt;

&lt;p&gt;今天英语课上老师给大家玩了类似剧情杀的内容。&lt;br /&gt;
是用英文写的，推理的时候还是很困难的，主要是不能很好的理解。&lt;br /&gt;
不过内容还是很有意思的，这学期选的这门课还是蛮轻松的。&lt;/p&gt;

&lt;p&gt;坐在研究室里吹着空调实在是太舒服了，要是能再安静一点就好了啊。&lt;/p&gt;

&lt;h3 id=&quot;2020年07月21日-145421&quot;&gt;2020年07月21日 14:54:21&lt;/h3&gt;
&lt;p&gt;今天又是一个大晴天，不过早上八点左右的时候忽然下了一会儿大暴雨，倾盆大雨一样的。&lt;br /&gt;
金泽的天气真的让人捉摸不定，天气还及其的闷热，&lt;br /&gt;
这又是下雨的前兆吧。&lt;/p&gt;

&lt;h3 id=&quot;2020年07月24日-105233&quot;&gt;2020年07月24日 10:52:33&lt;/h3&gt;
&lt;p&gt;刚刚放晴几天的天气，就在这放假的两天时间里突然就开始下起了暴雨。&lt;br /&gt;
满心欢喜的以为雨季已经过去了，看来还是我想多了。&lt;/p&gt;

&lt;p&gt;2070s跑起来深度学习速度还是很快的，比笔记本的速度要快了不少。&lt;br /&gt;
但是wsl还是不能调用gpu训练比较麻烦，只能放到win下面跑了。&lt;br /&gt;
光用cpu跑的话太浪费gpu的性能了。&lt;/p&gt;

&lt;h3 id=&quot;2020年07月26日-184122&quot;&gt;2020年07月26日 18:41:22&lt;/h3&gt;
&lt;p&gt;下午一点多的时候忽然下雨，坐在窗边的我吓了一跳。&lt;br /&gt;
真的是忽然，来的也快去的也快。&lt;/p&gt;

&lt;p&gt;今天写了knn的算法。sklearn这个软件库真的很好用，一些典型的机器学习算法都已经封包了，直接调用api就好了。&lt;br /&gt;
数据处理方面，pandas真是一个利器，里面的很多参数也特别详细。&lt;/p&gt;

&lt;p&gt;现在了解的机器学习知识也是越来越多了，感觉已经渐渐的有入门的趋势了。&lt;br /&gt;
加油&lt;/p&gt;

&lt;h3 id=&quot;2020年07月27日-154318&quot;&gt;2020年07月27日 15:43:18&lt;/h3&gt;
&lt;p&gt;今天开始处理cats-vs-dogs数据集。&lt;br /&gt;
没想到自己进行数据集划分的时候要这么麻烦，要将原本的数据分别划分为train, validation和test。&lt;br /&gt;
用os方法写的代码也及其复杂，难道就没有更简单一点的方法了吗。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="每日一笔" /><summary type="html">2020年07月03日 16:12:31 许就没有来研究室了。 今天来的人还算很多，没有见到教授。 免费的空调太舒服了啊，哈哈哈。 2020年07月07日 16:31:11 今天上了三个小时的英语课，听着日本人说着日式英语觉得还挺好玩的这节课。 虽然上课的时间有点长就是了。 然后接着来研究室蹭空调吧，不过大家都没来貌似。 一个人坐在这里也挺安心的。 2020年07月09日 18:14:18 要学的东西太多了，感觉压力很大。 这学期的报告也太多了，有些都是没什么意义的报告，但为了学分也要写。 2020年07月10日 15:46:12 今天去把驾照翻译了，马上要开始换驾照啦。 还久违的去逛了香林坊，真的疫情开始之后就再也没有去过了啊。 下了一天的下雨，真的是梅雨季节啊。 2020年07月13日 18:07:34 金泽的小雨还是一直下，梅雨天气一直持续着。 最近看电视剧太多了，有点太疯狂了。 现在人总是不能接受延迟满足，总是寻求一时的刺激获得满足。 真是变得越来越没有耐心了。 2020年07月16日 14:48:17 终于不下雨了，不过今天也实在是太热了吧！ 昨天买到了半价羊肉，还买了半价牛肉，真是太难得了啊。 不过今天的牛肉套料放多了，现在嘴里还有一股调料味。🤦‍ 不下雨太好了，今天终于能跑步了。 加油看论文吧。 2020年07月20日 17:06:02 难得连续几天的时间这里都没有下雨，一直都是晴天的状态。 虽然天气很热，但是没有雨真的是太好了啊。 今天英语课上老师给大家玩了类似剧情杀的内容。 是用英文写的，推理的时候还是很困难的，主要是不能很好的理解。 不过内容还是很有意思的，这学期选的这门课还是蛮轻松的。 坐在研究室里吹着空调实在是太舒服了，要是能再安静一点就好了啊。 2020年07月21日 14:54:21 今天又是一个大晴天，不过早上八点左右的时候忽然下了一会儿大暴雨，倾盆大雨一样的。 金泽的天气真的让人捉摸不定，天气还及其的闷热， 这又是下雨的前兆吧。 2020年07月24日 10:52:33 刚刚放晴几天的天气，就在这放假的两天时间里突然就开始下起了暴雨。 满心欢喜的以为雨季已经过去了，看来还是我想多了。 2070s跑起来深度学习速度还是很快的，比笔记本的速度要快了不少。 但是wsl还是不能调用gpu训练比较麻烦，只能放到win下面跑了。 光用cpu跑的话太浪费gpu的性能了。 2020年07月26日 18:41:22 下午一点多的时候忽然下雨，坐在窗边的我吓了一跳。 真的是忽然，来的也快去的也快。 今天写了knn的算法。sklearn这个软件库真的很好用，一些典型的机器学习算法都已经封包了，直接调用api就好了。 数据处理方面，pandas真是一个利器，里面的很多参数也特别详细。 现在了解的机器学习知识也是越来越多了，感觉已经渐渐的有入门的趋势了。 加油 2020年07月27日 15:43:18 今天开始处理cats-vs-dogs数据集。 没想到自己进行数据集划分的时候要这么麻烦，要将原本的数据分别划分为train, validation和test。 用os方法写的代码也及其复杂，难道就没有更简单一点的方法了吗。</summary></entry><entry><title type="html">晃晃悠悠</title><link href="http://localhost:4000/2020/07/06/%E6%99%83%E6%99%83%E6%82%A0%E6%82%A0.html" rel="alternate" type="text/html" title="晃晃悠悠" /><published>2020-07-06T00:00:00+09:00</published><updated>2020-07-06T00:00:00+09:00</updated><id>http://localhost:4000/2020/07/06/%E6%99%83%E6%99%83%E6%82%A0%E6%82%A0</id><content type="html" xml:base="http://localhost:4000/2020/07/06/%E6%99%83%E6%99%83%E6%82%A0%E6%82%A0.html">&lt;h2 id=&quot;2020年07月06日-164840&quot;&gt;2020年07月06日 16:48:40&lt;/h2&gt;
&lt;p&gt;三天以后，考试开始了，那段日子怎么过的，想想手心就出汗，有趣的是华杨在考第一门普通物理前做了一个怪梦特有意思，他梦见他站在考场外面看着同学们一个个进去，心情非常不好，于是蹲下拉了一泡屎，监考老师催他进考场，他不去，蹲在那儿玩屎，老师说，进去呀，他说，等会儿，让我再玩会儿。&lt;/p&gt;

&lt;p&gt;11月中旬，我忽然开始疯狂地复习功课，因为快考试了，我如果还想把大学混下去的话，每门功课就得考到75分以上。叫我奇怪的是，我做到了，当然，除了对儿虾的那门，考完试后，我们班有两个同学被开除了。我听说了他们的名字，可不记得他们的模样了，这是两个不声不响的同学，听说学的很用功，他们默默地考进来，又默默地被开除出去，真惨。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="读后感" /><summary type="html">2020年07月06日 16:48:40 三天以后，考试开始了，那段日子怎么过的，想想手心就出汗，有趣的是华杨在考第一门普通物理前做了一个怪梦特有意思，他梦见他站在考场外面看着同学们一个个进去，心情非常不好，于是蹲下拉了一泡屎，监考老师催他进考场，他不去，蹲在那儿玩屎，老师说，进去呀，他说，等会儿，让我再玩会儿。 11月中旬，我忽然开始疯狂地复习功课，因为快考试了，我如果还想把大学混下去的话，每门功课就得考到75分以上。叫我奇怪的是，我做到了，当然，除了对儿虾的那门，考完试后，我们班有两个同学被开除了。我听说了他们的名字，可不记得他们的模样了，这是两个不声不响的同学，听说学的很用功，他们默默地考进来，又默默地被开除出去，真惨。</summary></entry><entry xml:lang="zh"><title type="html">每日一笔</title><link href="http://localhost:4000/2020/06/20/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94.html" rel="alternate" type="text/html" title="每日一笔" /><published>2020-06-20T00:00:00+09:00</published><updated>2020-06-20T00:00:00+09:00</updated><id>http://localhost:4000/2020/06/20/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94</id><content type="html" xml:base="http://localhost:4000/2020/06/20/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94.html">&lt;h2 id=&quot;每日一笔记录生活&quot;&gt;每日一笔，记录生活&lt;/h2&gt;
&lt;h3 id=&quot;2020年06月20日-114155&quot;&gt;2020年06月20日 11:41:55&lt;/h3&gt;
&lt;p&gt;跳过5月的时间，生活直接从6月开始。&lt;/p&gt;

&lt;p&gt;这几天过的和做梦一样，短短的时间之内连续两天彻夜未眠。&lt;br /&gt;
仿佛对我来说，有这样的时候就说明有什么事情发生了。&lt;/p&gt;

&lt;p&gt;这次我确实有所成长，但还是不够成熟。&lt;br /&gt;
但至少要好很多，因为我的内心变得足够强大了。&lt;br /&gt;
也许是直接变得没心没肺了吧。&lt;/p&gt;

&lt;p&gt;不过我能感受到我的内心变得足够强大了，而且也能感受到现在的顾虑比以前多了很多。&lt;br /&gt;
这样究竟算是好的改变，还是坏的转变呢？&lt;/p&gt;

&lt;p&gt;不知道从什么时候开始变得婆婆妈妈的了，再也没有从前的潇洒从容自信。&lt;br /&gt;
真的变了很多呀，你。&lt;/p&gt;

&lt;p&gt;毕竟每次受伤的总是我自己，吃亏的也是我。&lt;/p&gt;

&lt;p&gt;害，好好学习吧，每天都想这些有的没的。&lt;/p&gt;

&lt;h3 id=&quot;2020年06月23日-172332&quot;&gt;2020年06月23日 17:23:32&lt;/h3&gt;
&lt;p&gt;最近开始在研究《红楼梦》。&lt;/p&gt;

&lt;p&gt;我这大概是疯了吧，也许是闲的吧。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="每日一笔" /><summary type="html">每日一笔，记录生活 2020年06月20日 11:41:55 跳过5月的时间，生活直接从6月开始。 这几天过的和做梦一样，短短的时间之内连续两天彻夜未眠。 仿佛对我来说，有这样的时候就说明有什么事情发生了。 这次我确实有所成长，但还是不够成熟。 但至少要好很多，因为我的内心变得足够强大了。 也许是直接变得没心没肺了吧。 不过我能感受到我的内心变得足够强大了，而且也能感受到现在的顾虑比以前多了很多。 这样究竟算是好的改变，还是坏的转变呢？ 不知道从什么时候开始变得婆婆妈妈的了，再也没有从前的潇洒从容自信。 真的变了很多呀，你。 毕竟每次受伤的总是我自己，吃亏的也是我。 害，好好学习吧，每天都想这些有的没的。 2020年06月23日 17:23:32 最近开始在研究《红楼梦》。 我这大概是疯了吧，也许是闲的吧。</summary></entry><entry><title type="html">周志华《机器学习》</title><link href="http://localhost:4000/2020/05/21/%E7%BB%AA%E8%AE%BA.html" rel="alternate" type="text/html" title="周志华《机器学习》" /><published>2020-05-21T00:00:00+09:00</published><updated>2020-05-21T00:00:00+09:00</updated><id>http://localhost:4000/2020/05/21/%E7%BB%AA%E8%AE%BA</id><content type="html" xml:base="http://localhost:4000/2020/05/21/%E7%BB%AA%E8%AE%BA.html">&lt;h1 id=&quot;绪论&quot;&gt;绪论&lt;/h1&gt;
&lt;h2 id=&quot;基本术语&quot;&gt;基本术语&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;数据集&lt;/li&gt;
  &lt;li&gt;示例&lt;/li&gt;
  &lt;li&gt;属性/特征&lt;/li&gt;
  &lt;li&gt;属性值&lt;/li&gt;
  &lt;li&gt;属性空间/样本空间/输入空间&lt;/li&gt;
  &lt;li&gt;特征向量&lt;/li&gt;
  &lt;li&gt;维数：有几个属性就是几维&lt;/li&gt;
  &lt;li&gt;学习/训练：从数据中学的模型的过程，通过执行某个算法完成。&lt;/li&gt;
  &lt;li&gt;训练数据
    &lt;ul&gt;
      &lt;li&gt;训练样本&lt;/li&gt;
      &lt;li&gt;训练样本&lt;/li&gt;
      &lt;li&gt;训练集&lt;/li&gt;
      &lt;li&gt;假设：学得模型对应了关于数据的某些前在的规律&lt;/li&gt;
      &lt;li&gt;真相/真实：潜在规律自身&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;预测&lt;/li&gt;
  &lt;li&gt;标记&lt;/li&gt;
  &lt;li&gt;样例：有标记信息的是示例&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;标记空间/输出空间&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;分类
    &lt;ul&gt;
      &lt;li&gt;二分类任务
        &lt;ul&gt;
          &lt;li&gt;正类&lt;/li&gt;
          &lt;li&gt;反类&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;多分类任务&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;回归&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;测试&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;测试样本&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;聚类&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;簇&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;监督学习：分类，回归&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;无监督学习：聚类&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;泛化&lt;/li&gt;
  &lt;li&gt;分布
    &lt;ul&gt;
      &lt;li&gt;独立分布&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;假设空间&quot;&gt;假设空间&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;归纳&lt;/li&gt;
  &lt;li&gt;演绎&lt;/li&gt;
  &lt;li&gt;概念&lt;/li&gt;
  &lt;li&gt;版本空间&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;归纳偏好&quot;&gt;归纳偏好&lt;/h2&gt;
&lt;p&gt;机器学习算法在学习过程中对某种类型假设的偏好&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;奥卡姆剃刀：若有多个假设与观察一致，则选择最简单的那个&lt;/li&gt;
  &lt;li&gt;没有免费午餐定理：没有一种机器学习算法是适用于所有情况的&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;发展历程&quot;&gt;发展历程&lt;/h2&gt;
&lt;h2 id=&quot;应用现状&quot;&gt;应用现状&lt;/h2&gt;
&lt;h2 id=&quot;阅读材料&quot;&gt;阅读材料&lt;/h2&gt;

&lt;h1 id=&quot;模型评估与选择&quot;&gt;模型评估与选择&lt;/h1&gt;
&lt;h2 id=&quot;经验误差与过拟合&quot;&gt;经验误差与过拟合&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;错误率&lt;/li&gt;
  &lt;li&gt;精度&lt;/li&gt;
  &lt;li&gt;误差
    &lt;ul&gt;
      &lt;li&gt;训练误差/经验误差&lt;/li&gt;
      &lt;li&gt;泛化误差&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;过拟合&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;欠拟合&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;模型选择问题&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;评估方法&quot;&gt;评估方法&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;测试集&lt;/li&gt;
  &lt;li&gt;测试误差&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;留出法&quot;&gt;留出法&lt;/h3&gt;
&lt;p&gt;分层采样&lt;/p&gt;

&lt;h3 id=&quot;交叉验证法k折交叉验证&quot;&gt;交叉验证法(k折交叉验证)&lt;/h3&gt;
&lt;p&gt;留一法&lt;/p&gt;

&lt;h3 id=&quot;自助法&quot;&gt;自助法&lt;/h3&gt;
&lt;p&gt;包外估计&lt;/p&gt;

&lt;h3 id=&quot;调参与最终模型&quot;&gt;调参与最终模型&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;调参&lt;/li&gt;
  &lt;li&gt;验证集&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;性能度量&quot;&gt;性能度量&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;均方误差&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;错误率与精度&quot;&gt;错误率与精度&lt;/h3&gt;
&lt;h3 id=&quot;查准率查全率与f1&quot;&gt;查准率，查全率与F1&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;查准率&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;查全率&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;真正例&lt;/li&gt;
  &lt;li&gt;真反例&lt;/li&gt;
  &lt;li&gt;假正例&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;假反例&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;混淆矩阵&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;比较学习器性能&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;平衡点（BEP）&lt;/li&gt;
  &lt;li&gt;F1度量&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;有多个二分类混淆矩阵&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;宏查准率&lt;/li&gt;
  &lt;li&gt;宏查全率&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;宏F1&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;微查准率&lt;/li&gt;
  &lt;li&gt;微查全率&lt;/li&gt;
  &lt;li&gt;微F1&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;roc与auc&quot;&gt;ROC与AUC&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;分类阈值&lt;/li&gt;
  &lt;li&gt;ROC：受试者工作特征曲线
    &lt;ul&gt;
      &lt;li&gt;真正例率&lt;/li&gt;
      &lt;li&gt;假正例率&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;AUC：ROC曲线下的面积，比较不同学习器ROC性能&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;代价敏感错误率与代价曲线&quot;&gt;代价敏感错误率与代价曲线&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;非均等代价：衡量不同类型错误所造成的不同损失&lt;/li&gt;
  &lt;li&gt;代价矩阵&lt;/li&gt;
  &lt;li&gt;代价敏感错误率&lt;/li&gt;
  &lt;li&gt;代价曲线&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;比较校验&quot;&gt;比较校验&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;统计假设检验&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;假设检验&quot;&gt;假设检验&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;二项检验
    &lt;ul&gt;
      &lt;li&gt;置信度&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;t检验&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;交叉验证t检验&quot;&gt;交叉验证t检验&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;成对t检验&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;mcnemar检验&quot;&gt;McNemar检验&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;列联表&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;friedman验证与nemenyi后续检验&quot;&gt;Friedman验证与Nemenyi后续检验&lt;/h3&gt;

&lt;h2 id=&quot;偏差与方差&quot;&gt;偏差与方差&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;偏差-方差分解&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;偏差：度量了学习算法的期望预测与真实结果的偏离程度，既刻画了学习算法本身的拟合程度&lt;/li&gt;
  &lt;li&gt;方差：度量了同样大小的训练集的变动所导致的学习性能的变化，既刻画了数据扰动所造成的影响&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;噪声：表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，既刻画了学习问题本身的难度&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;偏差-方差窘境&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;线性模型&quot;&gt;线性模型&lt;/h1&gt;
&lt;h2 id=&quot;基本形式&quot;&gt;基本形式&lt;/h2&gt;

&lt;h2 id=&quot;线性回归&quot;&gt;线性回归&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;欧氏距离&lt;/li&gt;
  &lt;li&gt;最小二乘法
    &lt;ul&gt;
      &lt;li&gt;参数估计&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;正则化项&lt;/li&gt;
  &lt;li&gt;对数线性回归&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;对数几率回归&quot;&gt;对数几率回归&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;替代函数
    &lt;ul&gt;
      &lt;li&gt;对数几率函数&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;几率&lt;/li&gt;
  &lt;li&gt;对数几率&lt;/li&gt;
  &lt;li&gt;极大似然法&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;线性判别分析&quot;&gt;线性判别分析&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;类内散度矩阵&lt;/li&gt;
  &lt;li&gt;类间散度矩阵&lt;/li&gt;
  &lt;li&gt;广义瑞利商&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;多分类学习&quot;&gt;多分类学习&lt;/h2&gt;
&lt;p&gt;拆分策略：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;OvO&lt;/li&gt;
  &lt;li&gt;OvR&lt;/li&gt;
  &lt;li&gt;MvM
    &lt;ul&gt;
      &lt;li&gt;纠错输出编码&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;类别不平衡问题&quot;&gt;类别不平衡问题&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;再缩放
    &lt;ul&gt;
      &lt;li&gt;欠采样&lt;/li&gt;
      &lt;li&gt;过采样&lt;/li&gt;
      &lt;li&gt;阈值移动&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;阅读材料-1&quot;&gt;阅读材料&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;稀疏表示&lt;/li&gt;
  &lt;li&gt;多标记学习&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;决策树&quot;&gt;决策树&lt;/h1&gt;
&lt;h2 id=&quot;基本流程&quot;&gt;基本流程&lt;/h2&gt;

&lt;h2 id=&quot;划分选择&quot;&gt;划分选择&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;纯度&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;信息增商&quot;&gt;信息增商&lt;/h3&gt;

&lt;h3 id=&quot;增益率&quot;&gt;增益率&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;固有值&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;基尼指数&quot;&gt;基尼指数&lt;/h3&gt;

&lt;h2 id=&quot;剪枝处理&quot;&gt;剪枝处理&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;预剪枝&lt;/li&gt;
  &lt;li&gt;后剪枝&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;预剪枝&quot;&gt;预剪枝&lt;/h3&gt;

&lt;h3 id=&quot;后剪枝&quot;&gt;后剪枝&lt;/h3&gt;

&lt;h2 id=&quot;连续与缺失值&quot;&gt;连续与缺失值&lt;/h2&gt;
&lt;h3 id=&quot;连续值处理&quot;&gt;连续值处理&lt;/h3&gt;

&lt;h3 id=&quot;缺失值处理&quot;&gt;缺失值处理&lt;/h3&gt;

&lt;h2 id=&quot;多变量决策树&quot;&gt;多变量决策树&lt;/h2&gt;

&lt;h1 id=&quot;神经网络&quot;&gt;神经网络&lt;/h1&gt;
&lt;h2 id=&quot;神经元模型&quot;&gt;神经元模型&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;神经元&lt;/li&gt;
  &lt;li&gt;激活函数
    &lt;ul&gt;
      &lt;li&gt;sigmoid&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;感知机与多层网络&quot;&gt;感知机与多层网络&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;哑节点&lt;/li&gt;
  &lt;li&gt;学习率&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;隐含层&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;多层前馈神经网络&lt;/li&gt;
  &lt;li&gt;双隐层前馈网络&lt;/li&gt;
  &lt;li&gt;连接权&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;误差逆传播算法&quot;&gt;误差逆传播算法&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;试错法
    &lt;ul&gt;
      &lt;li&gt;早停&lt;/li&gt;
      &lt;li&gt;正则化&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;全局最小与局部极小&quot;&gt;全局最小与局部极小&lt;/h2&gt;

&lt;h2 id=&quot;其他常见神经网络&quot;&gt;其他常见神经网络&lt;/h2&gt;
&lt;h3 id=&quot;rbf网络&quot;&gt;RBF网络&lt;/h3&gt;
&lt;h3 id=&quot;art网络&quot;&gt;ART网络&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;竞争型学习&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;som网络&quot;&gt;SOM网络&lt;/h3&gt;
&lt;h3 id=&quot;级联相关网络&quot;&gt;级联相关网络&lt;/h3&gt;
&lt;h3 id=&quot;elman网络&quot;&gt;Elman网络&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;递归神经网络&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;boltzmann机&quot;&gt;Boltzmann机&lt;/h3&gt;

&lt;h2 id=&quot;深度学习&quot;&gt;深度学习&lt;/h2&gt;

&lt;h1 id=&quot;支持向量机&quot;&gt;支持向量机&lt;/h1&gt;
&lt;h2 id=&quot;间隔与支持向量机&quot;&gt;间隔与支持向量机&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;支持向量&lt;/li&gt;
  &lt;li&gt;间隔&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;对偶问题&quot;&gt;对偶问题&lt;/h2&gt;

&lt;h2 id=&quot;核函数&quot;&gt;核函数&lt;/h2&gt;

&lt;h2 id=&quot;软间隔与正则化&quot;&gt;软间隔与正则化&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;软间隔：某些样本不满足约束&lt;/li&gt;
  &lt;li&gt;替代损失函数&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;支持向量回归&quot;&gt;支持向量回归&lt;/h2&gt;

&lt;h2 id=&quot;核方法&quot;&gt;核方法&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;核线性判别分析&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;贝叶斯分类器&quot;&gt;贝叶斯分类器&lt;/h1&gt;
&lt;h2 id=&quot;贝叶斯决策论&quot;&gt;贝叶斯决策论&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;贝叶斯最优分类器&lt;/li&gt;
  &lt;li&gt;贝叶斯风险&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;极大似然估计&quot;&gt;极大似然估计&lt;/h2&gt;
&lt;h2 id=&quot;朴素贝叶斯分类器&quot;&gt;朴素贝叶斯分类器&lt;/h2&gt;

&lt;h2 id=&quot;半朴素贝叶斯分类器&quot;&gt;半朴素贝叶斯分类器&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;独依赖估计&lt;/li&gt;
  &lt;li&gt;SPODE方法&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;贝叶斯网&quot;&gt;贝叶斯网&lt;/h2&gt;
&lt;h3 id=&quot;结构&quot;&gt;结构&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;道德图
    &lt;h3 id=&quot;学习&quot;&gt;学习&lt;/h3&gt;
  &lt;/li&gt;
  &lt;li&gt;评分搜索&lt;/li&gt;
  &lt;li&gt;各种评分函数&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;推断&quot;&gt;推断&lt;/h3&gt;

&lt;h2 id=&quot;em算法&quot;&gt;EM算法&lt;/h2&gt;

&lt;h1 id=&quot;集成学习&quot;&gt;集成学习&lt;/h1&gt;
&lt;h2 id=&quot;个体与集成&quot;&gt;个体与集成&lt;/h2&gt;

&lt;h2 id=&quot;boosting&quot;&gt;Boosting&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;AdaBoost算法&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bagging与随机森林&quot;&gt;Bagging与随机森林&lt;/h2&gt;
&lt;h3 id=&quot;bagging&quot;&gt;Bagging&lt;/h3&gt;

&lt;h3 id=&quot;随机森林&quot;&gt;随机森林&lt;/h3&gt;
&lt;p&gt;bagging的一个扩展变体。&lt;/p&gt;

&lt;h2 id=&quot;结合策略&quot;&gt;结合策略&lt;/h2&gt;
&lt;h3 id=&quot;平均法&quot;&gt;平均法&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;简单平均法&lt;/li&gt;
  &lt;li&gt;加全平均法&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;投票法&quot;&gt;投票法&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;绝对多数投票法&lt;/li&gt;
  &lt;li&gt;相对多数投票法&lt;/li&gt;
  &lt;li&gt;加权投票法&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;学习法&quot;&gt;学习法&lt;/h3&gt;

&lt;h2 id=&quot;多样性&quot;&gt;多样性&lt;/h2&gt;
&lt;h3 id=&quot;误差-分歧分解&quot;&gt;误差-分歧分解&lt;/h3&gt;
&lt;p&gt;个体学习器准确性越高，多样性越大，则集成越好。&lt;/p&gt;

&lt;h3 id=&quot;多样性度量&quot;&gt;多样性度量&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;数据样本扰动&lt;/li&gt;
  &lt;li&gt;输入属性扰动&lt;/li&gt;
  &lt;li&gt;输出表示扰动&lt;/li&gt;
  &lt;li&gt;算法参数扰动&lt;/li&gt;
&lt;/ul&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><category term="机器学习" /><summary type="html">绪论 基本术语 数据集 示例 属性/特征 属性值 属性空间/样本空间/输入空间 特征向量 维数：有几个属性就是几维 学习/训练：从数据中学的模型的过程，通过执行某个算法完成。 训练数据 训练样本 训练样本 训练集 假设：学得模型对应了关于数据的某些前在的规律 真相/真实：潜在规律自身 预测 标记 样例：有标记信息的是示例 标记空间/输出空间 分类 二分类任务 正类 反类 多分类任务 回归 测试 测试样本 聚类 簇 监督学习：分类，回归 无监督学习：聚类 泛化 分布 独立分布 假设空间 归纳 演绎 概念 版本空间 归纳偏好 机器学习算法在学习过程中对某种类型假设的偏好 奥卡姆剃刀：若有多个假设与观察一致，则选择最简单的那个 没有免费午餐定理：没有一种机器学习算法是适用于所有情况的 发展历程 应用现状 阅读材料 模型评估与选择 经验误差与过拟合 错误率 精度 误差 训练误差/经验误差 泛化误差 过拟合 欠拟合 模型选择问题 评估方法 测试集 测试误差 留出法 分层采样 交叉验证法(k折交叉验证) 留一法 自助法 包外估计 调参与最终模型 调参 验证集 性能度量 均方误差 错误率与精度 查准率，查全率与F1 查准率 查全率 真正例 真反例 假正例 假反例 混淆矩阵 比较学习器性能 平衡点（BEP） F1度量 有多个二分类混淆矩阵 宏查准率 宏查全率 宏F1 微查准率 微查全率 微F1 ROC与AUC 分类阈值 ROC：受试者工作特征曲线 真正例率 假正例率 AUC：ROC曲线下的面积，比较不同学习器ROC性能 代价敏感错误率与代价曲线 非均等代价：衡量不同类型错误所造成的不同损失 代价矩阵 代价敏感错误率 代价曲线 比较校验 统计假设检验 假设检验 二项检验 置信度 t检验 交叉验证t检验 成对t检验 McNemar检验 列联表 Friedman验证与Nemenyi后续检验 偏差与方差 偏差-方差分解 偏差：度量了学习算法的期望预测与真实结果的偏离程度，既刻画了学习算法本身的拟合程度 方差：度量了同样大小的训练集的变动所导致的学习性能的变化，既刻画了数据扰动所造成的影响 噪声：表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，既刻画了学习问题本身的难度 偏差-方差窘境 线性模型 基本形式 线性回归 欧氏距离 最小二乘法 参数估计 正则化项 对数线性回归 对数几率回归 替代函数 对数几率函数 几率 对数几率 极大似然法 线性判别分析 类内散度矩阵 类间散度矩阵 广义瑞利商 多分类学习 拆分策略： OvO OvR MvM 纠错输出编码 类别不平衡问题 再缩放 欠采样 过采样 阈值移动 阅读材料 稀疏表示 多标记学习 决策树 基本流程 划分选择 纯度 信息增商 增益率 固有值 基尼指数 剪枝处理 预剪枝 后剪枝 预剪枝 后剪枝 连续与缺失值 连续值处理 缺失值处理 多变量决策树 神经网络 神经元模型 神经元 激活函数 sigmoid 感知机与多层网络 哑节点 学习率 隐含层 多层前馈神经网络 双隐层前馈网络 连接权 误差逆传播算法 试错法 早停 正则化 全局最小与局部极小 其他常见神经网络 RBF网络 ART网络 竞争型学习 SOM网络 级联相关网络 Elman网络 递归神经网络 Boltzmann机 深度学习 支持向量机 间隔与支持向量机 支持向量 间隔 对偶问题 核函数 软间隔与正则化 软间隔：某些样本不满足约束 替代损失函数 支持向量回归 核方法 核线性判别分析 贝叶斯分类器 贝叶斯决策论 贝叶斯最优分类器 贝叶斯风险 极大似然估计 朴素贝叶斯分类器 半朴素贝叶斯分类器 独依赖估计 SPODE方法 贝叶斯网 结构 道德图 学习 评分搜索 各种评分函数 推断 EM算法 集成学习 个体与集成 Boosting AdaBoost算法 Bagging与随机森林 Bagging 随机森林 bagging的一个扩展变体。 结合策略 平均法 简单平均法 加全平均法 投票法 绝对多数投票法 相对多数投票法 加权投票法 学习法 多样性 误差-分歧分解 个体学习器准确性越高，多样性越大，则集成越好。 多样性度量 数据样本扰动 输入属性扰动 输出表示扰动 算法参数扰动</summary></entry><entry><title type="html">卷积神经网络笔记</title><link href="http://localhost:4000/2020/05/20/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B0.html" rel="alternate" type="text/html" title="卷积神经网络笔记" /><published>2020-05-20T00:00:00+09:00</published><updated>2020-05-20T00:00:00+09:00</updated><id>http://localhost:4000/2020/05/20/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B0</id><content type="html" xml:base="http://localhost:4000/2020/05/20/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B0.html">&lt;h1 id=&quot;卷积层&quot;&gt;卷积层&lt;/h1&gt;
&lt;h2 id=&quot;概述和直观介绍&quot;&gt;概述和直观介绍&lt;/h2&gt;
&lt;p&gt;卷积层的参数是有一些可学习的滤波器集合构成的。每个滤波器在空间上（宽度和高度）都比较小，但是深度和输入数据一致。举例来说，卷积神经网络第一层的一个典型的滤波器的尺寸可以是5x5x3（宽高都是5像素，深度是3是因为图像应为颜色通道，所以有3的深度）。在前向传播的时候，让每个滤波器都在输入数据的宽度和高度上滑动（更精确地说是卷积），然后计算整个滤波器和输入数据任一处的内积。当滤波器沿着输入数据的宽度和高度滑过后，会生成一个2维的激活图（activation map），激活图给出了在每个空间位置处滤波器的反应。&lt;/p&gt;

&lt;h2 id=&quot;局部连接&quot;&gt;局部连接&lt;/h2&gt;
&lt;p&gt;在处理图像这样的高维度输入时，让每个神经元都与前一层中的所有神经元进行全连接是不现实的。相反，我们让每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的感受野（receptive field），它的尺寸是一个超参数（其实就是滤波器的空间尺寸）。在深度方向上，这个连接的大小总是和输入量的深度相等。需要再次强调的是，我们对待空间维度（宽和高）与深度维度是不同的：连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;例1：假设输入数据体尺寸为[32x32x3]（比如CIFAR-10的RGB图像），如果感受野（或滤波器尺寸）是5x5，那么卷积层中的每个神经元会有输入数据体中[5x5x3]区域的权重，共5x5x3=75个权重（还要加一个偏差参数）。注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;空间排列&quot;&gt;空间排列&lt;/h2&gt;
&lt;p&gt;上文讲解了卷积层中每个神经元与输入数据体之间的连接方式，但是尚未讨论输出数据体中神经元的数量，以及它们的排列方式。3个超参数控制着输出数据体的尺寸：深度（depth），步长（stride）和零填充（zero-padding）。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;输出数据体的深度是一个超参数：它和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西。举例来说，如果第一个卷积层的输入是原始图像，那么在深度维度上的不同神经元将可能被不同方向的边界，或者是颜色斑点激活。我们将这些沿着深度方向排列、感受野相同的神经元集合称为深度列（depth column），也有人使用纤维（fibre）来称呼它们。&lt;/li&gt;
  &lt;li&gt;在滑动滤波器的时候，必须指定步长。当步长为1，滤波器每次移动1个像素。当步长为2（或者不常用的3，或者更多，这些在实际中很少使用），滤波器滑动时每次移动2个像素。这个操作会让输出数据体在空间上变小。&lt;/li&gt;
  &lt;li&gt;这个零填充（zero-padding）的尺寸是一个超参数。零填充有一个良好性质，即可以控制输出数据体的空间尺寸（最常用的是用来保持输入数据体在空间上的尺寸，这样输入和输出的宽高都相等）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;参数共享&quot;&gt;参数共享&lt;/h2&gt;
&lt;p&gt;在卷积层中使用参数共享是用来控制参数的数量。&lt;/p&gt;

&lt;h2 id=&quot;小结&quot;&gt;小结&lt;/h2&gt;
&lt;p&gt;我们总结一下卷积层的性质：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;输入数据体的尺寸为 W1xH1xD1&lt;/li&gt;
  &lt;li&gt;4个超参数：
    &lt;ul&gt;
      &lt;li&gt;滤波器的数量K&lt;/li&gt;
      &lt;li&gt;滤波器的空间尺寸F&lt;/li&gt;
      &lt;li&gt;步长S&lt;/li&gt;
      &lt;li&gt;零填充数量P&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;输出数据体的尺寸为 W2xH2xD2&lt;/li&gt;
  &lt;li&gt;在输出数据体中，第d个深度切片，用第d个滤波器和输入数据进行有效卷积运算的结果（使用步长s），最后在加上第d个偏差。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;用矩阵乘法实现&quot;&gt;用矩阵乘法实现&lt;/h2&gt;
&lt;p&gt;卷积运算本质上就是在滤波器和输入数据的局部区域间做点积。卷积层的常用实现方式就是利用这一点，将卷积层的前向传播变成一个巨大的矩阵乘法。&lt;/p&gt;

&lt;h1 id=&quot;汇聚层&quot;&gt;汇聚层&lt;/h1&gt;
&lt;p&gt;通常，在连续的卷积层之间会周期性地插入一个汇聚层。它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。汇聚层使用MAX操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。&lt;/p&gt;

&lt;h2 id=&quot;不使用汇聚层&quot;&gt;不使用汇聚层&lt;/h2&gt;
&lt;p&gt;很多人不喜欢汇聚操作，认为可以不使用它。比如在Striving for Simplicity: The All Convolutional Net一文中，提出使用一种只有重复的卷积层组成的结构，抛弃汇聚层。通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为，在训练一个良好的生成模型时，弃用汇聚层也是很重要的。比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）。现在看起来，未来的卷积网络结构中，可能会很少使用甚至不使用汇聚层。&lt;/p&gt;

&lt;h1 id=&quot;全连接层&quot;&gt;全连接层&lt;/h1&gt;
&lt;p&gt;在全连接层中，神经元对于前一层中的所有激活数据是全部连接的，这个常规神经网络中一样。它们的激活可以先用矩阵乘法，再加上偏差。更多细节请查看神经网络章节。&lt;/p&gt;

&lt;h1 id=&quot;把全连接层转化成卷积层&quot;&gt;把全连接层转化成卷积层&lt;/h1&gt;
&lt;p&gt;全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在两类层中，神经元都是计算点积，所以它们的函数形式是一样的。因此，将此两者相互转化是可能的：&lt;/p&gt;

&lt;h2 id=&quot;全连接层转化为卷积层&quot;&gt;全连接层转化为卷积层&lt;/h2&gt;
&lt;p&gt;在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是224x224x3的图像，一系列的卷积层和汇聚层将图像数据变为尺寸为7x7x512的激活数据体（在AlexNet中就是这样，通过使用5个汇聚层来对输入数据进行空间上的降采样，每次尺寸下降一半，所以最终空间尺寸为224/2/2/2/2/2=7）。从这里可以看到，AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。&lt;/p&gt;

&lt;h1 id=&quot;卷积神经网络的结构&quot;&gt;卷积神经网络的结构&lt;/h1&gt;
&lt;p&gt;卷积神经网络通常是由三种层构成：卷积层，汇聚层（除非特别说明，一般就是最大值汇聚）和全连接层（简称FC）。ReLU激活函数也应该算是是一层，它逐元素地进行激活函数操作。在本节中将讨论在卷积神经网络中这些层通常是如何组合在一起的。&lt;/p&gt;

&lt;h2 id=&quot;层的排列规律&quot;&gt;层的排列规律&lt;/h2&gt;
&lt;p&gt;卷积神经网络最常见的形式就是将一些卷积层和ReLU层放在一起，其后紧跟汇聚层，然后重复如此直到图像在空间上被缩小到一个足够小的尺寸，在某个地方过渡成成全连接层也较为常见。最后的全连接层得到输出，比如分类评分等。换句话说，最常见的卷积神经网络结构如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;INPUT -&amp;gt; [[CONV -&amp;gt; RELU]*N -&amp;gt; POOL?]*M -&amp;gt; [FC -&amp;gt; RELU]*K -&amp;gt; FC
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;其中*指的是重复次数，POOL?指的是一个可选的汇聚层。其中N &amp;gt;=0,通常N&amp;lt;=3,M&amp;gt;=0,K&amp;gt;=0,通常K&amp;lt;3。&lt;/p&gt;

&lt;h2 id=&quot;层的尺寸设置规律&quot;&gt;层的尺寸设置规律&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;输入层（包含图像的）应该能被2整除很多次。常用数字包括32（比如CIFAR-10），64，96（比如STL-10）或224（比如ImageNet卷积神经网络），384和512。&lt;/li&gt;
  &lt;li&gt;卷积层应该使用小尺寸滤波器（比如3x3或最多5x5），使用步长1。还有一点非常重要，就是对输入数据进行零填充，这样卷积层就不会改变输入数据在空间维度上的尺寸。&lt;/li&gt;
  &lt;li&gt;汇聚层负责对输入数据的空间维度进行降采样。最常用的设置是用用2x2感受野的最大值汇聚，步长为2。注意这一操作将会把输入数据中75%的激活数据丢弃（因为对宽度和高度都进行了2的降采样）。另一个不那么常用的设置是使用3x3的感受野，步长为2。最大值汇聚的感受野尺寸很少有超过3的，因为汇聚操作过于激烈，易造成数据信息丢失，这通常会导致算法性能变差。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;qa&quot;&gt;Q&amp;amp;A&lt;/h1&gt;
&lt;h2 id=&quot;减少尺寸设置的问题&quot;&gt;减少尺寸设置的问题&lt;/h2&gt;
&lt;p&gt;上文中展示的两种设置是很好的，因为所有的卷积层都能保持其输入数据的空间尺寸，汇聚层只负责对数据体从空间维度进行降采样。如果使用的步长大于1并且不对卷积层的输入数据使用零填充，那么就必须非常仔细地监督输入数据体通过整个卷积神经网络结构的过程，确认所有的步长和滤波器都尺寸互相吻合，卷积神经网络的结构美妙对称地联系在一起。&lt;/p&gt;

&lt;h2 id=&quot;为什么在卷积层使用1的步长&quot;&gt;为什么在卷积层使用1的步长？&lt;/h2&gt;
&lt;p&gt;在实际应用中，更小的步长效果更好。上文也已经提过，步长为1可以让空间维度的降采样全部由汇聚层负责，卷积层只负责对输入数据体的深度进行变换。&lt;/p&gt;

&lt;h2 id=&quot;为何使用零填充&quot;&gt;为何使用零填充？&lt;/h2&gt;
&lt;p&gt;使用零填充除了前面提到的可以让卷积层的输出数据保持和输入数据在空间维度的不变，还可以提高算法性能。如果卷积层值进行卷积而不进行零填充，那么数据体的尺寸就会略微减小，那么图像边缘的信息就会过快地损失掉。&lt;/p&gt;

&lt;h2 id=&quot;因为内存限制所做的妥协&quot;&gt;因为内存限制所做的妥协&lt;/h2&gt;
&lt;p&gt;在某些案例（尤其是早期的卷积神经网络结构）中，基于前面的各种规则，内存的使用量迅速飙升。例如，使用64个尺寸为3x3的滤波器对224x224x3的图像进行卷积，零填充为1，得到的激活数据体尺寸是[224x224x64]。这个数量就是一千万的激活数据，或者就是72MB的内存（每张图就是这么多，激活函数和梯度都是）。因为GPU通常因为内存导致性能瓶颈，所以做出一些妥协是必须的。在实践中，人们倾向于在网络的第一个卷积层做出妥协。例如，可以妥协可能是在第一个卷积层使用步长为2，尺寸为7x7的滤波器（比如在ZFnet中）。在AlexNet中，滤波器的尺寸的11x11，步长为4。&lt;/p&gt;

&lt;h1 id=&quot;参考&quot;&gt;参考&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit&quot;&gt;(知乎) CS231n课程笔记翻译：卷积神经网络笔记&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://cs231n.github.io/convolutional-networks/&quot;&gt;(cs231n) Convolutional Neural Networks (CNNs / ConvNets)&lt;/a&gt;&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><category term="cs231n" /><summary type="html">卷积层 概述和直观介绍 卷积层的参数是有一些可学习的滤波器集合构成的。每个滤波器在空间上（宽度和高度）都比较小，但是深度和输入数据一致。举例来说，卷积神经网络第一层的一个典型的滤波器的尺寸可以是5x5x3（宽高都是5像素，深度是3是因为图像应为颜色通道，所以有3的深度）。在前向传播的时候，让每个滤波器都在输入数据的宽度和高度上滑动（更精确地说是卷积），然后计算整个滤波器和输入数据任一处的内积。当滤波器沿着输入数据的宽度和高度滑过后，会生成一个2维的激活图（activation map），激活图给出了在每个空间位置处滤波器的反应。 局部连接 在处理图像这样的高维度输入时，让每个神经元都与前一层中的所有神经元进行全连接是不现实的。相反，我们让每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的感受野（receptive field），它的尺寸是一个超参数（其实就是滤波器的空间尺寸）。在深度方向上，这个连接的大小总是和输入量的深度相等。需要再次强调的是，我们对待空间维度（宽和高）与深度维度是不同的：连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致。 例1：假设输入数据体尺寸为[32x32x3]（比如CIFAR-10的RGB图像），如果感受野（或滤波器尺寸）是5x5，那么卷积层中的每个神经元会有输入数据体中[5x5x3]区域的权重，共5x5x3=75个权重（还要加一个偏差参数）。注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。 空间排列 上文讲解了卷积层中每个神经元与输入数据体之间的连接方式，但是尚未讨论输出数据体中神经元的数量，以及它们的排列方式。3个超参数控制着输出数据体的尺寸：深度（depth），步长（stride）和零填充（zero-padding）。 输出数据体的深度是一个超参数：它和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西。举例来说，如果第一个卷积层的输入是原始图像，那么在深度维度上的不同神经元将可能被不同方向的边界，或者是颜色斑点激活。我们将这些沿着深度方向排列、感受野相同的神经元集合称为深度列（depth column），也有人使用纤维（fibre）来称呼它们。 在滑动滤波器的时候，必须指定步长。当步长为1，滤波器每次移动1个像素。当步长为2（或者不常用的3，或者更多，这些在实际中很少使用），滤波器滑动时每次移动2个像素。这个操作会让输出数据体在空间上变小。 这个零填充（zero-padding）的尺寸是一个超参数。零填充有一个良好性质，即可以控制输出数据体的空间尺寸（最常用的是用来保持输入数据体在空间上的尺寸，这样输入和输出的宽高都相等）。 参数共享 在卷积层中使用参数共享是用来控制参数的数量。 小结 我们总结一下卷积层的性质： 输入数据体的尺寸为 W1xH1xD1 4个超参数： 滤波器的数量K 滤波器的空间尺寸F 步长S 零填充数量P 输出数据体的尺寸为 W2xH2xD2 在输出数据体中，第d个深度切片，用第d个滤波器和输入数据进行有效卷积运算的结果（使用步长s），最后在加上第d个偏差。 用矩阵乘法实现 卷积运算本质上就是在滤波器和输入数据的局部区域间做点积。卷积层的常用实现方式就是利用这一点，将卷积层的前向传播变成一个巨大的矩阵乘法。 汇聚层 通常，在连续的卷积层之间会周期性地插入一个汇聚层。它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。汇聚层使用MAX操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。 不使用汇聚层 很多人不喜欢汇聚操作，认为可以不使用它。比如在Striving for Simplicity: The All Convolutional Net一文中，提出使用一种只有重复的卷积层组成的结构，抛弃汇聚层。通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为，在训练一个良好的生成模型时，弃用汇聚层也是很重要的。比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）。现在看起来，未来的卷积网络结构中，可能会很少使用甚至不使用汇聚层。 全连接层 在全连接层中，神经元对于前一层中的所有激活数据是全部连接的，这个常规神经网络中一样。它们的激活可以先用矩阵乘法，再加上偏差。更多细节请查看神经网络章节。 把全连接层转化成卷积层 全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在两类层中，神经元都是计算点积，所以它们的函数形式是一样的。因此，将此两者相互转化是可能的： 全连接层转化为卷积层 在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是224x224x3的图像，一系列的卷积层和汇聚层将图像数据变为尺寸为7x7x512的激活数据体（在AlexNet中就是这样，通过使用5个汇聚层来对输入数据进行空间上的降采样，每次尺寸下降一半，所以最终空间尺寸为224/2/2/2/2/2=7）。从这里可以看到，AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。 卷积神经网络的结构 卷积神经网络通常是由三种层构成：卷积层，汇聚层（除非特别说明，一般就是最大值汇聚）和全连接层（简称FC）。ReLU激活函数也应该算是是一层，它逐元素地进行激活函数操作。在本节中将讨论在卷积神经网络中这些层通常是如何组合在一起的。 层的排列规律 卷积神经网络最常见的形式就是将一些卷积层和ReLU层放在一起，其后紧跟汇聚层，然后重复如此直到图像在空间上被缩小到一个足够小的尺寸，在某个地方过渡成成全连接层也较为常见。最后的全连接层得到输出，比如分类评分等。换句话说，最常见的卷积神经网络结构如下： INPUT -&amp;gt; [[CONV -&amp;gt; RELU]*N -&amp;gt; POOL?]*M -&amp;gt; [FC -&amp;gt; RELU]*K -&amp;gt; FC 其中*指的是重复次数，POOL?指的是一个可选的汇聚层。其中N &amp;gt;=0,通常N&amp;lt;=3,M&amp;gt;=0,K&amp;gt;=0,通常K&amp;lt;3。 层的尺寸设置规律 输入层（包含图像的）应该能被2整除很多次。常用数字包括32（比如CIFAR-10），64，96（比如STL-10）或224（比如ImageNet卷积神经网络），384和512。 卷积层应该使用小尺寸滤波器（比如3x3或最多5x5），使用步长1。还有一点非常重要，就是对输入数据进行零填充，这样卷积层就不会改变输入数据在空间维度上的尺寸。 汇聚层负责对输入数据的空间维度进行降采样。最常用的设置是用用2x2感受野的最大值汇聚，步长为2。注意这一操作将会把输入数据中75%的激活数据丢弃（因为对宽度和高度都进行了2的降采样）。另一个不那么常用的设置是使用3x3的感受野，步长为2。最大值汇聚的感受野尺寸很少有超过3的，因为汇聚操作过于激烈，易造成数据信息丢失，这通常会导致算法性能变差。 Q&amp;amp;A 减少尺寸设置的问题 上文中展示的两种设置是很好的，因为所有的卷积层都能保持其输入数据的空间尺寸，汇聚层只负责对数据体从空间维度进行降采样。如果使用的步长大于1并且不对卷积层的输入数据使用零填充，那么就必须非常仔细地监督输入数据体通过整个卷积神经网络结构的过程，确认所有的步长和滤波器都尺寸互相吻合，卷积神经网络的结构美妙对称地联系在一起。 为什么在卷积层使用1的步长？ 在实际应用中，更小的步长效果更好。上文也已经提过，步长为1可以让空间维度的降采样全部由汇聚层负责，卷积层只负责对输入数据体的深度进行变换。 为何使用零填充？ 使用零填充除了前面提到的可以让卷积层的输出数据保持和输入数据在空间维度的不变，还可以提高算法性能。如果卷积层值进行卷积而不进行零填充，那么数据体的尺寸就会略微减小，那么图像边缘的信息就会过快地损失掉。 因为内存限制所做的妥协 在某些案例（尤其是早期的卷积神经网络结构）中，基于前面的各种规则，内存的使用量迅速飙升。例如，使用64个尺寸为3x3的滤波器对224x224x3的图像进行卷积，零填充为1，得到的激活数据体尺寸是[224x224x64]。这个数量就是一千万的激活数据，或者就是72MB的内存（每张图就是这么多，激活函数和梯度都是）。因为GPU通常因为内存导致性能瓶颈，所以做出一些妥协是必须的。在实践中，人们倾向于在网络的第一个卷积层做出妥协。例如，可以妥协可能是在第一个卷积层使用步长为2，尺寸为7x7的滤波器（比如在ZFnet中）。在AlexNet中，滤波器的尺寸的11x11，步长为4。 参考 (知乎) CS231n课程笔记翻译：卷积神经网络笔记 (cs231n) Convolutional Neural Networks (CNNs / ConvNets)</summary></entry><entry><title type="html">房思琪的初恋乐园</title><link href="http://localhost:4000/2020/05/20/%E6%88%BF%E6%80%9D%E7%90%AA%E7%9A%84%E5%88%9D%E6%81%8B%E4%B9%90%E5%9B%AD.html" rel="alternate" type="text/html" title="房思琪的初恋乐园" /><published>2020-05-20T00:00:00+09:00</published><updated>2020-05-20T00:00:00+09:00</updated><id>http://localhost:4000/2020/05/20/%E6%88%BF%E6%80%9D%E7%90%AA%E7%9A%84%E5%88%9D%E6%81%8B%E4%B9%90%E5%9B%AD</id><content type="html" xml:base="http://localhost:4000/2020/05/20/%E6%88%BF%E6%80%9D%E7%90%AA%E7%9A%84%E5%88%9D%E6%81%8B%E4%B9%90%E5%9B%AD.html">&lt;h1 id=&quot;书评&quot;&gt;书评&lt;/h1&gt;
&lt;h2 id=&quot;2020年05月20日-160729&quot;&gt;2020年05月20日 16:07:29&lt;/h2&gt;
&lt;p&gt;这本书读起来太吃力了，感觉心中承受了很大的压力与痛苦。&lt;/p&gt;

&lt;p&gt;仿佛我变成了李国华，曾经干过很过分的事情，对一个房思琪和很多人。&lt;br /&gt;
一瞬间我又变成了房思琪，看着伊纹姐承受暴力，又被憧憬的李老师以“爱”为名义肆意蹂躏。&lt;/p&gt;

&lt;p&gt;作者的文笔太好了，一个句子反反复复读几遍才能明白其中深意。&lt;br /&gt;
有的时候觉得脑子跟不上作者的文笔了。&lt;/p&gt;

&lt;p&gt;拉回现实的时候喘口气，还好只是小说。&lt;br /&gt;
我多么幸运，幸福。&lt;/p&gt;

&lt;h2 id=&quot;2020年05月21日-162208&quot;&gt;2020年05月21日 16:22:08&lt;/h2&gt;
&lt;p&gt;第二章失乐园快读完了，心里还是很压抑。&lt;br /&gt;
感觉作者的思路很乱，一段话需要反反复复的理解，值得深思。&lt;/p&gt;

&lt;p&gt;也许是我理解能力太差了，作者文笔还是太好了。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="读后感" /><summary type="html">书评 2020年05月20日 16:07:29 这本书读起来太吃力了，感觉心中承受了很大的压力与痛苦。 仿佛我变成了李国华，曾经干过很过分的事情，对一个房思琪和很多人。 一瞬间我又变成了房思琪，看着伊纹姐承受暴力，又被憧憬的李老师以“爱”为名义肆意蹂躏。 作者的文笔太好了，一个句子反反复复读几遍才能明白其中深意。 有的时候觉得脑子跟不上作者的文笔了。 拉回现实的时候喘口气，还好只是小说。 我多么幸运，幸福。 2020年05月21日 16:22:08 第二章失乐园快读完了，心里还是很压抑。 感觉作者的思路很乱，一段话需要反反复复的理解，值得深思。 也许是我理解能力太差了，作者文笔还是太好了。</summary></entry><entry><title type="html">深度学习用于计算机视觉</title><link href="http://localhost:4000/2020/05/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89.html" rel="alternate" type="text/html" title="深度学习用于计算机视觉" /><published>2020-05-18T00:00:00+09:00</published><updated>2020-05-18T00:00:00+09:00</updated><id>http://localhost:4000/2020/05/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89</id><content type="html" xml:base="http://localhost:4000/2020/05/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89.html">&lt;h1 id=&quot;卷积神经网络简介&quot;&gt;卷积神经网络简介&lt;/h1&gt;
&lt;h2 id=&quot;卷积运算&quot;&gt;卷积运算&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;密集连层：从输入特征空间中学到的是全局模式&lt;/li&gt;
  &lt;li&gt;卷积层：局部模式&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;卷积神经网络具有以下两个特性：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;学到的模式具有 &lt;strong&gt;平移不变性&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;可以学到 &lt;strong&gt;模式的空间层次结构&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;卷积由以下两个关键参数定义：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;从输入中提取的图块尺寸&lt;/li&gt;
  &lt;li&gt;输出特征图的深度：卷积所计算的过滤器的数量。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;输出的宽度和高度可能与输入的不同，原因：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;边界效应，可通过对输入特征图进行填充来抵消&lt;/li&gt;
  &lt;li&gt;使用了 &lt;strong&gt;步幅&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;最大池化运算&quot;&gt;最大池化运算&lt;/h2&gt;
&lt;p&gt;作用：对特征图进行下采样，与步进卷积类似。 &lt;br /&gt;
原因：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;减少需要处理的特征图的元素个数&lt;/li&gt;
  &lt;li&gt;通过让连续卷积层的观察窗口越来越大从而引入空间过滤器的层级结构&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;在小型数据集上从头开始训练一个卷积神经网络&quot;&gt;在小型数据集上从头开始训练一个卷积神经网络&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;数据增强&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;深度学习与小数据问题的相关性&quot;&gt;深度学习与小数据问题的相关性&lt;/h3&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><category term="Python深度学习" /><summary type="html">卷积神经网络简介 卷积运算 密集连层：从输入特征空间中学到的是全局模式 卷积层：局部模式 卷积神经网络具有以下两个特性： 学到的模式具有 平移不变性 可以学到 模式的空间层次结构 卷积由以下两个关键参数定义： 从输入中提取的图块尺寸 输出特征图的深度：卷积所计算的过滤器的数量。 输出的宽度和高度可能与输入的不同，原因： 边界效应，可通过对输入特征图进行填充来抵消 使用了 步幅 最大池化运算 作用：对特征图进行下采样，与步进卷积类似。 原因： 减少需要处理的特征图的元素个数 通过让连续卷积层的观察窗口越来越大从而引入空间过滤器的层级结构 在小型数据集上从头开始训练一个卷积神经网络 数据增强 深度学习与小数据问题的相关性</summary></entry><entry><title type="html">你当像鸟飞向你的山</title><link href="http://localhost:4000/2020/04/25/%E4%BD%A0%E5%BD%93%E5%83%8F%E9%B8%9F%E9%A3%9E%E5%90%91%E4%BD%A0%E7%9A%84%E5%B1%B1.html" rel="alternate" type="text/html" title="你当像鸟飞向你的山" /><published>2020-04-25T00:00:00+09:00</published><updated>2020-04-25T00:00:00+09:00</updated><id>http://localhost:4000/2020/04/25/%E4%BD%A0%E5%BD%93%E5%83%8F%E9%B8%9F%E9%A3%9E%E5%90%91%E4%BD%A0%E7%9A%84%E5%B1%B1</id><content type="html" xml:base="http://localhost:4000/2020/04/25/%E4%BD%A0%E5%BD%93%E5%83%8F%E9%B8%9F%E9%A3%9E%E5%90%91%E4%BD%A0%E7%9A%84%E5%B1%B1.html"></content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="读后感" /><summary type="html"></summary></entry></feed>
<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh"><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="zh" /><updated>2020-02-21T10:15:30+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">旭的小窝</title><subtitle>这里讲我的故事
</subtitle><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><entry><title type="html">深度学习入门</title><link href="http://localhost:4000/2020/02/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8.html" rel="alternate" type="text/html" title="深度学习入门" /><published>2020-02-18T00:00:00+09:00</published><updated>2020-02-18T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8</id><content type="html" xml:base="http://localhost:4000/2020/02/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8.html">&lt;h1 id=&quot;神经网络的数据表示&quot;&gt;神经网络的数据表示&lt;/h1&gt;

&lt;h2 id=&quot;张量tensor&quot;&gt;张量（tensor）&lt;/h2&gt;
&lt;p&gt;使用张量作为基本数据结构。&lt;/p&gt;

&lt;p&gt;是一个数据容器。&lt;br /&gt;
它包含的数据几乎总是数值数据，因此它是数字的容器。&lt;br /&gt;
矩阵是二维张量。&lt;/p&gt;

&lt;p&gt;张量是矩阵向任意维度的推广。
张量的 &lt;strong&gt;维度（dimension)&lt;/strong&gt; 叫做 &lt;strong&gt;轴（axis）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;标量（0D张量）&lt;br /&gt;
仅包含一个数字的张量叫做 &lt;strong&gt;标量(scalar)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;向量（1D张量）&lt;br /&gt;
数字组成的数组叫做 &lt;strong&gt;向量(vector)&lt;/strong&gt; 或 &lt;strong&gt;一维张量（1D张量）&lt;/strong&gt;&lt;br /&gt;
一维张量只有一个轴。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;矩阵（2D张量）&lt;br /&gt;
向量组成的数组叫做 &lt;strong&gt;矩阵（matrix）&lt;/strong&gt; 或 &lt;strong&gt;二维张量（2D张量）&lt;/strong&gt;&lt;br /&gt;
矩阵有两个轴（行和列）&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;3D张量与更高维度的张量
将多个矩阵组合成一个新的数组，可以得到一个3D张量。&lt;br /&gt;
直观的理解为数字组成的立方体。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;高维度张量以此类推。&lt;/p&gt;

&lt;h2 id=&quot;关键属性&quot;&gt;关键属性&lt;/h2&gt;
&lt;p&gt;张量由以下三个关键属性来定义。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;轴的个数（阶）&lt;br /&gt;
3D张量有3个轴，矩阵有2个轴。&lt;/li&gt;
  &lt;li&gt;形状&lt;br /&gt;
张量沿每个轴的维度大小（元素个数）。&lt;/li&gt;
  &lt;li&gt;数据类型&lt;br /&gt;
张量中所包含数据的类型。&lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;warning&quot;&gt;很多库中不存在字符串张量。&lt;/p&gt;

&lt;h2 id=&quot;数据批量&quot;&gt;数据批量&lt;/h2&gt;
&lt;p&gt;数据张量的第一个轴（0轴）都是&lt;strong&gt;样本轴（sample axis，样本维度）&lt;/strong&gt;。
深度学习模型不会同时处理整个数据集，而是将数据拆分成小批量。&lt;br /&gt;
对于这种批量张量，第一个轴（0轴)叫做&lt;strong&gt;批量轴（batch axis）&lt;/strong&gt;或&lt;strong&gt;批量维度（batch dimension）&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&quot;现实世界中的数据张量&quot;&gt;现实世界中的数据张量&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;向量数据：2D张量&lt;/li&gt;
  &lt;li&gt;时间序列数据 或 序列数据：3D张量&lt;/li&gt;
  &lt;li&gt;图像：4D张量&lt;/li&gt;
  &lt;li&gt;视频：5D张量&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;张量运算&quot;&gt;张量运算&lt;/h1&gt;
&lt;h2 id=&quot;逐元素运算&quot;&gt;逐元素运算&lt;/h2&gt;
&lt;h2 id=&quot;广播&quot;&gt;广播&lt;/h2&gt;
&lt;p&gt;两个形状不同的张量进行操作，会发生什么？&lt;/p&gt;

&lt;p&gt;较小的张量会被&lt;strong&gt;广播（broadcast）&lt;/strong&gt;，以匹配较大张量的形状。&lt;/p&gt;

&lt;p&gt;广播包含以下两个步骤：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;向较小的张量添加轴（叫做&lt;strong&gt;广播轴&lt;/strong&gt;）。&lt;/li&gt;
  &lt;li&gt;将较小的张量沿着新轴重复，使其形状与较大张量相同。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;张量点积&quot;&gt;张量点积&lt;/h2&gt;
&lt;p&gt;点积运算，也叫&lt;strong&gt;张量积（tensor product）&lt;/strong&gt;。&lt;br /&gt;
它将输入张量的元素合并在一起。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;z = np.dot(x,y)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;数学符号中&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;z = x.y
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p class=&quot;warning&quot;&gt;两个向量之间的点积是一个标量，而且只有元素个数相同的向量之间才能做点积。&lt;/p&gt;

&lt;h2 id=&quot;张量变形tensor-reshaping&quot;&gt;张量变形（tensor reshaping）&lt;/h2&gt;
&lt;p&gt;张量变形是指改变张量的行和列，以得到想要的形状。&lt;/p&gt;

&lt;p&gt;特殊的张量变形：转置（transposition）。&lt;/p&gt;

&lt;h1 id=&quot;基于梯度的优化&quot;&gt;基于梯度的优化&lt;/h1&gt;
&lt;h2 id=&quot;训练循环training-loop&quot;&gt;训练循环（training loop)&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;抽取训练样本x和对应目标y组成的数据批量&lt;/li&gt;
  &lt;li&gt;在x上运行网络[前向传播（forward pass）]，得到预测值y_pred&lt;/li&gt;
  &lt;li&gt;计算网络在这批数据上的损失，用于衡量y_pred和y之间的距离。&lt;/li&gt;
  &lt;li&gt;更新网络的所有权重，使网络在这批数据上的损失略微下降。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;对于第四步，更好的方法是利用网络中所有运算都是 &lt;strong&gt;可微（differentiable）&lt;/strong&gt; 的，计算损失相对于网络系数的 &lt;strong&gt;梯度（gradient）&lt;/strong&gt;，然后向梯度的反方向改变系数，从而使损失降低。&lt;/p&gt;

&lt;p class=&quot;info&quot;&gt;可微的意思是“可以被求导”&lt;/p&gt;

&lt;h2 id=&quot;张量运算的导数梯度&quot;&gt;张量运算的导数：梯度&lt;/h2&gt;
&lt;h2 id=&quot;随机梯度下降&quot;&gt;随机梯度下降&lt;/h2&gt;
&lt;h3 id=&quot;小批量随机梯度下降mini-batch-stochastic-gradient-descent&quot;&gt;小批量随机梯度下降（mini-batch stochastic gradient descent）&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;抽取训练样本x和对应目标y组成的数据批量&lt;/li&gt;
  &lt;li&gt;在x上运行网络，得到预测值y_pred&lt;/li&gt;
  &lt;li&gt;计算网络在这批数据上的损失，用于衡量y_pred和y之间的距离。&lt;/li&gt;
  &lt;li&gt;计算损失相对于网络参数的梯度【一次**反向传播（backward pass）】。&lt;/li&gt;
  &lt;li&gt;将参数沿着梯度的反方向移动一点，从而使这批数据上的损失减小一点。&lt;/li&gt;
&lt;/ol&gt;

&lt;p class=&quot;info&quot;&gt;&lt;strong&gt;随机（stochastic）&lt;/strong&gt;是指每批数据都是随机抽取的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;真SGD&lt;/strong&gt;：每次迭代时只抽取一个样本和目标。&lt;br /&gt;
&lt;strong&gt;批量SGD&lt;/strong&gt;：每一次迭代都在所有数据上运行。这样每次更新都更加准确，但计算代价也高很多。&lt;/p&gt;

&lt;p&gt;SGD还有很多种变体，带动量的SGD等变体，被称为&lt;strong&gt;优化方法（optimization method）&lt;/strong&gt;或 &lt;strong&gt;优化器（optimizer）&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;动量方法的实现过程是，每一步都移动小球，不仅要考虑当前的斜率值（当前的加速度），还要考虑当前的速度（来自于之前的加速度）。&lt;br /&gt;
在神经网络中，更新参数w不仅要考虑当前的梯度值，还要考虑上一次的参数更新。&lt;/p&gt;

&lt;h2 id=&quot;链式求导反向传播算法&quot;&gt;链式求导：反向传播算法&lt;/h2&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><summary type="html">神经网络的数据表示 张量（tensor） 使用张量作为基本数据结构。 是一个数据容器。 它包含的数据几乎总是数值数据，因此它是数字的容器。 矩阵是二维张量。 张量是矩阵向任意维度的推广。 张量的 维度（dimension) 叫做 轴（axis） 标量（0D张量） 仅包含一个数字的张量叫做 标量(scalar) 向量（1D张量） 数字组成的数组叫做 向量(vector) 或 一维张量（1D张量） 一维张量只有一个轴。 矩阵（2D张量） 向量组成的数组叫做 矩阵（matrix） 或 二维张量（2D张量） 矩阵有两个轴（行和列） 3D张量与更高维度的张量 将多个矩阵组合成一个新的数组，可以得到一个3D张量。 直观的理解为数字组成的立方体。 高维度张量以此类推。 关键属性 张量由以下三个关键属性来定义。 轴的个数（阶） 3D张量有3个轴，矩阵有2个轴。 形状 张量沿每个轴的维度大小（元素个数）。 数据类型 张量中所包含数据的类型。 很多库中不存在字符串张量。 数据批量 数据张量的第一个轴（0轴）都是样本轴（sample axis，样本维度）。 深度学习模型不会同时处理整个数据集，而是将数据拆分成小批量。 对于这种批量张量，第一个轴（0轴)叫做批量轴（batch axis）或批量维度（batch dimension）。 现实世界中的数据张量 向量数据：2D张量 时间序列数据 或 序列数据：3D张量 图像：4D张量 视频：5D张量 张量运算 逐元素运算 广播 两个形状不同的张量进行操作，会发生什么？ 较小的张量会被广播（broadcast），以匹配较大张量的形状。 广播包含以下两个步骤： 向较小的张量添加轴（叫做广播轴）。 将较小的张量沿着新轴重复，使其形状与较大张量相同。 张量点积 点积运算，也叫张量积（tensor product）。 它将输入张量的元素合并在一起。 z = np.dot(x,y) 数学符号中 z = x.y 两个向量之间的点积是一个标量，而且只有元素个数相同的向量之间才能做点积。 张量变形（tensor reshaping） 张量变形是指改变张量的行和列，以得到想要的形状。 特殊的张量变形：转置（transposition）。 基于梯度的优化 训练循环（training loop) 抽取训练样本x和对应目标y组成的数据批量 在x上运行网络[前向传播（forward pass）]，得到预测值y_pred 计算网络在这批数据上的损失，用于衡量y_pred和y之间的距离。 更新网络的所有权重，使网络在这批数据上的损失略微下降。 对于第四步，更好的方法是利用网络中所有运算都是 可微（differentiable） 的，计算损失相对于网络系数的 梯度（gradient），然后向梯度的反方向改变系数，从而使损失降低。 可微的意思是“可以被求导” 张量运算的导数：梯度 随机梯度下降 小批量随机梯度下降（mini-batch stochastic gradient descent） 抽取训练样本x和对应目标y组成的数据批量 在x上运行网络，得到预测值y_pred 计算网络在这批数据上的损失，用于衡量y_pred和y之间的距离。 计算损失相对于网络参数的梯度【一次**反向传播（backward pass）】。 将参数沿着梯度的反方向移动一点，从而使这批数据上的损失减小一点。 随机（stochastic）是指每批数据都是随机抽取的。 真SGD：每次迭代时只抽取一个样本和目标。 批量SGD：每一次迭代都在所有数据上运行。这样每次更新都更加准确，但计算代价也高很多。 SGD还有很多种变体，带动量的SGD等变体，被称为优化方法（optimization method）或 优化器（optimizer）。 动量方法的实现过程是，每一步都移动小球，不仅要考虑当前的斜率值（当前的加速度），还要考虑当前的速度（来自于之前的加速度）。 在神经网络中，更新参数w不仅要考虑当前的梯度值，还要考虑上一次的参数更新。 链式求导：反向传播算法</summary></entry><entry><title type="html">最优化笔记</title><link href="http://localhost:4000/2020/02/18/%E6%9C%80%E4%BC%98%E5%8C%96.html" rel="alternate" type="text/html" title="最优化笔记" /><published>2020-02-18T00:00:00+09:00</published><updated>2020-02-18T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/18/%E6%9C%80%E4%BC%98%E5%8C%96</id><content type="html" xml:base="http://localhost:4000/2020/02/18/%E6%9C%80%E4%BC%98%E5%8C%96.html">&lt;h1 id=&quot;cs231n-optimization&quot;&gt;cs231n optimization&lt;/h1&gt;
&lt;p&gt;Optimization is the process of finding the set of parameters W that minimize the loss function.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit&quot;&gt;(知乎)最优化笔记&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;http://cs231n.github.io/optimization-1/&quot;&gt;cs231n&lt;/a&gt;&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><category term="cs231n" /><summary type="html">cs231n optimization Optimization is the process of finding the set of parameters W that minimize the loss function. (知乎)最优化笔记 cs231n</summary></entry><entry><title type="html">PyTorch张量</title><link href="http://localhost:4000/2020/02/17/Pytorch.html" rel="alternate" type="text/html" title="PyTorch张量" /><published>2020-02-17T00:00:00+09:00</published><updated>2020-02-17T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/17/Pytorch</id><content type="html" xml:base="http://localhost:4000/2020/02/17/Pytorch.html">&lt;h1 id=&quot;基本数据类型&quot;&gt;基本数据类型&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;标量（0维）&lt;br /&gt;
一维长度为1的张量确实也可以表示张量
为了语义更清晰，在版本PyTorch0.3之后，两个概念从形式上加以区分，并增加了长度为零的 tensor。&lt;/li&gt;
  &lt;li&gt;张量（1维） Bias or Linear input （单张图片输入）&lt;/li&gt;
  &lt;li&gt;张量（2维） Linear input batch （多张图片输入）&lt;/li&gt;
  &lt;li&gt;张量（3维） RNN input Batch （循环神经网络批量输入）
[word,sentence,feature]&lt;/li&gt;
  &lt;li&gt;张量（4维） CNN input Batch （卷积神经网络批量输入）
[batch,channel,height,width] ’r’,’g’,’b’三原色通道&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;张量-tensors&quot;&gt;张量 Tensors&lt;/h1&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from __future__ import print_function
import torch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;引入pytorch&lt;/p&gt;

&lt;h3 id=&quot;创建一个-5x3-矩阵-但是未初始化&quot;&gt;创建一个 5x3 矩阵, 但是未初始化:&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = torch.empty(5, 3)
print(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;创建一个随机初始化的矩阵&quot;&gt;创建一个随机初始化的矩阵:&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = torch.rand(5, 3)
print(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;创建一个0填充的矩阵数据类型为long&quot;&gt;创建一个0填充的矩阵，数据类型为long:&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = torch.zeros(5, 3, dtype=torch.long)
print(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;创建tensor并使用现有数据初始化&quot;&gt;创建tensor并使用现有数据初始化:&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = torch.tensor([5.5, 3])
print(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;根据现有的张量创建张量&quot;&gt;根据现有的张量创建张量:&lt;/h3&gt;
&lt;p&gt;这些方法将重用输入张量的属性，例如， dtype，除非设置新的值进行覆盖&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = x.new_ones(5, 3, dtype=torch.double)      # new_* 方法来创建对象
print(x)

x = torch.randn_like(x, dtype=torch.float)    # 覆盖 dtype!
print(x)                                      #  对象的size 是相同的，只是值和类型发生了变化
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;获取size&quot;&gt;获取size&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print(x.size())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;加法&quot;&gt;加法&lt;/h1&gt;
&lt;h3 id=&quot;加法1&quot;&gt;加法1&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;y = torch.rand(5, 3)
print(x + y)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tensor([[ 0.2218,  0.8329, -1.3406],
        [-0.2737,  0.8382,  1.4644],
        [-0.3806,  0.2332, -0.4300],
        [-0.6603,  1.8713,  1.9648],
        [ 1.4351, -0.6195, -0.5985]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;加法2&quot;&gt;加法2&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print(torch.add(x, y))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;提供输出tensor作为参数&quot;&gt;提供输出tensor作为参数&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;result = torch.empty(5, 3)
torch.add(x, y, out=result)
print(result)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;替换&quot;&gt;替换&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# adds x to y
y.add_(x)
print(y)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p class=&quot;warning&quot;&gt;任何 以&lt;code class=&quot;highlighter-rouge&quot;&gt;_&lt;/code&gt; 结尾的操作都会用结果替换原变量. 例如: &lt;code class=&quot;highlighter-rouge&quot;&gt;x.copy_(y)&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;x.t_()&lt;/code&gt;, 都会改变 &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;使用索引操作张量&quot;&gt;使用索引操作张量&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print(x[:, 1])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tensor([ 1.6401,  0.3637,  1.5745, -1.9971,  1.2926])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;torchview-改变张量的维度和大小&quot;&gt;torch.view 改变张量的维度和大小&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8)  #  size -1 从其他维度推断
print(x.size(), y.size(), z.size())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;如果你有只有一个元素的张量&quot;&gt;如果你有只有一个元素的张量&lt;/h3&gt;
&lt;p&gt;使用.item()来得到Python数据类型的数值&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = torch.randn(1)
print(x)
print(x.item())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tensor([-0.4353])
-0.43528521060943604
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><category term="PyTorch" /><summary type="html">基本数据类型 标量（0维） 一维长度为1的张量确实也可以表示张量 为了语义更清晰，在版本PyTorch0.3之后，两个概念从形式上加以区分，并增加了长度为零的 tensor。 张量（1维） Bias or Linear input （单张图片输入） 张量（2维） Linear input batch （多张图片输入） 张量（3维） RNN input Batch （循环神经网络批量输入） [word,sentence,feature] 张量（4维） CNN input Batch （卷积神经网络批量输入） [batch,channel,height,width] ’r’,’g’,’b’三原色通道 张量 Tensors from __future__ import print_function import torch 引入pytorch 创建一个 5x3 矩阵, 但是未初始化: x = torch.empty(5, 3) print(x) 创建一个随机初始化的矩阵: x = torch.rand(5, 3) print(x) 创建一个0填充的矩阵，数据类型为long: x = torch.zeros(5, 3, dtype=torch.long) print(x) 创建tensor并使用现有数据初始化: x = torch.tensor([5.5, 3]) print(x) 根据现有的张量创建张量: 这些方法将重用输入张量的属性，例如， dtype，除非设置新的值进行覆盖 x = x.new_ones(5, 3, dtype=torch.double) # new_* 方法来创建对象 print(x) x = torch.randn_like(x, dtype=torch.float) # 覆盖 dtype! print(x) # 对象的size 是相同的，只是值和类型发生了变化 获取size print(x.size()) 加法 加法1 y = torch.rand(5, 3) print(x + y) tensor([[ 0.2218, 0.8329, -1.3406], [-0.2737, 0.8382, 1.4644], [-0.3806, 0.2332, -0.4300], [-0.6603, 1.8713, 1.9648], [ 1.4351, -0.6195, -0.5985]]) 加法2 print(torch.add(x, y)) 提供输出tensor作为参数 result = torch.empty(5, 3) torch.add(x, y, out=result) print(result) 替换 # adds x to y y.add_(x) print(y) 任何 以_ 结尾的操作都会用结果替换原变量. 例如: x.copy_(y), x.t_(), 都会改变 x. 使用索引操作张量 print(x[:, 1]) tensor([ 1.6401, 0.3637, 1.5745, -1.9971, 1.2926]) torch.view 改变张量的维度和大小 x = torch.randn(4, 4) y = x.view(16) z = x.view(-1, 8) # size -1 从其他维度推断 print(x.size(), y.size(), z.size()) 如果你有只有一个元素的张量 使用.item()来得到Python数据类型的数值 x = torch.randn(1) print(x) print(x.item()) tensor([-0.4353]) -0.43528521060943604</summary></entry><entry><title type="html">自动求导</title><link href="http://localhost:4000/2020/02/17/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC.html" rel="alternate" type="text/html" title="自动求导" /><published>2020-02-17T00:00:00+09:00</published><updated>2020-02-17T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/17/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC</id><content type="html" xml:base="http://localhost:4000/2020/02/17/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC.html">&lt;h1 id=&quot;autograd-自动求导机制&quot;&gt;Autograd: 自动求导机制&lt;/h1&gt;
&lt;p&gt;PyTorch 中所有神经网络的核心是 autograd 包。&lt;/p&gt;

&lt;p&gt;autograd包为张量上的所有操作提供了自动求导。 它是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。&lt;/p&gt;

&lt;h2 id=&quot;张量torchtensor&quot;&gt;张量（torch.Tensor）&lt;/h2&gt;
&lt;p&gt;torch.Tensor是这个包的核心类。&lt;/p&gt;

&lt;p&gt;如果设置 .requires_grad 为 True，那么将会追踪所有对于该张量的操作。&lt;br /&gt;
当完成计算后通过调用 .backward()，自动计算所有的梯度，这个张量的所有梯度将会自动积累到 .grad 属性。&lt;/p&gt;

&lt;p&gt;要阻止张量跟踪历史记录，可以调用.detach()方法将其与计算历史记录分离，并禁止跟踪它将来的计算记录。&lt;/p&gt;

&lt;p&gt;在自动梯度计算中还有另外一个重要的类Function.&lt;/p&gt;

&lt;p&gt;如果需要计算导数，你可以在Tensor上调用.backward()。&lt;br /&gt;
如果Tensor是一个标量（即它包含一个元素数据）则不需要为backward()指定任何参数，但是如果它有更多的元素，你需要指定一个gradient 参数来匹配张量的形状。&lt;/p&gt;

&lt;h3 id=&quot;过程&quot;&gt;过程&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;导入包
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;创建一个张量并设置 requires_grad=True 用来追踪他的计算历史
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x = torch.ones(2, 2, requires_grad=True)
print(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;对张量进行操作:
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;y = x + 2
print(y)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;结果y已经被计算出来了，所以，grad_fn已经被自动生成了。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print(y.grad_fn)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;对y进行一个操作
```
out = z.mean()&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;print(z, out)&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;6. .requires_grad_( ... ) 
.requires_grad_( ... ) 可以改变现有张量的 requires_grad属性。 如果没有指定的话，默认输入的flag是 False。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;a = torch.randn(2, 2)
a = ((a * 3) / (a - 1))
print(a.requires_grad)
a.requires_grad_(True)
print(a.requires_grad)
b = (a * a).sum()
print(b.grad_fn)&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;False
True
&amp;lt;SumBackward0 object at 0x000002004F7D5608&amp;gt;
```&lt;/p&gt;
&lt;h2 id=&quot;梯度&quot;&gt;梯度&lt;/h2&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><category term="PyTorch" /><summary type="html">Autograd: 自动求导机制 PyTorch 中所有神经网络的核心是 autograd 包。 autograd包为张量上的所有操作提供了自动求导。 它是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。 张量（torch.Tensor） torch.Tensor是这个包的核心类。 如果设置 .requires_grad 为 True，那么将会追踪所有对于该张量的操作。 当完成计算后通过调用 .backward()，自动计算所有的梯度，这个张量的所有梯度将会自动积累到 .grad 属性。 要阻止张量跟踪历史记录，可以调用.detach()方法将其与计算历史记录分离，并禁止跟踪它将来的计算记录。 在自动梯度计算中还有另外一个重要的类Function. 如果需要计算导数，你可以在Tensor上调用.backward()。 如果Tensor是一个标量（即它包含一个元素数据）则不需要为backward()指定任何参数，但是如果它有更多的元素，你需要指定一个gradient 参数来匹配张量的形状。 过程 导入包 import torch 创建一个张量并设置 requires_grad=True 用来追踪他的计算历史 x = torch.ones(2, 2, requires_grad=True) print(x) 对张量进行操作: y = x + 2 print(y) 结果y已经被计算出来了，所以，grad_fn已经被自动生成了。 print(y.grad_fn) 对y进行一个操作 ``` out = z.mean() print(z, out) 6. .requires_grad_( ... ) .requires_grad_( ... ) 可以改变现有张量的 requires_grad属性。 如果没有指定的话，默认输入的flag是 False。 a = torch.randn(2, 2) a = ((a * 3) / (a - 1)) print(a.requires_grad) a.requires_grad_(True) print(a.requires_grad) b = (a * a).sum() print(b.grad_fn) False True &amp;lt;SumBackward0 object at 0x000002004F7D5608&amp;gt; ``` 梯度</summary></entry><entry><title type="html">彭寒薇，生日快乐！</title><link href="http://localhost:4000/2020/02/17/%E5%BD%AD%E5%AF%92%E8%96%87%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90.html" rel="alternate" type="text/html" title="彭寒薇，生日快乐！" /><published>2020-02-17T00:00:00+09:00</published><updated>2020-02-17T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/17/%E5%BD%AD%E5%AF%92%E8%96%87%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90!</id><content type="html" xml:base="http://localhost:4000/2020/02/17/%E5%BD%AD%E5%AF%92%E8%96%87%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90.html">&lt;p&gt;彭寒薇，生日快乐！&lt;/p&gt;

&lt;p&gt;这个生日一定是你过得最特别的一个，也是我给别人过得最特别的一个。&lt;/p&gt;

&lt;p&gt;祝你天天快乐，每天开心！&lt;/p&gt;

&lt;div&gt;&lt;div class=&quot;extensions extensions--video&quot;&gt;
  &lt;iframe src=&quot;//player.bilibili.com/player.html?aid=89520991&amp;amp;page=1&quot; frameborder=&quot;no&quot; scrolling=&quot;no&quot; allowfullscreen=&quot;true&quot;&gt;
  &lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="写给谁的情书" /><summary type="html">彭寒薇，生日快乐！ 这个生日一定是你过得最特别的一个，也是我给别人过得最特别的一个。 祝你天天快乐，每天开心！</summary></entry><entry><title type="html">请回答1988</title><link href="http://localhost:4000/2020/02/17/%E8%AF%B7%E5%9B%9E%E7%AD%941988.html" rel="alternate" type="text/html" title="请回答1988" /><published>2020-02-17T00:00:00+09:00</published><updated>2020-02-17T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/17/%E8%AF%B7%E5%9B%9E%E7%AD%941988</id><content type="html" xml:base="http://localhost:4000/2020/02/17/%E8%AF%B7%E5%9B%9E%E7%AD%941988.html">&lt;h1 id=&quot;写在开头&quot;&gt;写在开头&lt;/h1&gt;
&lt;p&gt;又开坑了这个电视剧，这次是二刷。姑且是下载好了，之后需要多长时间才能看完就不知道了。毕竟这个电视剧每一集的时间还是很长的，而我现在也没有那么多时间都话在看电视剧上面。&lt;/p&gt;

&lt;p&gt;慢慢看，慢慢写感想吧。&lt;/p&gt;

&lt;h3 id=&quot;2020年02月17日-164957&quot;&gt;2020年02月17日 16:49:57&lt;/h3&gt;
&lt;p&gt;发现我真的是一个多情的人，今天看了三集《请回答1988》。被剧中的剧情有所触动，哭的稀里哗啦的。说好的男儿有泪不轻弹呢，到我这边就不起效果了呀。&lt;/p&gt;

&lt;p&gt;虽然是第二遍看这个电视剧了，但是时隔这么多年再看感概还是那么的真实。&lt;br /&gt;
虽然细节的剧情已经全部都忘记了，但是演员塑造的人物性格还是记着一清二楚的。&lt;/p&gt;

&lt;p&gt;我想，我喜欢这个电视剧就是因为它讲的故事特别的真实吧。&lt;br /&gt;
其实也不是什么轰轰烈烈的故事，只不过是一个胡同，一个时代背景下，普普通通的三口人家的生活。但是在我看来，确实能治愈我的心灵。&lt;br /&gt;
让我明白一些道理，也让我知道大家都是怎么生活的。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="观后感" /><summary type="html">写在开头 又开坑了这个电视剧，这次是二刷。姑且是下载好了，之后需要多长时间才能看完就不知道了。毕竟这个电视剧每一集的时间还是很长的，而我现在也没有那么多时间都话在看电视剧上面。 慢慢看，慢慢写感想吧。 2020年02月17日 16:49:57 发现我真的是一个多情的人，今天看了三集《请回答1988》。被剧中的剧情有所触动，哭的稀里哗啦的。说好的男儿有泪不轻弹呢，到我这边就不起效果了呀。 虽然是第二遍看这个电视剧了，但是时隔这么多年再看感概还是那么的真实。 虽然细节的剧情已经全部都忘记了，但是演员塑造的人物性格还是记着一清二楚的。 我想，我喜欢这个电视剧就是因为它讲的故事特别的真实吧。 其实也不是什么轰轰烈烈的故事，只不过是一个胡同，一个时代背景下，普普通通的三口人家的生活。但是在我看来，确实能治愈我的心灵。 让我明白一些道理，也让我知道大家都是怎么生活的。</summary></entry><entry xml:lang="zh"><title type="html">每日一笔</title><link href="http://localhost:4000/2020/02/09/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94.html" rel="alternate" type="text/html" title="每日一笔" /><published>2020-02-09T00:00:00+09:00</published><updated>2020-02-09T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/09/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94</id><content type="html" xml:base="http://localhost:4000/2020/02/09/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94.html">&lt;h2 id=&quot;每日一笔记录生活&quot;&gt;每日一笔，记录生活&lt;/h2&gt;
&lt;h3 id=&quot;2020年02月09日-114045&quot;&gt;2020年02月09日 11:40:45&lt;/h3&gt;
&lt;p&gt;今天早上偶然找到了戴胜的微博，点进去看了看之后，很是震惊。&lt;br /&gt;
原来在我努力讨好一个人的时候，我的情敌正在默默努力提高自己。&lt;br /&gt;
看来这就是敌我差距啊。不被人喜欢也是正常的事情啦。&lt;br /&gt;
好好加油啊，陈。&lt;/p&gt;

&lt;p&gt;今天开始收拾东西，明天搬家。&lt;br /&gt;
东西越收拾越多，已经有两个大箱子了。&lt;br /&gt;
最可怕的是搬家的时候找不到人帮忙，看来我做人真的是有问题的吧。&lt;/p&gt;

&lt;p&gt;加油收拾家，努力生活。
今日一笔结束。&lt;/p&gt;

&lt;h3 id=&quot;2020年02月11日-001111&quot;&gt;2020年02月11日 00:11:11&lt;/h3&gt;
&lt;p&gt;说是每日一笔，其实现在已经是第二天的凌晨了。&lt;/p&gt;

&lt;p&gt;今天一天过得特别忙碌，没什么时间写东西，也没什么时间拍照片呀。&lt;br /&gt;
忙是忙了些，感觉上还是很充实的。&lt;br /&gt;
今天打工的时候想，拿出珍藏已久的口琴吹一吹。&lt;br /&gt;
又想到吹太大声音的话，会不会影响到周围的邻居啊。别过几天再被人举报了。&lt;br /&gt;
那就太尴尬了。😅&lt;/p&gt;

&lt;p&gt;话说是不是应该剪视频了，之前的素材还存着挺多的呢。
努力生活呀，陈。&lt;/p&gt;

&lt;p&gt;今日一笔结束。&lt;/p&gt;

&lt;h3 id=&quot;2020年02月12日-110949&quot;&gt;2020年02月12日 11:09:49&lt;/h3&gt;
&lt;p&gt;搬家之后特别的忙，有很多事情等着去处理。&lt;/p&gt;

&lt;p&gt;其实已经不知道第几次觉得自己时间不够用了。&lt;br /&gt;
也不知道是不是时间利用的效率不够高。每天起床的时间都挺早的，可惜还是觉得时间很紧张。要是一天能多一点时间给我就好啦！&lt;/p&gt;

&lt;p&gt;今天早上走上山，感觉还不错。&lt;br /&gt;
犹记得以前的自己就很爱散步，中午休息的时间都会去公司周围的公园走上四十分钟。到了日本之后也许是住的离学校比较近，每天运动的时间都很少了。&lt;br /&gt;
趁着这个机会，以后正好可以多运动一下。虽然上下学的时候不方便，但是能运动就是很好的机会啦！而且这边环境着实不错，多运动运动有益身心健康。&lt;br /&gt;
跑步的事情差不多也要继续开始了。前几天下的小雪也消融了，正好是一个重新开始的好机会。&lt;/p&gt;

&lt;p&gt;加油生活呀，陈。&lt;/p&gt;

&lt;h3 id=&quot;2020年02月13日-151008&quot;&gt;2020年02月13日 15:10:08&lt;/h3&gt;
&lt;p&gt;昨天晚上出去跑步了。&lt;br /&gt;
这几天金泽的天气实在太不给力了，才刚跑了一会儿就开始下雨了。虽然不是瓢泼大雨，但还是有一些影响。&lt;br /&gt;
然后很长时间没有进行运动，体力下降的太快了。昨天才跑了2km多一点，晚上的时候就觉得大腿内侧有一些疼痛。顺便一说，昨天的配速有5.5min/km，虽然跑的有些快，但也不应该腿会这么疼。&lt;br /&gt;
之前买了一个迪卡侬的夜跑灯，昨天用了一下之后感觉太好用啦实在是，简直就是夜跑的利器啊！&lt;br /&gt;
昨天还买了红酒喝！感觉自己的生活越来越懂得享受了。晚上的时候喝一杯刚好睡觉，很享受微醺的感觉。&lt;br /&gt;
今天中午第一次在会馆做饭，认识了很多新的同学。&lt;/p&gt;

&lt;p&gt;我要先做好自己，然后再做其他的事情。加油！&lt;/p&gt;

&lt;h3 id=&quot;2020年02月14日-095432&quot;&gt;2020年02月14日 09:54:32&lt;/h3&gt;
&lt;p&gt;昨天继续跑步，3km，天气很给力没有闹小矛盾。&lt;br /&gt;
侥幸买到了两盒口罩，还买了一包暖宝宝。&lt;br /&gt;
今天继续录视频，在找人帮忙的路上充满了坎坷。应该仔细反思是不是自己为人处事出现了问题。&lt;br /&gt;
准备好了情人节巧克力，在イオン纠结了半天，巧克力的种类实在是太多了，头秃的很。&lt;/p&gt;

&lt;p&gt;突然发现，就我个人来说。有压力，有对手的时候进步最快。以前也是这样的，考完修士之后感觉自己失去了目标，然后又是各种乱七八糟的事情搞得每天不知道自己在干什么。一些好的习惯都渐渐的忘掉了，也降低了对自己的要求。&lt;br /&gt;
现在看到了对手，也看到了未来。虽然明确的方向还是没有，但是比起计划总是能先行动起来了。我相信只要保持下去，就能找回原来的自己。&lt;br /&gt;
原来，自己和自己奋斗，也可以变成一个有趣的人。&lt;/p&gt;

&lt;h3 id=&quot;2020年02月16日-233210&quot;&gt;2020年02月16日 23:32:10&lt;/h3&gt;
&lt;p&gt;又是连着两天没有没有跑步。&lt;br /&gt;
昨天晚上打工，今天下了一整天的小雨。总是想跑步的时候，总是有一些额外的事情出来干扰我的计划。&lt;br /&gt;
心烦。&lt;/p&gt;

&lt;p&gt;今天和黄同学收了一个小太阳。虽然最近的温度有所回升，但是晚上的时候还是能感受到屋子里面的温度明显低了很多。有了小太阳之后，也感觉不是那么的寒冷了。&lt;br /&gt;
想起来在上海的时候，那个时候心疼电费，冬天不舍得开空调。洗完澡的时候也是最难熬的时候，因为冷的透彻心扉。那个时候就靠着一个小小的小太阳度过每个难熬的夜晚。那段时间的经历真的让人记忆深刻。&lt;br /&gt;
晚上睡觉的时候开一会儿电热毯才能入睡，关掉之后必须马上进入梦乡，要不然特别的冷。&lt;br /&gt;
距离那个时候过去了差不多一年的时间，有时会想起那时的生活。每天虽然很辛苦，但是能感受到在真真正正的奋斗着。一个人在陌生的城市，既孤独又拼搏。&lt;/p&gt;

&lt;p&gt;年轻时候的人生就是这样的吧。&lt;br /&gt;
每日一笔结束。&lt;/p&gt;

&lt;h3 id=&quot;2020年02月18日-142225&quot;&gt;2020年02月18日 14:22:25&lt;/h3&gt;
&lt;p&gt;中午的时候去图书馆坐了一会儿，就是上次那个位置。&lt;br /&gt;
上次去过一次之后就爱上了，视野很开阔，能看到很不错风景。&lt;br /&gt;
今天中午的时候又飘了一会儿雪花，景色真的很不错。&lt;/p&gt;

&lt;p&gt;要走的时候回头看到一个人趴在桌子上，和你很像。&lt;br /&gt;
路过的时候仔细看了看，趴在桌子上也看不到正面。&lt;br /&gt;
但是衣服，裤子，鞋都很像。&lt;br /&gt;
也许是我看错了，我想世界上应该没有两个人，穿衣打扮都这么相似的吧。&lt;br /&gt;
而且这个时候你应该还在做实验。之后才想到，其实应该看看袜子像不像的吧。&lt;/p&gt;

&lt;p&gt;后来还是默默的走开了，回去的路上在想，是不是应该买一杯咖啡放在桌子上。&lt;br /&gt;
但是又觉得太自作多情了，这种剧情应该只存在于电影故事之中吧。&lt;/p&gt;

&lt;p&gt;昨天买了布鲁斯口琴，明天就能到啦！&lt;br /&gt;
加油生活呀！&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><summary type="html">每日一笔，记录生活 2020年02月09日 11:40:45 今天早上偶然找到了戴胜的微博，点进去看了看之后，很是震惊。 原来在我努力讨好一个人的时候，我的情敌正在默默努力提高自己。 看来这就是敌我差距啊。不被人喜欢也是正常的事情啦。 好好加油啊，陈。 今天开始收拾东西，明天搬家。 东西越收拾越多，已经有两个大箱子了。 最可怕的是搬家的时候找不到人帮忙，看来我做人真的是有问题的吧。 加油收拾家，努力生活。 今日一笔结束。 2020年02月11日 00:11:11 说是每日一笔，其实现在已经是第二天的凌晨了。 今天一天过得特别忙碌，没什么时间写东西，也没什么时间拍照片呀。 忙是忙了些，感觉上还是很充实的。 今天打工的时候想，拿出珍藏已久的口琴吹一吹。 又想到吹太大声音的话，会不会影响到周围的邻居啊。别过几天再被人举报了。 那就太尴尬了。😅 话说是不是应该剪视频了，之前的素材还存着挺多的呢。 努力生活呀，陈。 今日一笔结束。 2020年02月12日 11:09:49 搬家之后特别的忙，有很多事情等着去处理。 其实已经不知道第几次觉得自己时间不够用了。 也不知道是不是时间利用的效率不够高。每天起床的时间都挺早的，可惜还是觉得时间很紧张。要是一天能多一点时间给我就好啦！ 今天早上走上山，感觉还不错。 犹记得以前的自己就很爱散步，中午休息的时间都会去公司周围的公园走上四十分钟。到了日本之后也许是住的离学校比较近，每天运动的时间都很少了。 趁着这个机会，以后正好可以多运动一下。虽然上下学的时候不方便，但是能运动就是很好的机会啦！而且这边环境着实不错，多运动运动有益身心健康。 跑步的事情差不多也要继续开始了。前几天下的小雪也消融了，正好是一个重新开始的好机会。 加油生活呀，陈。 2020年02月13日 15:10:08 昨天晚上出去跑步了。 这几天金泽的天气实在太不给力了，才刚跑了一会儿就开始下雨了。虽然不是瓢泼大雨，但还是有一些影响。 然后很长时间没有进行运动，体力下降的太快了。昨天才跑了2km多一点，晚上的时候就觉得大腿内侧有一些疼痛。顺便一说，昨天的配速有5.5min/km，虽然跑的有些快，但也不应该腿会这么疼。 之前买了一个迪卡侬的夜跑灯，昨天用了一下之后感觉太好用啦实在是，简直就是夜跑的利器啊！ 昨天还买了红酒喝！感觉自己的生活越来越懂得享受了。晚上的时候喝一杯刚好睡觉，很享受微醺的感觉。 今天中午第一次在会馆做饭，认识了很多新的同学。 我要先做好自己，然后再做其他的事情。加油！ 2020年02月14日 09:54:32 昨天继续跑步，3km，天气很给力没有闹小矛盾。 侥幸买到了两盒口罩，还买了一包暖宝宝。 今天继续录视频，在找人帮忙的路上充满了坎坷。应该仔细反思是不是自己为人处事出现了问题。 准备好了情人节巧克力，在イオン纠结了半天，巧克力的种类实在是太多了，头秃的很。 突然发现，就我个人来说。有压力，有对手的时候进步最快。以前也是这样的，考完修士之后感觉自己失去了目标，然后又是各种乱七八糟的事情搞得每天不知道自己在干什么。一些好的习惯都渐渐的忘掉了，也降低了对自己的要求。 现在看到了对手，也看到了未来。虽然明确的方向还是没有，但是比起计划总是能先行动起来了。我相信只要保持下去，就能找回原来的自己。 原来，自己和自己奋斗，也可以变成一个有趣的人。 2020年02月16日 23:32:10 又是连着两天没有没有跑步。 昨天晚上打工，今天下了一整天的小雨。总是想跑步的时候，总是有一些额外的事情出来干扰我的计划。 心烦。 今天和黄同学收了一个小太阳。虽然最近的温度有所回升，但是晚上的时候还是能感受到屋子里面的温度明显低了很多。有了小太阳之后，也感觉不是那么的寒冷了。 想起来在上海的时候，那个时候心疼电费，冬天不舍得开空调。洗完澡的时候也是最难熬的时候，因为冷的透彻心扉。那个时候就靠着一个小小的小太阳度过每个难熬的夜晚。那段时间的经历真的让人记忆深刻。 晚上睡觉的时候开一会儿电热毯才能入睡，关掉之后必须马上进入梦乡，要不然特别的冷。 距离那个时候过去了差不多一年的时间，有时会想起那时的生活。每天虽然很辛苦，但是能感受到在真真正正的奋斗着。一个人在陌生的城市，既孤独又拼搏。 年轻时候的人生就是这样的吧。 每日一笔结束。 2020年02月18日 14:22:25 中午的时候去图书馆坐了一会儿，就是上次那个位置。 上次去过一次之后就爱上了，视野很开阔，能看到很不错风景。 今天中午的时候又飘了一会儿雪花，景色真的很不错。 要走的时候回头看到一个人趴在桌子上，和你很像。 路过的时候仔细看了看，趴在桌子上也看不到正面。 但是衣服，裤子，鞋都很像。 也许是我看错了，我想世界上应该没有两个人，穿衣打扮都这么相似的吧。 而且这个时候你应该还在做实验。之后才想到，其实应该看看袜子像不像的吧。 后来还是默默的走开了，回去的路上在想，是不是应该买一杯咖啡放在桌子上。 但是又觉得太自作多情了，这种剧情应该只存在于电影故事之中吧。 昨天买了布鲁斯口琴，明天就能到啦！ 加油生活呀！</summary></entry><entry><title type="html">TensorFlow安装</title><link href="http://localhost:4000/2020/02/09/TensorFlow%E5%AE%89%E8%A3%85.html" rel="alternate" type="text/html" title="TensorFlow安装" /><published>2020-02-09T00:00:00+09:00</published><updated>2020-02-09T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/09/TensorFlow%E5%AE%89%E8%A3%85</id><content type="html" xml:base="http://localhost:4000/2020/02/09/TensorFlow%E5%AE%89%E8%A3%85.html">&lt;p class=&quot;warning&quot;&gt;安装手顺适用于Ubuntu&lt;/p&gt;
&lt;h2 id=&quot;检查版本号&quot;&gt;检查版本号&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python3 --version
pip3 --version
virtualenv --version
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;如果没有的话，安装&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt update
sudo apt install python3-dev python3-pip
sudo pip3 install -U virtualenv  # system-wide install
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;安装virtual-environment&quot;&gt;安装virtual environment&lt;/h2&gt;
&lt;p&gt;这一步为了给TensorFlow创建一个单独的运行环境。推荐。&lt;/p&gt;
&lt;h3 id=&quot;创建新的虚拟环境&quot;&gt;创建新的虚拟环境&lt;/h3&gt;
&lt;p&gt;存放在./venv文件下。用python当作解释程序&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;virtualenv --system-site-packages -p python3 ./venv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;启动虚拟环境&quot;&gt;启动虚拟环境&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;source ./venv/bin/activate  # sh, bash, ksh, or zsh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;如果启动成功，在命令符前会出现(venv)&lt;/p&gt;
&lt;h3 id=&quot;升级pip&quot;&gt;升级pip&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install --upgrade pip

pip list  # show packages installed within the virtual environment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;安装tensorflow-pip-packag&quot;&gt;安装TensorFlow pip packag&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install --upgrade tensorflow
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;确认安装成功&quot;&gt;确认安装成功&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python -c &quot;import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;安装结束可以开始使用啦&quot;&gt;安装结束，可以开始使用啦！&lt;/h2&gt;

&lt;h2 id=&quot;退出venv&quot;&gt;退出venv&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;deactivate  # don't exit until you're done using TensorFlow
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;在运行TensorFlow的时候，应该一直使用虚拟环境&lt;/p&gt;

&lt;h2 id=&quot;在vscode中运行&quot;&gt;在vscode中运行&lt;/h2&gt;
&lt;h3 id=&quot;需要配置运行环境settingsjson&quot;&gt;需要配置运行环境，settings.json&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
    &quot;python.pythonPath&quot;: &quot;/home/chenkaixu/venv/bin/python3.6&quot;,
    &quot;python.venvPath&quot;: &quot;/home/chenkaixu/venv&quot;,

    &quot;python.linting.pylintPath&quot;: &quot;pylint&quot;,
    &quot;python.formatting.autopep8Path&quot;: &quot;autopep8&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="ComputerVision" /><category term="TensorFlow" /><summary type="html">安装手顺适用于Ubuntu 检查版本号 python3 --version pip3 --version virtualenv --version 如果没有的话，安装 sudo apt update sudo apt install python3-dev python3-pip sudo pip3 install -U virtualenv # system-wide install 安装virtual environment 这一步为了给TensorFlow创建一个单独的运行环境。推荐。 创建新的虚拟环境 存放在./venv文件下。用python当作解释程序 virtualenv --system-site-packages -p python3 ./venv 启动虚拟环境 source ./venv/bin/activate # sh, bash, ksh, or zsh 如果启动成功，在命令符前会出现(venv) 升级pip pip install --upgrade pip pip list # show packages installed within the virtual environment 安装TensorFlow pip packag pip install --upgrade tensorflow 确认安装成功 python -c &quot;import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))&quot; 安装结束，可以开始使用啦！ 退出venv deactivate # don't exit until you're done using TensorFlow 在运行TensorFlow的时候，应该一直使用虚拟环境 在vscode中运行 需要配置运行环境，settings.json { &quot;python.pythonPath&quot;: &quot;/home/chenkaixu/venv/bin/python3.6&quot;, &quot;python.venvPath&quot;: &quot;/home/chenkaixu/venv&quot;, &quot;python.linting.pylintPath&quot;: &quot;pylint&quot;, &quot;python.formatting.autopep8Path&quot;: &quot;autopep8&quot; }</summary></entry><entry><title type="html">TensorFlow快速开始</title><link href="http://localhost:4000/2020/02/09/TensorFlow%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B.html" rel="alternate" type="text/html" title="TensorFlow快速开始" /><published>2020-02-09T00:00:00+09:00</published><updated>2020-02-09T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/09/TensorFlow%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B</id><content type="html" xml:base="http://localhost:4000/2020/02/09/TensorFlow%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B.html">&lt;p class=&quot;info&quot;&gt;本篇教程来自TensorFlow官网&lt;a href=&quot;https://www.tensorflow.org/tutorials/quickstart/beginner&quot;&gt;初学者的 TensorFlow 2.0 教程&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;使用MNIST数据集&lt;/p&gt;
&lt;h2 id=&quot;下载并安装tensorflow&quot;&gt;下载并安装TensorFlow&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from __future__ import absolute_import, division, print_function, unicode_literals

# 安装 TensorFlow

import tensorflow as tf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;载入mnist数据集&quot;&gt;载入MNIST数据集&lt;/h2&gt;
&lt;p&gt;将样本从整数转换为浮点数&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;将模型的各层堆叠起来&quot;&gt;将模型的各层堆叠起来&lt;/h2&gt;
&lt;p&gt;搭建&lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/Sequential&quot;&gt;tf.keras.Sequential&lt;/a&gt;模型，为训练选择优化器和损失函数&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'softmax'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'adam'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'sparse_categorical_crossentropy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;训练并验证模型&quot;&gt;训练并验证模型&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model.fit(x_train, y_train, epochs=5)

model.evaluate(x_test,  y_test, verbose=2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;结束&quot;&gt;结束&lt;/h2&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="TensorFlow" /><category term="ComputerVision" /><summary type="html">本篇教程来自TensorFlow官网初学者的 TensorFlow 2.0 教程 使用MNIST数据集 下载并安装TensorFlow from __future__ import absolute_import, division, print_function, unicode_literals # 安装 TensorFlow import tensorflow as tf 载入MNIST数据集 将样本从整数转换为浮点数 mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 将模型的各层堆叠起来 搭建tf.keras.Sequential模型，为训练选择优化器和损失函数 model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10, activation='softmax') ]) model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) 训练并验证模型 model.fit(x_train, y_train, epochs=5) model.evaluate(x_test, y_test, verbose=2) 结束</summary></entry><entry><title type="html">Peng</title><link href="http://localhost:4000/2020/02/07/%E5%BD%AD%E5%AF%92%E8%96%87.html" rel="alternate" type="text/html" title="Peng" /><published>2020-02-07T00:00:00+09:00</published><updated>2020-02-07T00:00:00+09:00</updated><id>http://localhost:4000/2020/02/07/%E5%BD%AD%E5%AF%92%E8%96%87</id><content type="html" xml:base="http://localhost:4000/2020/02/07/%E5%BD%AD%E5%AF%92%E8%96%87.html">&lt;h2 id=&quot;2020年02月08日&quot;&gt;2020年02月08日&lt;/h2&gt;

&lt;p&gt;我想，我在写这篇文章的时候，内心一定很痛苦吧。&lt;br /&gt;
也许早就没了感觉，也许感觉受到了欺骗。&lt;br /&gt;
也许一切就已经都过去了吧。&lt;/p&gt;

&lt;p&gt;就不知道说什么好，心中明明有很多话想说，但是又觉得自己可笑。&lt;/p&gt;

&lt;p&gt;我真的就是那种及其容易陷入自我矛盾中的人，这样称之为倔强也不为过。&lt;br /&gt;
这段感情，真的是充满了坎坷。&lt;br /&gt;
我自认为自从复合以来，我各方面虽然不是特别完美，但都做得面面俱到。&lt;br /&gt;
你想要什么，想做什么，我都给你。把我最好的东西全都给你。&lt;/p&gt;

&lt;p&gt;但为什么最后还是要分手。&lt;br /&gt;
真的就想不明白。&lt;/p&gt;

&lt;p&gt;仿佛就是为你付出了太多，已经收不回来了。&lt;/p&gt;

&lt;h2 id=&quot;2020年02月09日-160432&quot;&gt;2020年02月09日 16:04:32&lt;/h2&gt;
&lt;p&gt;今天想了想，昨天和大黄说了那么多还挺没意思的。&lt;br /&gt;
自己到底在干啥呢，说实话自己怎么想的，他们根本不在乎啊。&lt;br /&gt;
因为这些人都是她的朋友，不是我的。就算我说的多有道理，多可怜，最后也不能站在我这边啊。&lt;br /&gt;
这段感情最可怕的就是，无人可说。说太多了就是我的错，不说吧自己憋得慌。&lt;br /&gt;
挺委屈的其实，我做人能做到这种地步。&lt;/p&gt;

&lt;p&gt;昨天晚上睡觉的时候，还想到了以前。😔&lt;/p&gt;

&lt;p&gt;和你在一起的时候，想成为一个有趣的人。&lt;br /&gt;
因为我觉得你足够有趣，是个有趣的人。&lt;br /&gt;
没想到最后自己成了一个可悲的人。&lt;/p&gt;

&lt;p&gt;现在都不知道在食堂的时候，你和我说分手，我是怎么笑的出来的。&lt;/p&gt;

&lt;p&gt;这个时候又想到了，当初答应你的好多东西都没给你。我的键盘，手柄，相机，ipad等等。&lt;br /&gt;
你想拿去就拿去，本来是准备搬到山下之后都给你的。&lt;br /&gt;
没想到分手来的这么突然。&lt;/p&gt;

&lt;p&gt;不过这次我觉得我足够珍惜你呀，可为什么就分手了呢。
这也是我到现在还一脸蒙蔽的地方呀。&lt;/p&gt;

&lt;h2 id=&quot;2020年02月11日-232604&quot;&gt;2020年02月11日 23:26:04&lt;/h2&gt;
&lt;p&gt;今天拿回了我的东西，你还做了一件让我很害怕和抓狂的事情。&lt;/p&gt;

&lt;p&gt;我是真的怕了，鬼知道我都经历了些什么东西。&lt;br /&gt;
本来以前虽然恐惧结婚，但挺期待爱情的。&lt;br /&gt;
张旭给我的会议都很美好，也许以前我们之间有过不愉快的事情，但是真的那一段感情体验及其良好。&lt;br /&gt;
我一直相信爱情里有很多美好的东西，以前也一直是这样的。经过这次，我是真的害怕，真的怕了。
感觉我就是个小白，被你玩来玩去的，你怎么开心怎么来，从来不顾及我的感受。
果然白给的不能随便要么，我也没这资格玩来玩去的。本来就和别人不一样，还非要享受温柔乡。太可怕了，太可怕了。&lt;/p&gt;

&lt;p&gt;不过现在倒是平静得很，内心没有一丝波澜。
感觉是你的常规操作。&lt;/p&gt;

&lt;p&gt;其实我记性是真的差，现在满脑子都还想着你对我的好，也不怪你也不恨你。&lt;br /&gt;
但我会想你对我的种种事情，真的记不起来了，就记着你对我挺好的，真的挺好的。人总是会忘掉不好的东西，记着好的东西。这也是我一直死皮赖脸追着你走的原因，总想着再努力一点，再努力一点你就可以对我好了。这个信念一直推着我向前走。&lt;br /&gt;
但是，你说你给你前男友寄口罩，你说戴来找你，我有多伤心，多难过，我没和你说过。因为我不想让你觉得是我限制了你的生活，限制了你的自由。导致你越来越变本加厉，和我在一起就是玩手机，和别人聊天，说拉黑就拉黑，想不理就不理，从来不顾及我的感受。&lt;br /&gt;
你说你对我好的时候我就对你不好了，所以你要对我一直很差。但是从我们复合以来，我从来没有对你很差过，我自己虽然做的不是十全十美，但我自认对你负责，也对自己负责。对你很好，把我的一切都给你。之前我们在一起的时候，我不能全身心的投入到这段感情中，因为我还没有缓过来。但是自从我们复合以来，我一直都特别珍惜你在乎你。也许在你看来远远不够，但你应该能看的出来我有很努力的和你在一起，想要变成你喜欢的人，能符合的上你的标准。&lt;/p&gt;

&lt;p&gt;本来觉得你是一个有趣的人，和你在一起我也一定可以变成一个有趣的人。可是没想到我最后都不知道要干什么了，干什么都怕你生气，怕你不高兴。然后就变得唯唯诺诺的，你说什么就干什么。其实我心里倒是挺高兴的，但是总感觉没有了灵魂。&lt;/p&gt;

&lt;p&gt;但是现在回头看看这段感情，我只看到了我一直跟着你身后追着你跑。一直跑一直跑，你偶尔回头看看我，更多的时候说走就走。也许你之前受了伤，所以到我身上就不愿意付出什么了，就像死心了一样。你说你以前从不这样谈恋爱，你可以好好的和他在一起。但是你却天天和我闹，和我吵，欺负我。说真的，我觉得不委屈。对你，我愿意付出，就算没有收到回报。&lt;/p&gt;

&lt;p&gt;今天你给我寄口罩的操作真的是震惊我了。&lt;br /&gt;
我不是不愿意给你，也不是和你要钱。我要是真的心疼钱，真的抠门，那么多钱我都花了为什么还不和你要。我就是震惊你说过我把口罩的事情给你解决了，你就不和我闹。我问遍了所有朋友，最后终于凑到了一些给你寄过去，没想到你说走就走，直接就拉黑。我实在是想不明白。&lt;/p&gt;

&lt;p&gt;也许你觉得这都是应该干的，想拉黑就拉黑，想干什么就干什么。但是你我都是成年人，生活中哪有这么多的事情可以肆意妄为的啊。你把我这边搞得一团糟，说走就走。对你是没什么影响，但是留下一堆烂摊子我需要给你收拾啊。你说你朋友换来换去的，但是你没有想过为什么吗。&lt;/p&gt;

&lt;p&gt;以前你说我对你不好，别人也看着我对你不好。我都承认，因为我心根本就不在你这里。之后意识到你的好了，我也后悔了。我拼命的想追回你，所以不管你对我怎么过分，我都不在乎。最后你终于答应我了，和我和好了。我全身心的对你好，但是仿佛是报复我一般，这次我能感受得到你对我一点都不好，你一点都不在乎我。&lt;/p&gt;

&lt;p&gt;和我分手其实很简单，我也不是一个喜欢纠缠不休的人。之前五年的感情，我说分手就分手。我一直都觉得一个人也可以。有人一起走是幸运，没有人一起走就享受孤独。这是我之前一直和你说过的话，我也一直坚信着这一点。但是经过这一次，我真的认识到了干净利落的重要性，永远不要拖泥带水。&lt;/p&gt;

&lt;p&gt;前任就像粑粑，在地上时间长了就干了，像一块巧克力。走过去看了看挺香的，拿起来尝了尝之后，还是变不成巧克力的啊！&lt;br /&gt;
和你在一起，我极度缺乏安全感。你从来没有给我让我安心的感觉。当着我的面和别的男生聊天，和你前男友纠缠不清，和喜欢你的人天天在聊天。你说这样的行为让我怎么能放得下心来。&lt;br /&gt;
我每天过得担惊受怕的，生怕那一天你被别人抢走了。你太爱玩了。我之前还相信你以前一直自闭从来不交朋友。但是你和我在一起之后，我看到你和谁都认识，和谁都能聊得来。我是真的害怕，仿佛我需要抢着和你聊天才行。如果我不和你聊天，你就和别人聊天。给我的感觉就是，和我聊不聊天其实都不重要。我一直都是这样的感觉。&lt;br /&gt;
你说过你想做改变，这样的改变不能说是不好的。但是，什么东西都要有一定的程度。我的感觉，在你心里，你的朋友第一位，前男友第二位，我能不能排到第三位都不好说。我多希望你能多在乎我一点。&lt;/p&gt;

&lt;p&gt;我还记得你说过你那个时候和真实哭，其实不是因为我找你你有多害怕，而是因为真实说了你两句不站在你这一边你觉得很委屈。但是你知道你这一哭，别人就觉得是我干了很过分的事情欺负了你，你受了委屈才哭的那么伤心。我的名声从那个时候开始，就变得一直走下坡路了。之后我又把真实骂了，其实一直挺想道歉的。但是真实删掉我好友之后我却看到你在笑，仿佛挺高兴的样子。说实话我和真实平时联系本来不多，能和她产生交集也是因为你的原因。最开始觉得为了你得罪再多人也没关系，就算打工的一半你说你难受，我翘掉打工赶紧骑车上山看你。为你付出再多，我一开始真的信心满满的。但是渐渐的却失去了该有的勇气。&lt;/p&gt;

&lt;p&gt;回头看看我们的感情，我看到了一片荒凉。心早就觉得不痛不痒了，就算你说再过分的话，做的再过分，再怎么伤害我，我都觉得没什么感觉了。&lt;br /&gt;
说实话我现在都觉得你对我挺好的，都觉得我们之间充满了美好的回忆。但是却不想回头看看，因为我不敢。不敢看到自己的心变得越来越破碎，看到自己变得越来越脆弱了起来。&lt;/p&gt;

&lt;p&gt;我终究是喜欢你的，也终究是狠不下心的一个人。&lt;br /&gt;
这就是我，真实的我。&lt;/p&gt;

&lt;h2 id=&quot;2020年02月12日-174710&quot;&gt;2020年02月12日 17:47:10&lt;/h2&gt;
&lt;p&gt;刚才忽然想起。&lt;/p&gt;

&lt;p&gt;以前你说分手的时候怎么对你前男友。所有东西都直接拉黑，电话也打不通。那个时候刚听到我还真的心疼了一下你前男友，没想到这么快的速度就轮到我了。&lt;/p&gt;

&lt;h2 id=&quot;2020年02月16日-164954&quot;&gt;2020年02月16日 16:49:54&lt;/h2&gt;
&lt;p&gt;给你准备的生日，也差不多了。&lt;/p&gt;

&lt;p&gt;为了爱你我损失了太多东西，也丢掉了太多东西。&lt;br /&gt;
为了你没有给自己留一点后路，把朋友的关系弄得很尴尬，也把无关的人得罪了。&lt;br /&gt;
在这段感情中的付出就像自杀式的一样。为了你我什么都不要了，本来开局的时候一手好牌，现在被我打的细碎。&lt;/p&gt;

&lt;p&gt;现在回头看来，我一路走来，得到的只有伤心和荒芜。&lt;/p&gt;

&lt;p&gt;仿佛这样对你，就是对以前的自己赎罪一样。我一直对自己说，是因为以前事情的报复，才走到现在这样的结果的。&lt;br /&gt;
但我回头仔细想，我一直都付出了真感情，我不觉得我应该遭受这样的对待和报复。&lt;br /&gt;
你也不应该把从前男友收到的伤害，转嫁到我头上来。&lt;br /&gt;
昨天听你说你情人节亲手做的巧克力，给你朋友都送了一遍，就是没有给我准备。想我还在超市细心挑选了半天，最后买了两盒觉得你会喜欢的巧克力送给你。&lt;br /&gt;
听了之后觉得自己特别滑稽，就像这次过生日一样，真的感觉自己很滑稽。大家都在看我的笑话，你也在看，只有我自己不自知，还满怀期待着准备着一些什么东西。 &lt;br /&gt;
结果就是自己打自己的连，碰的一鼻子灰而已。&lt;/p&gt;

&lt;p&gt;这次我终于学会了应该心疼自己才对。&lt;/p&gt;

&lt;h2 id=&quot;2020年02月17日-164957&quot;&gt;2020年02月17日 16:49:57&lt;/h2&gt;
&lt;p&gt;突然想到，我对你的担心，顾虑，不放心以及一直找你。其实是我自己自卑的一种体现。&lt;br /&gt;
不知道从什么时候开始，以前的我就一直有这样的感觉。&lt;br /&gt;
总想把对方的行动掌握在自己的预料之内。&lt;/p&gt;

&lt;p&gt;所以才会太过在意，也太让自己心慌了。&lt;br /&gt;
这样对双方都不好其实，搞得大家都很累。我没了自己的生活，你也没了自己的空间。&lt;br /&gt;
我心里也知道这样不好，但有的时候就是没办法好好控制自己的情绪。&lt;br /&gt;
其实我心里明白，像这样越抓越紧也只会越快的失去你。我本来也不是这样的人，但一直都不能很好的处理这份距离感。&lt;/p&gt;

&lt;p&gt;以前的时候，有抓过紧的时候。那种感觉很让人窒息，最后当然没有什么好结果。当然也有完全放任不管的时候，但这样就会说我什么都不关心。&lt;br /&gt;
距离感真的很重要，我也真的很难掌握一个健康的距离感。我是一个在生活上很独立的人，心里其实并不是很喜欢过多干涉另一半的生活。但是上一段感情的经历真的让我怕了。我只能越来越多的管你，干涉你的生活。&lt;br /&gt;
我一旦陷入这样的境地中，真的就非常容易胡思乱想。总是给自己设置一些有的没的“敌人”，感觉身边处处都存在着威胁。但其实大家生活这么忙，哪有时间去干我想象中的那些事情。&lt;br /&gt;
就因为我的想法不受控制，最后我们双方都感觉很累。&lt;/p&gt;

&lt;p&gt;就像你的前男友和戴，我知道你们之间没啥，而且你也没有和我主动提起过几次。&lt;br /&gt;
但是我自己心里不愿意放过自己，总是惦记着有这么两个人。你也发现，我和你说话总是会不经意的提起他们，但是我其实根本不care一些细微末节的东西的。更多的东西全是我自己幻想出来的。时间长了就变得疑神疑鬼的，变得想一直找你，了解你在干什么。这也是我没有安全感的一种体现，但更多的问题还是我对自己的不自信，对自己魅力以及人生的不自信。有的时候，我真的自卑到了极点。&lt;/p&gt;

&lt;p&gt;所以我选择运动，选择一个健康的生活方式。&lt;br /&gt;
无关自身条件，只和心态有关系。我一直认为运动可以让我变得充满自信，也可以让我充满吸引力。&lt;br /&gt;
事实上也正是如此，我应该一直坚持运动才对。&lt;/p&gt;

&lt;p&gt;有的时候，我呀，就是想太多，做太少。&lt;br /&gt;
这样不好，很不好。&lt;/p&gt;

&lt;p&gt;记录下来，未来共勉。&lt;/p&gt;</content><author><name>chenkaixu</name><email>chenkaixusan@gmail.com</email></author><category term="感情记录" /><summary type="html">2020年02月08日 我想，我在写这篇文章的时候，内心一定很痛苦吧。 也许早就没了感觉，也许感觉受到了欺骗。 也许一切就已经都过去了吧。 就不知道说什么好，心中明明有很多话想说，但是又觉得自己可笑。 我真的就是那种及其容易陷入自我矛盾中的人，这样称之为倔强也不为过。 这段感情，真的是充满了坎坷。 我自认为自从复合以来，我各方面虽然不是特别完美，但都做得面面俱到。 你想要什么，想做什么，我都给你。把我最好的东西全都给你。 但为什么最后还是要分手。 真的就想不明白。 仿佛就是为你付出了太多，已经收不回来了。 2020年02月09日 16:04:32 今天想了想，昨天和大黄说了那么多还挺没意思的。 自己到底在干啥呢，说实话自己怎么想的，他们根本不在乎啊。 因为这些人都是她的朋友，不是我的。就算我说的多有道理，多可怜，最后也不能站在我这边啊。 这段感情最可怕的就是，无人可说。说太多了就是我的错，不说吧自己憋得慌。 挺委屈的其实，我做人能做到这种地步。 昨天晚上睡觉的时候，还想到了以前。😔 和你在一起的时候，想成为一个有趣的人。 因为我觉得你足够有趣，是个有趣的人。 没想到最后自己成了一个可悲的人。 现在都不知道在食堂的时候，你和我说分手，我是怎么笑的出来的。 这个时候又想到了，当初答应你的好多东西都没给你。我的键盘，手柄，相机，ipad等等。 你想拿去就拿去，本来是准备搬到山下之后都给你的。 没想到分手来的这么突然。 不过这次我觉得我足够珍惜你呀，可为什么就分手了呢。 这也是我到现在还一脸蒙蔽的地方呀。 2020年02月11日 23:26:04 今天拿回了我的东西，你还做了一件让我很害怕和抓狂的事情。 我是真的怕了，鬼知道我都经历了些什么东西。 本来以前虽然恐惧结婚，但挺期待爱情的。 张旭给我的会议都很美好，也许以前我们之间有过不愉快的事情，但是真的那一段感情体验及其良好。 我一直相信爱情里有很多美好的东西，以前也一直是这样的。经过这次，我是真的害怕，真的怕了。 感觉我就是个小白，被你玩来玩去的，你怎么开心怎么来，从来不顾及我的感受。 果然白给的不能随便要么，我也没这资格玩来玩去的。本来就和别人不一样，还非要享受温柔乡。太可怕了，太可怕了。 不过现在倒是平静得很，内心没有一丝波澜。 感觉是你的常规操作。 其实我记性是真的差，现在满脑子都还想着你对我的好，也不怪你也不恨你。 但我会想你对我的种种事情，真的记不起来了，就记着你对我挺好的，真的挺好的。人总是会忘掉不好的东西，记着好的东西。这也是我一直死皮赖脸追着你走的原因，总想着再努力一点，再努力一点你就可以对我好了。这个信念一直推着我向前走。 但是，你说你给你前男友寄口罩，你说戴来找你，我有多伤心，多难过，我没和你说过。因为我不想让你觉得是我限制了你的生活，限制了你的自由。导致你越来越变本加厉，和我在一起就是玩手机，和别人聊天，说拉黑就拉黑，想不理就不理，从来不顾及我的感受。 你说你对我好的时候我就对你不好了，所以你要对我一直很差。但是从我们复合以来，我从来没有对你很差过，我自己虽然做的不是十全十美，但我自认对你负责，也对自己负责。对你很好，把我的一切都给你。之前我们在一起的时候，我不能全身心的投入到这段感情中，因为我还没有缓过来。但是自从我们复合以来，我一直都特别珍惜你在乎你。也许在你看来远远不够，但你应该能看的出来我有很努力的和你在一起，想要变成你喜欢的人，能符合的上你的标准。 本来觉得你是一个有趣的人，和你在一起我也一定可以变成一个有趣的人。可是没想到我最后都不知道要干什么了，干什么都怕你生气，怕你不高兴。然后就变得唯唯诺诺的，你说什么就干什么。其实我心里倒是挺高兴的，但是总感觉没有了灵魂。 但是现在回头看看这段感情，我只看到了我一直跟着你身后追着你跑。一直跑一直跑，你偶尔回头看看我，更多的时候说走就走。也许你之前受了伤，所以到我身上就不愿意付出什么了，就像死心了一样。你说你以前从不这样谈恋爱，你可以好好的和他在一起。但是你却天天和我闹，和我吵，欺负我。说真的，我觉得不委屈。对你，我愿意付出，就算没有收到回报。 今天你给我寄口罩的操作真的是震惊我了。 我不是不愿意给你，也不是和你要钱。我要是真的心疼钱，真的抠门，那么多钱我都花了为什么还不和你要。我就是震惊你说过我把口罩的事情给你解决了，你就不和我闹。我问遍了所有朋友，最后终于凑到了一些给你寄过去，没想到你说走就走，直接就拉黑。我实在是想不明白。 也许你觉得这都是应该干的，想拉黑就拉黑，想干什么就干什么。但是你我都是成年人，生活中哪有这么多的事情可以肆意妄为的啊。你把我这边搞得一团糟，说走就走。对你是没什么影响，但是留下一堆烂摊子我需要给你收拾啊。你说你朋友换来换去的，但是你没有想过为什么吗。 以前你说我对你不好，别人也看着我对你不好。我都承认，因为我心根本就不在你这里。之后意识到你的好了，我也后悔了。我拼命的想追回你，所以不管你对我怎么过分，我都不在乎。最后你终于答应我了，和我和好了。我全身心的对你好，但是仿佛是报复我一般，这次我能感受得到你对我一点都不好，你一点都不在乎我。 和我分手其实很简单，我也不是一个喜欢纠缠不休的人。之前五年的感情，我说分手就分手。我一直都觉得一个人也可以。有人一起走是幸运，没有人一起走就享受孤独。这是我之前一直和你说过的话，我也一直坚信着这一点。但是经过这一次，我真的认识到了干净利落的重要性，永远不要拖泥带水。 前任就像粑粑，在地上时间长了就干了，像一块巧克力。走过去看了看挺香的，拿起来尝了尝之后，还是变不成巧克力的啊！ 和你在一起，我极度缺乏安全感。你从来没有给我让我安心的感觉。当着我的面和别的男生聊天，和你前男友纠缠不清，和喜欢你的人天天在聊天。你说这样的行为让我怎么能放得下心来。 我每天过得担惊受怕的，生怕那一天你被别人抢走了。你太爱玩了。我之前还相信你以前一直自闭从来不交朋友。但是你和我在一起之后，我看到你和谁都认识，和谁都能聊得来。我是真的害怕，仿佛我需要抢着和你聊天才行。如果我不和你聊天，你就和别人聊天。给我的感觉就是，和我聊不聊天其实都不重要。我一直都是这样的感觉。 你说过你想做改变，这样的改变不能说是不好的。但是，什么东西都要有一定的程度。我的感觉，在你心里，你的朋友第一位，前男友第二位，我能不能排到第三位都不好说。我多希望你能多在乎我一点。 我还记得你说过你那个时候和真实哭，其实不是因为我找你你有多害怕，而是因为真实说了你两句不站在你这一边你觉得很委屈。但是你知道你这一哭，别人就觉得是我干了很过分的事情欺负了你，你受了委屈才哭的那么伤心。我的名声从那个时候开始，就变得一直走下坡路了。之后我又把真实骂了，其实一直挺想道歉的。但是真实删掉我好友之后我却看到你在笑，仿佛挺高兴的样子。说实话我和真实平时联系本来不多，能和她产生交集也是因为你的原因。最开始觉得为了你得罪再多人也没关系，就算打工的一半你说你难受，我翘掉打工赶紧骑车上山看你。为你付出再多，我一开始真的信心满满的。但是渐渐的却失去了该有的勇气。 回头看看我们的感情，我看到了一片荒凉。心早就觉得不痛不痒了，就算你说再过分的话，做的再过分，再怎么伤害我，我都觉得没什么感觉了。 说实话我现在都觉得你对我挺好的，都觉得我们之间充满了美好的回忆。但是却不想回头看看，因为我不敢。不敢看到自己的心变得越来越破碎，看到自己变得越来越脆弱了起来。 我终究是喜欢你的，也终究是狠不下心的一个人。 这就是我，真实的我。 2020年02月12日 17:47:10 刚才忽然想起。 以前你说分手的时候怎么对你前男友。所有东西都直接拉黑，电话也打不通。那个时候刚听到我还真的心疼了一下你前男友，没想到这么快的速度就轮到我了。 2020年02月16日 16:49:54 给你准备的生日，也差不多了。 为了爱你我损失了太多东西，也丢掉了太多东西。 为了你没有给自己留一点后路，把朋友的关系弄得很尴尬，也把无关的人得罪了。 在这段感情中的付出就像自杀式的一样。为了你我什么都不要了，本来开局的时候一手好牌，现在被我打的细碎。 现在回头看来，我一路走来，得到的只有伤心和荒芜。 仿佛这样对你，就是对以前的自己赎罪一样。我一直对自己说，是因为以前事情的报复，才走到现在这样的结果的。 但我回头仔细想，我一直都付出了真感情，我不觉得我应该遭受这样的对待和报复。 你也不应该把从前男友收到的伤害，转嫁到我头上来。 昨天听你说你情人节亲手做的巧克力，给你朋友都送了一遍，就是没有给我准备。想我还在超市细心挑选了半天，最后买了两盒觉得你会喜欢的巧克力送给你。 听了之后觉得自己特别滑稽，就像这次过生日一样，真的感觉自己很滑稽。大家都在看我的笑话，你也在看，只有我自己不自知，还满怀期待着准备着一些什么东西。 结果就是自己打自己的连，碰的一鼻子灰而已。 这次我终于学会了应该心疼自己才对。 2020年02月17日 16:49:57 突然想到，我对你的担心，顾虑，不放心以及一直找你。其实是我自己自卑的一种体现。 不知道从什么时候开始，以前的我就一直有这样的感觉。 总想把对方的行动掌握在自己的预料之内。 所以才会太过在意，也太让自己心慌了。 这样对双方都不好其实，搞得大家都很累。我没了自己的生活，你也没了自己的空间。 我心里也知道这样不好，但有的时候就是没办法好好控制自己的情绪。 其实我心里明白，像这样越抓越紧也只会越快的失去你。我本来也不是这样的人，但一直都不能很好的处理这份距离感。 以前的时候，有抓过紧的时候。那种感觉很让人窒息，最后当然没有什么好结果。当然也有完全放任不管的时候，但这样就会说我什么都不关心。 距离感真的很重要，我也真的很难掌握一个健康的距离感。我是一个在生活上很独立的人，心里其实并不是很喜欢过多干涉另一半的生活。但是上一段感情的经历真的让我怕了。我只能越来越多的管你，干涉你的生活。 我一旦陷入这样的境地中，真的就非常容易胡思乱想。总是给自己设置一些有的没的“敌人”，感觉身边处处都存在着威胁。但其实大家生活这么忙，哪有时间去干我想象中的那些事情。 就因为我的想法不受控制，最后我们双方都感觉很累。 就像你的前男友和戴，我知道你们之间没啥，而且你也没有和我主动提起过几次。 但是我自己心里不愿意放过自己，总是惦记着有这么两个人。你也发现，我和你说话总是会不经意的提起他们，但是我其实根本不care一些细微末节的东西的。更多的东西全是我自己幻想出来的。时间长了就变得疑神疑鬼的，变得想一直找你，了解你在干什么。这也是我没有安全感的一种体现，但更多的问题还是我对自己的不自信，对自己魅力以及人生的不自信。有的时候，我真的自卑到了极点。 所以我选择运动，选择一个健康的生活方式。 无关自身条件，只和心态有关系。我一直认为运动可以让我变得充满自信，也可以让我充满吸引力。 事实上也正是如此，我应该一直坚持运动才对。 有的时候，我呀，就是想太多，做太少。 这样不好，很不好。 记录下来，未来共勉。</summary></entry></feed>
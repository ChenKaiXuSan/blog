---
title: 深度学习入门
tag:  ComputerVision
---

# 神经网络的数据表示

## 张量（tensor）
使用张量作为基本数据结构。

是一个数据容器。  
它包含的数据几乎总是数值数据，因此它是数字的容器。  
矩阵是二维张量。

张量是矩阵向任意维度的推广。
张量的 **维度（dimension)** 叫做 **轴（axis）**
- 标量（0D张量）  
仅包含一个数字的张量叫做 **标量(scalar)**
- 向量（1D张量）  
数字组成的数组叫做 **向量(vector)** 或 **一维张量（1D张量）**  
一维张量只有一个轴。

- 矩阵（2D张量）  
向量组成的数组叫做 **矩阵（matrix）** 或 **二维张量（2D张量）**  
矩阵有两个轴（行和列）

- 3D张量与更高维度的张量
将多个矩阵组合成一个新的数组，可以得到一个3D张量。  
直观的理解为数字组成的立方体。

高维度张量以此类推。

## 关键属性
张量由以下三个关键属性来定义。
- 轴的个数（阶）  
3D张量有3个轴，矩阵有2个轴。
- 形状  
张量沿每个轴的维度大小（元素个数）。
- 数据类型  
张量中所包含数据的类型。  

很多库中不存在字符串张量。  
{:.warning}

## 数据批量
数据张量的第一个轴（0轴）都是**样本轴（sample axis，样本维度）**。
深度学习模型不会同时处理整个数据集，而是将数据拆分成小批量。  
对于这种批量张量，第一个轴（0轴)叫做**批量轴（batch axis）**或**批量维度（batch dimension）**。

## 现实世界中的数据张量
- 向量数据：2D张量
- 时间序列数据 或 序列数据：3D张量
- 图像：4D张量
- 视频：5D张量

# 张量运算
## 逐元素运算
## 广播
两个形状不同的张量进行操作，会发生什么？

较小的张量会被**广播（broadcast）**，以匹配较大张量的形状。  

广播包含以下两个步骤：

1. 向较小的张量添加轴（叫做**广播轴**）。
2. 将较小的张量沿着新轴重复，使其形状与较大张量相同。

## 张量点积
点积运算，也叫**张量积（tensor product）**。  
它将输入张量的元素合并在一起。
```
z = np.dot(x,y)
```
数学符号中
```
z = x.y
```
两个向量之间的点积是一个标量，而且只有元素个数相同的向量之间才能做点积。
{:.warning}

## 张量变形（tensor reshaping）
张量变形是指改变张量的行和列，以得到想要的形状。

特殊的张量变形：转置（transposition）。

# 基于梯度的优化
## 训练循环（training loop)
1. 抽取训练样本x和对应目标y组成的数据批量
2. 在x上运行网络[前向传播（forward pass）]，得到预测值y_pred
3. 计算网络在这批数据上的损失，用于衡量y_pred和y之间的距离。
4. 更新网络的所有权重，使网络在这批数据上的损失略微下降。

对于第四步，更好的方法是利用网络中所有运算都是 **可微（differentiable）** 的，计算损失相对于网络系数的 **梯度（gradient）**，然后向梯度的反方向改变系数，从而使损失降低。

可微的意思是“可以被求导”  
{:.info}

## 张量运算的导数：梯度
## 随机梯度下降
### 小批量随机梯度下降（mini-batch stochastic gradient descent）
1. 抽取训练样本x和对应目标y组成的数据批量
2. 在x上运行网络，得到预测值y_pred
3. 计算网络在这批数据上的损失，用于衡量y_pred和y之间的距离。
4. 计算损失相对于网络参数的梯度【一次**反向传播（backward pass）】。
5. 将参数沿着梯度的反方向移动一点，从而使这批数据上的损失减小一点。

**随机（stochastic）**是指每批数据都是随机抽取的。
{:.info}

**真SGD**：每次迭代时只抽取一个样本和目标。  
**批量SGD**：每一次迭代都在所有数据上运行。这样每次更新都更加准确，但计算代价也高很多。

SGD还有很多种变体，带动量的SGD等变体，被称为**优化方法（optimization method）**或 **优化器（optimizer）**。

动量方法的实现过程是，每一步都移动小球，不仅要考虑当前的斜率值（当前的加速度），还要考虑当前的速度（来自于之前的加速度）。  
在神经网络中，更新参数w不仅要考虑当前的梯度值，还要考虑上一次的参数更新。

## 链式求导：反向传播算法
---
title: NIPS 2016 Tutorial:Generative Adversarial Networks
tag: [ComputerVision]
modify_date: 2020-09-20
---

## 摘要
这篇论文总结了在NIPS2016上作者提出的向导。  
这个向导主要包括：
1. 为什么生成模型值得研究
2. 生成模型怎么工作，以及GANs与其他生成模型的比较
3. GANs工作的一些细节
4. GAN相关的研究课题
5. 结合GAN与其他发放的最先进图像模型
最后，这篇向导为读者提供了3个练习，以及练习的答案。

# 介绍
这个报告总结了在2016NIPS大会上关于GANs的内容。为了确保这个向导对观众最有用，这个向导主要面向去确定在大会初期由观众提出的问题。这个向导不计划全面的介绍GANs的领域；很多出彩的论文没有在这里被提及，只是因为这些论文没有对频繁出现的问题作出解释，另外因为这个向导被设置为2个小时的口述演讲，也没有无限的时间覆盖所有的研究内容。

这个向导将介绍：
1. 为什么生成模型是一个值得研究的题目
2. 生成模型是如何工作的，以及GAN与其他生成模型的区别
3. GANs工作的细节
4. GANs的相关研究课题
5. 最先进的包含GANs和其他方法的图像模型
最后，这个向导包含3个练习给读者完成，同时提供了这3个练习的答案。

此向导的幻灯片可以查看在PDF和Keynote格式在下面的链接中：
http://www.iangoodfellow.com/slides/2016-12-04-NIPS.pdf 
http://www.iangoodfellow.com/slides/2016-12-04-NIPS.key

视频被NIPS基金会录制，可以在稍后进行观看。
https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Generative-Adversarial-Networks

生成对抗网络是生成模型的一个分类。生成模型在很多方面有应用。在这个向导中，这个模型指的是用某种方式去学习并表达针对一个训练数据集分布的估计，也就是$p_{data}$,然后学习描绘一个估计对于一些分布。结果是一个概率分布$p_{model}$。在一些例子中，模型明确的估计$p_{model}$，就像图一那样。在其他情况下，模型只可能从$p_{model}$中生成样本，如图2。一些模型可以两个都做。GANs主要关注在样本生成，尽管可以设计GANs都做两者。    

[//]: # (使用GitHub图片的网络链接，用相对链接不能显示图片不知道为什么。)
![Image](https://raw.githubusercontent.com/ChenKaiXuSan/blog/master/image/gan/201704/28/fig01.png)  
{:.border}

Figure1:有些生成模型通过密度估计。这些模型通过从未知的数据生成分布$p_{data}$来获取训练数据样本，然后得到此未知分布的估计。$p_{model}$估计可以给一个特殊的x值进行估计以获得对于真实密度$p_{model}(x)$的$p_{model}(x)$估计。这个图片展示了一个一维数据收集样本的过程，和一个高斯模型。

![Image](https://raw.githubusercontent.com/ChenKaiXuSan/blog/master/image/gan/201704/28/fig02.png)  
{:.border}

Figure2:一些生成模型可以从模型分布中生成样本。此图介绍的过程中，我们展示了ImageNet数据集中的样本。

[//]: # (这样的写法可以在GitHubpage上显示出来，但是在本地无法显示)
[//]: ![Image](/image/gan/201704/28/eq01.jpg)

# 为什么学习生成模型？
一个可能合理的解释对于这个问题，仅仅是因为生成模型可能比提供的密度函数估计有更好的表现。毕竟，当适用于图像时，模型只是尝试提供更多的图片，但是世界并不缺乏图片。  
这里有一些学习生成模型的原因，包括：  
- 从生成模型训练和采样是一个很好的测试对于我们的能力在表现和控制高纬度概率分布上。高纬度概率分布是一个重要的课题，在宽广的多样化的应用数学和工程领域。
- 生成模型可以在很多方面被应用到**增强学习中（reinforcement learning)**。增强学习算法可以分成两类；model-base和model-free，model-base算法包含一个生成模型。时间序列数据的生成模型可以被应用到模拟可能的未来。例如模型可以被应用到 **计划(planning)**，和在多样化方面的增强学习。一个生成模型应用到 **计划(planning)** 可以通过未来世界的状态学习到一个条件分布，给定当前世界的状态和一个agent可能采取的假设行动座位输入。agent可以询问模型用不同的潜在行为，然后选择行动倾向于产生与自己期望状态一致的模型的预测行为。最近一个模型的例子，Finn et al(2016b)，然后最近的一个例子应用于planning模型，Finn and Levine(2016)。另一方面生成模型可能应用于增强学习使他可以学习一个虚幻的环境，错误的行动不会对agent产生伤害。生成模型还可以用于引导探索保持足迹对于提前企图被观测的或不同的行动有多大的不同情况。生成模型，特别是GANs，还可以用于反向**增强学习(inverse reinforcement learning)**。关于增强学习的内容在5.6节会有更多的描述。
- 生成模型可以通过缺失数据训练，以及可以对缺失数据的输入提供预测。一个特别有趣的缺失数据的例子是**半监督学习(semi-supervised learning)**,对于很多训练样本的标签是缺失的。流行的深度学习算法典型的需要特别多的标签样例去概括的很好。半监督学习是一个策略对于降低标签数量。学习算法可以通过学习很多未标记的样本来提高推广能力，未标定的数据更容易的获得。生成模型，特别是GANs，可以让半监督学习表现的更好。在5.4节将进行更详细的描述。
- 生成模型，特别是GANs，使机器学习可以用于**多模(multi-modal)** 输出问题。对于很多任务来说，一个输入可能对应多个正确的输出，每一个输出都是可以接受的。一些传统的训练机器学习模型的平均，像对期望输出和预测输出的均方差进行最小化的方法，不可能训练模型像产生多个不同正确答案那样。一个里理想情况的例子是，预测下一个 **架构(fram)** 在一个视频中，就像图3展示的那样。  
![Image](https://raw.githubusercontent.com/ChenKaiXuSan/blog/master/image/gan/201704/28/fig03.png)  
{:.border}  
Figure3:*Lotter et al.(2015)* 提供了一个精彩的展示对于多模数据建模。在这个例子中，一个模型被训练去预测一个视频序列的下一帧。这个视频描画了一个计算机底灰的一个可移动的3D人类头部模型。左侧图像展示了真实的下一帧图像，这个模型将理想的预测图片。中间的图片展示了利用 **mean squared error(mse)** 方法训练的在真实的下一帧和模型预测的下一帧之间的可能发生的情况。模型被强迫的去选择一个单独的回答对于下一帧看起来像什么。因为有很多可能的未来，相应的头部有轻微的不同位置，单一回答这个模型选择了对于很多明显不同的平均值。这就导致了耳朵消失，眼睛变得模糊。使用一个额外的GAN loss，在右侧的图像就有很多输出变得容易理解，每个都是清晰的和可以认出的真实的图片。  
- 最后，很多任务本质上需要从一些分布上产生真实的生成数据。

这样的任务本质上需要生成优质的样本包括：
- 单图像超解析度（single image super-resolution):在这个任务中，目标是将低解析度图像合成为高解析度相等的图像。生成模型是必须的，因为这个任务需要模型加入更多信息到图片中比原始输入的图像。
![Image](https://raw.githubusercontent.com/ChenKaiXuSan/blog/master/image/gan/201704/28/fig04.png)  
{:.border}  
Figure4：*Ledig et al.(2016)* 展示了非常好的单张超分辨率图片的结果，这组结果展示了从多模分布(multimodal distribution)使用生成模型训练的生成真实样本的获益结果。最左边的图片是原本的高分辨率图片。然后通过downsampled去制作一个低分辨率的图片，然后使用不同的方法去试图恢复高分辨率图片。 **双三次插值(bicubic method)** 是一个简单的插值方法，不需要使用全部训练数据的统计。**SRResNet** 是用mse训练的神经络。**SRGAN** 是基于GAN的神经网络，SRGAN性能提升是因为他可以理解多个正确的回答，而不是对多种正确答案做平均化处理，从而中得到一个最好的输出结果。

这里有很多可能的高分辨率图像对应与低分辨率的。这个模型应该选择一个图像，从概率分布中选择可能的图像。选择一个从可能图像的平局值的图像将产生太过模糊的结果。详情看图4。

- 艺术创作任务。两个最近的任务都展示了生成模型，特别是GAN，可以用于创建交互式的程序帮助用户创建真实的图片对应于粗糙的场景在用户的想象中。查看图片5和图片6。

![Image](https://raw.githubusercontent.com/ChenKaiXuSan/blog/master/image/gan/201704/28/fig05.jpg)  
{:.border}  
Figure5: *Zhu et al.(2016)* 开发了一个交互的程序称为 **interactive generative adversarial networks(iGAN)**。用户可以给一个图片画一个粗略的框架，然后iGAN使用GAN去产生最相似的真实图片。在这个示范中，一个用户乱画了一些绿色的线，iGAN生成了长满草的区域，用户画了一个黑色的三角形，iGAN生成了一个细致的山。艺术创作的应用是一个生成模型生成图像的原因。一个影片介绍了iGAN可以从下面的链接中查看：
https://www.youtube.com/watch?v=9c4z6YsBGQ0

![Image](https://raw.githubusercontent.com/ChenKaiXuSan/blog/master/image/gan/201704/28/fig06.jpg)  
{:.border}  
FIgure6：*Brock et al(2016)* 开发了内省对抗网络 **introspective adversarial networks(IAN)**。用户对图像做简单的修改，比如在一个区域中涂成黑色，从而希望加上黑色的头发，然后IAN可以将这种简单的涂抹转换为用户期望的照相写实主义的照片。应用允许用户在照片上创建真实的改动，这是学习生成模型创建图像的一个原因。

- Image-to-Image转换应用可以将航空图像转换为地图或将素描转换为图片。还有很长的路对于创建应用，很难预料但是可以被发现。详情看图7。

![Image](https://raw.githubusercontent.com/ChenKaiXuSan/blog/master/image/gan/201704/28/fig07.jpg) 
{:.border}  
Figure7: *Isola et al.(2016)* 创建了一个叫做图像到图像转换的应用，包含很多图像种类的转换：将卫星图像转换为地图，将素描转换成照片写实主义的照片，等。因为很多这些转换有多个正确的输出对于每个输入，所以很重要的是使用生成模型去正确的训练模型。特别的，*Isola et al(2016)* 使用了GAN。图像到图像的转换提供了很多示例，一个创新的设计可以找到多种意料之外的应用对于生成模型。在未来，肯定很多创新应用会被发现。

所有这些以及其他生成模型的应用提供了足够多的理由去投入时间和资源到提高生成模型中。

# 生成模型是怎么工作的？GANs与其他模型有什么区别？
我们现在已经知道了生成模型可以做什么，以及值得去使用的原因。现在我们可以讨论：一个生成模型实际是如何工作的？特别的，GAN是如何工作的，以及与其他生成模型的区别？

## 最大似然估计(Maximum likelihood estimation)
为了简化讨论，我们将要集中在生成模型使用最大似然工作的方面上。不是每个生成模型都是用最大似然。一些生成模型默认不使用最大似然，但是可以是以哦那个。（GAN就是这一类）。通过忽略不使用最大似然的模型，集中在那些本来通常不使用最大似然的模型的最大似然版本，我们可以消除一些分散注意力的不同在不同模型之间。  

最大似然估计的基本理念是用来估计概率模型参数$\theta$的一种方法，定义一个模型并提供一个概率分布估计。之后使用最大似然作为概率，模型分配给训练的数据：$\prod_{i=1}^{m}p_{model}(x^{(i)};\theta)$，一个数据集包含$m$去训练样本$x^{(i)}$。

简单来说，最大似然估计为模型选择了一个参数，最大化训练数据的可能性。在对数空间(log space)最容易到底此目的，我们计算一个算数和，而不是对每个样本进行处理。通过使用算术和使得对应模型的似然的导数的数学表达变得简单， 并且当在数字计算机上计算时， 其对数值问题不敏感， 比如说，当对几个很小的概率值进行相乘计算时会出现下溢（underflow）问题。

![Image](https://raw.githubusercontent.com/ChenKaiXuSan/blog/master/image/gan/201704/28/eq01.jpg) 
{:.border}
在方程2中，我们使用了以下特性：$\arg\max_{v}f(v)=\arg\max_{v}\log f(v)$,对于正数$v$，对数函数可以对所有范围的值进行最大化，却不改变最大化的区域。

最大似然的处理过程可以参考图8。

![Image](https://raw.githubusercontent.com/ChenKaiXuSan/blog/master/image/gan/201704/28/fig08.jpg) 
{:.border}
Figure8: 最大似然处理过程包括： 首先根据数据生成的分布来采样一些样本来构建训练数据集，然后通过提高模型在这些数据上概率值，来最大化此训练数据上的似然。这张图片展示了一个针对一维数据的高斯模型，我们可以看到不同的数据点会拉高密度函数的不同的部分。由于这个密度函数的总和必须是1，所以不可能将所有点都拉高到最大的概率；随着一个点被拉高将不可避免的使其他的地方被拉低。最终的密度函数是所有局部点的向上力的一个平衡。

我们也可以认为最大似然估计是最小化数据生成分布（data generating distriution）与模型分布的**KL散度**：
![Image](https://raw.githubusercontent.com/ChenKaiXuSan/blog/master/image/gan/201704/28/eq04.jpg) 
{:.border}

理想情况下，如果$p_{data}$是$p_{model}(x;\theta)$分布的一种分布， 那么模型就可以很好的覆盖$p_{data}$。在实际应用中，我们无法获取$p_{data}$本身，只能使用包含有$m$个样本的训练数据集。使用这$m$个样本可以定义一个经验性分布$\hat{p}{data}$来近似$p{data}$。最小化$\hat{p}{data}$$与$p{data}$的KL散度等价于在训练数据集上最大化对数似然(log-likelihood)。

关于更多的最大似然以及其他统计估计方法的内容， 请参考*Goodfellow et al. (2016)*的第五章。

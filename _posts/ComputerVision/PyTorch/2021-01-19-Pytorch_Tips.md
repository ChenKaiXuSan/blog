---
title: Pytorch Tips
tags: [ComputerVision, PyTorch]
---

è®°å½•ä¸€ä¸‹ä½¿ç”¨pytorchä¸­å‘çŽ°çš„å°æŠ€å·§ã€‚
å†™çš„å†™çš„è§‰å¾—åº”è¯¥ä¹ŸæŠŠä¸€äº›python3çš„ä¸œè¥¿æ”¾è¿›åŽ»ã€‚  
ç„¶åŽè¿™ç¯‡å°±å˜æˆäº†å†™ä»£ç çš„ç¬”è®°ã€‚ðŸ¤¦â€

### 2021å¹´01æœˆ19æ—¥ 18:50:01
##### .size() vs .shape, which one should be used?
  .shape is an alias for .size(), and was added to more closely match numpy
##### range()æ˜¾ç¤ºçš„æ˜¯ä¸€ä¸ªèŒƒå›´ï¼Œä»Žå¼€å§‹åˆ°ç»“æŸã€‚åªæœ‰æ”¾åˆ°listä¸­æ‰ä¼šæ˜¾ç¤ºæ‰€æœ‰ç”Ÿæˆçš„æ•°ã€‚
  list(range(0, 5, 1)) #ä»Ž0~4ï¼Œæ­¥é•¿1

### 2021å¹´01æœˆ20æ—¥ 11:56:07
##### torch.randn å’Œ torch.rand çš„åŒºåˆ«
- torch.randn
  Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).
- torch.rand
  Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1).The shape of the tensor is defined by the variable argument size.

### 2021å¹´01æœˆ25æ—¥ 11:33:44
##### torch.nn.Embedding
A simple lookup table that stores embeddings of a fixed dictionary and size.  
This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.
##### python3 enumerate()
enumerate() å‡½æ•°ç”¨äºŽå°†ä¸€ä¸ªå¯éåŽ†çš„æ•°æ®å¯¹è±¡(å¦‚åˆ—è¡¨ã€å…ƒç»„æˆ–å­—ç¬¦ä¸²)ç»„åˆä¸ºä¸€ä¸ªç´¢å¼•åºåˆ—ï¼ŒåŒæ—¶åˆ—å‡ºæ•°æ®å’Œæ•°æ®ä¸‹æ ‡ï¼Œä¸€èˆ¬ç”¨åœ¨ for å¾ªçŽ¯å½“ä¸­ã€‚
  ```
  >>> seasons = ['Spring', 'Summer', 'Fall', 'Winter']
  >>> list(enumerate(seasons))
  [(0, 'Spring'), (1, 'Summer'), (2, 'Fall'), (3, 'Winter')]
  >>> list(enumerate(seasons, start=1))       # å°æ ‡ä»Ž 1 å¼€å§‹
  [(1, 'Spring'), (2, 'Summer'), (3, 'Fall'), (4, 'Winter')]
  ```
##### fill_(value)
Fills self tensor with the specified value.
##### torch.tensor.detach()
Returns a new Tensor, detached from the current graph.
The result will never require gradient.
##### torch.nn.Upsample()
Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.

### 2021å¹´01æœˆ26æ—¥ 12:09:26
##### torch.nn.BatchNorm1d / torch.nn.BatchNorm2d
It is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling.
  - torch.nn.BatchNorm2d: Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension)
  - torch.nn.BatchNorm1d: 2D and 3D input.
##### torch.FloatTensor / torch.cuda.FloatTensor
å‰è€…åœ¨cpuä¸Šè¿ç®—ï¼ŒåŽè€…åœ¨gpuä¸Šè¿ç®—
##### onehotç¼–ç 
one hotç¼–ç æ˜¯å°†ç±»åˆ«å˜é‡è½¬æ¢ä¸ºæœºå™¨å­¦ä¹ ç®—æ³•æ˜“äºŽåˆ©ç”¨çš„ä¸€ç§å½¢å¼çš„è¿‡ç¨‹ã€‚
å¦‚æžœåŽŸæœ¬çš„æ ‡ç­¾ç¼–ç æ˜¯æœ‰åºçš„ï¼Œonehotç¼–ç ä¼šä¸¢å¤±é¡ºåºä¿¡æ¯ã€‚

### 2021å¹´01æœˆ27æ—¥ 21:26:36
##### D.cuda()
Moves all model parameters and buffers to the GPU.
##### å…³äºŽwindowç³»ç»Ÿè·¯å¾„é—®é¢˜
  - å¯ä»¥ç›´æŽ¥å¤åˆ¶è·¯å¾„ï¼Œä½†è¦åœ¨å‰é¢åŠ ä¸Šr''
  - æŽ¨èæŠŠ\æ”¹æˆ/

### 2021å¹´01æœˆ28æ—¥ 09:27:43
##### torch.squeeze()
Returns a tensor with all the dimensions of input of size 1 removed.
For example, if input is of shape: (AÃ—1Ã—BÃ—CÃ—1Ã—D) then the out tensor will be of shape: (AÃ—BÃ—CÃ—D) .
##### torch.stack() / torch.cat()
è¿žæŽ¥tensorçš„æ—¶å€™ä½¿ç”¨
  - torch.stack() : å¢žåŠ ä¸€ä¸ªæ–°ç»´åº¦ï¼Œç†è§£ä¸ºå åŠ 
  - torch.cat() : å¢žåŠ çŽ°æœ‰çš„ç»´åº¦ï¼Œç†è§£ä¸ºç»­æŽ¥

### 2021å¹´01æœˆ29æ—¥ 17:21:03
##### numpy.transpose(a)
Reverse or permute the axes of an array; returns the modified array.
For an array a with two axes, transpose(a) gives the matrix transpose.
##### pickle (python3)
æ¨¡å— pickle å®žçŽ°äº†å¯¹ä¸€ä¸ª Python å¯¹è±¡ç»“æž„çš„äºŒè¿›åˆ¶åºåˆ—åŒ–å’Œååºåˆ—åŒ–ã€‚  
  - pickle.dumps(obj, file)
  å°†å¯¹è±¡ obj å°å­˜ä»¥åŽçš„å¯¹è±¡å†™å…¥å·²æ‰“å¼€çš„ file object fileã€‚
##### matplotlib.pyplot.imread(fname, format)
Read an image from a file into an array.

### 2021å¹´02æœˆ01æ—¥ 16:04:22
##### Lib/functools.py
functools æ¨¡å—åº”ç”¨äºŽé«˜é˜¶å‡½æ•°ï¼Œå³å‚æ•°æˆ–ï¼ˆå’Œï¼‰è¿”å›žå€¼ä¸ºå…¶ä»–å‡½æ•°çš„å‡½æ•°ã€‚ 
é€šå¸¸æ¥è¯´ï¼Œæ­¤æ¨¡å—çš„åŠŸèƒ½é€‚ç”¨äºŽæ‰€æœ‰å¯è°ƒç”¨å¯¹è±¡ã€‚
##### linear_attention_transformer
A fully featured Transformer that mixes (QKáµ€)V local attention with Q(Káµ€V) global attention (scales linearly with respect to sequence length) for efficient long-range language modeling.
``` 
 pip install linear-attention-transformer
```
##### torch.Tensor.contiguous(memory_format=torch.contiguous_format)
Returns a contiguous in memory tensor containing the same data as self tensor. If self tensor is already in the specified memory format, this function returns the self tensor.
##### torch.mean(input, dim, keepdim=False)
Returns the mean value of each row of the input tensor in the given dimension dim. If dim is a list of dimensions, reduce over all of them.


### 2021å¹´02æœˆ06æ—¥ 17:00:22
##### torch.numel(input) â†’ int
Returns the total number of elements in the input tensor.
```
>>> a = torch.zeros(4,4)
>>> torch.numel(a)
16
```

##### torch.isnan(input) â†’ Tensor
Returns a new tensor with boolean elements representing if each element of input is NaN or not. Complex values are considered NaN when either their real and/or imaginary part is NaN.
```
>>> torch.isnan(torch.tensor([1, float('nan'), 2]))
tensor([False, True, False])
```

##### torch.Tensor.uniform_(from=0, to=1) â†’ Tensor
Fills self tensor with numbers sampled from the continuous uniform distribution:

$$
P(x) = \dfrac{1}{\text{to} - \text{from}}
$$
â€‹	
##### torch.Tensor.expand(*sizes) â†’ Tensor
Returns a new view of the self tensor with singleton dimensions expanded to a larger size.
Passing -1 as the size for a dimension means not changing the size of that dimension.
Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1.
```
>>> x = torch.tensor([[1], [2], [3]])
>>> x.size()
torch.Size([3, 1])
>>> x.expand(3, 4)
tensor([[ 1,  1,  1,  1],
        [ 2,  2,  2,  2],
        [ 3,  3,  3,  3]])
>>> x.expand(-1, 4)   # -1 means not changing the size of that dimension
tensor([[ 1,  1,  1,  1],
        [ 2,  2,  2,  2],
        [ 3,  3,  3,  3]])
```

##### torch.acos(input, *, out=None) â†’ Tensor
Computes the inverse cosine of each element in input.
$$
\text{out}_{i} = \cos^{-1}(\text{input}_{i})
$$

##### torch.norm(input, p='fro', dim=None, keepdim=False, out=None, dtype=None)
Returns the matrix norm or vector norm of a given tensor.

### 2021å¹´02æœˆ09æ—¥ 14:55:55
##### torchvision.transforms.Lambda(lambd)
Apply a user-defined lambda as a transform. This transform does not support torchscript.

##### torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None)
Down/up samples the input to either the given size or the given scale_factor
The algorithm used for interpolation is determined by mode.

##### torch.flip(input, dims) â†’ Tensor
Reverse the order of a n-D tensor along given axis in dims.
```
>>> x = torch.arange(8).view(2, 2, 2)
>>> x
tensor([[[ 0,  1],
         [ 2,  3]],

        [[ 4,  5],
         [ 6,  7]]])
>>> torch.flip(x, [0, 1])
tensor([[[ 6,  7],
         [ 4,  5]],

        [[ 2,  3],
         [ 0,  1]]])
```

##### torch.rsqrt(input, *, out=None) â†’ Tensor
Returns a new tensor with the reciprocal of the square-root of each of the elements of input.
$$
\text{out}_{i} = \frac{1}{\sqrt{\text{input}_{i}}}
$$
```
>>> a = torch.randn(4)
>>> a
tensor([-0.0370,  0.2970,  1.5420, -0.9105])
>>> torch.rsqrt(a)
tensor([    nan,  1.8351,  0.8053,     nan])
```

##### permute(*dims) â†’ Tensor
Returns a view of the original tensor with its dimensions permuted.
```
>>> x = torch.randn(2, 3, 5)
>>> x.size()
torch.Size([2, 3, 5])
>>> x.permute(2, 0, 1).size()
torch.Size([5, 2, 3])
```

### 2021å¹´02æœˆ10æ—¥ 18:46:39
##### torch.nn.ModuleList(modules: Optional[Iterable[torch.nn.modules.module.Module]] = None)
Holds submodules in a list.

ModuleList can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all Module methods.

```
class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])

    def forward(self, x):
        # ModuleList can act as an iterable, or be indexed using ints
        for i, l in enumerate(self.linears):
            x = self.linears[i // 2](x) + l(x)
        return x
```

### 2021å¹´02æœˆ16æ—¥ 15:08:08
##### torch.nn.init.uniform_(tensor, a=0.0, b=1.0)
Fills the input Tensor with values drawn from the uniform distribution
$$ \mathcal{U}(a, b) $$
```
>>> w = torch.empty(3, 5)
>>> nn.init.uniform_(w)
```

##### torch.nn.init.constant_(tensor, val)
Fills the input Tensor with the value $ \text{val} $.
```
>>> w = torch.empty(3, 5)
>>> nn.init.constant_(w, 0.3)
```

##### torch.pow(input, exponent, *, out=None) â†’ Tensor
Takes the power of each element in input with exponent and returns a tensor with the result.

exponent can be either a single float number or a Tensor with the same number of elements as input.

When exponent is a scalar value, the operation applied is:
$$
\text{out}_i = x_i ^ \text{exponent}
â€‹$$
 
When exponent is a tensor, the operation applied is:
$$
\text{out}_i = x_i ^ {\text{exponent}_i}
â€‹$$

When exponent is a tensor, the shapes of input and exponent must be broadcastable.
```
>>> a = torch.randn(4)
>>> a
tensor([ 0.4331,  1.2475,  0.6834, -0.2791])
>>> torch.pow(a, 2)
tensor([ 0.1875,  1.5561,  0.4670,  0.0779])
>>> exp = torch.arange(1., 5.)

>>> a = torch.arange(1., 5.)
>>> a
tensor([ 1.,  2.,  3.,  4.])
>>> exp
tensor([ 1.,  2.,  3.,  4.])
>>> torch.pow(a, exp)
tensor([   1.,    4.,   27.,  256.])
```